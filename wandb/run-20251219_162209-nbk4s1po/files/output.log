/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████| 3/3 [00:12<00:00,  4.14s/it]
RANK-0 training started !
/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████| 3/3 [00:04<00:00,  1.63s/it]
  0%|                                                                                                              | 0/16 [00:00<?, ?it/s]/pfs/rl-train/wenhaoli/spotlight/train.py:406: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  length = torch.tensor(length, dtype=torch.int64, device=local_rank)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [01:15<00:00,  4.73s/it]
[rank0]:W1219 16:24:21.444000 2189878 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0] Graph break from `Tensor.item()`, consider setting:
[rank0]:W1219 16:24:21.444000 2189878 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0]     torch._dynamo.config.capture_scalar_outputs = True
[rank0]:W1219 16:24:21.444000 2189878 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0] or:
[rank0]:W1219 16:24:21.444000 2189878 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1
[rank0]:W1219 16:24:21.444000 2189878 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0] to include these operations in the captured graph.
[rank0]:W1219 16:24:21.444000 2189878 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0]
[rank0]:W1219 16:24:21.444000 2189878 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0] Graph break: from user code at:
[rank0]:W1219 16:24:21.444000 2189878 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0]   File "/pfs/rl-train/wenhaoli/spotlight/train.py", line 182, in compute_attn_supervise_loss
[rank0]:W1219 16:24:21.444000 2189878 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0]     return top_acc.item(), oth_fp.item(), loss
[rank0]:W1219 16:24:21.444000 2189878 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0]
[rank0]:W1219 16:24:21.444000 2189878 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0]
layer:     0 | step:     0 | loss: 67.439 | tacc: 0.631 | oacc: 0.578
layer:     0 | step:     1 | loss: 67.372 | tacc: 0.629 | oacc: 0.577
layer:     0 | step:     2 | loss: 67.304 | tacc: 0.631 | oacc: 0.577
layer:     0 | step:     3 | loss: 67.063 | tacc: 0.625 | oacc: 0.567
layer:     0 | step:     4 | loss: 67.000 | tacc: 0.621 | oacc: 0.563
layer:     0 | step:     5 | loss: 66.822 | tacc: 0.616 | oacc: 0.556
layer:     0 | step:     6 | loss: 66.595 | tacc: 0.611 | oacc: 0.549
layer:     0 | step:     7 | loss: 66.399 | tacc: 0.594 | oacc: 0.530
layer:     0 | step:     8 | loss: 66.198 | tacc: 0.585 | oacc: 0.520
layer:     0 | step:     9 | loss: 65.850 | tacc: 0.571 | oacc: 0.500
layer:     0 | step:    10 | loss: 65.584 | tacc: 0.561 | oacc: 0.485
layer:     0 | step:    11 | loss: 65.423 | tacc: 0.547 | oacc: 0.468
layer:     0 | step:    12 | loss: 65.186 | tacc: 0.533 | oacc: 0.451
layer:     0 | step:    13 | loss: 64.992 | tacc: 0.523 | oacc: 0.437
layer:     0 | step:    14 | loss: 64.761 | tacc: 0.516 | oacc: 0.424
layer:     0 | step:    15 | loss: 64.680 | tacc: 0.506 | oacc: 0.413
layer:     0 | step:    16 | loss: 64.388 | tacc: 0.507 | oacc: 0.406
layer:     0 | step:    17 | loss: 64.216 | tacc: 0.499 | oacc: 0.395
layer:     0 | step:    18 | loss: 64.140 | tacc: 0.496 | oacc: 0.391
layer:     0 | step:    19 | loss: 63.863 | tacc: 0.502 | oacc: 0.391
layer:     0 | step:    20 | loss: 63.482 | tacc: 0.510 | oacc: 0.389
layer:     0 | step:    21 | loss: 63.282 | tacc: 0.513 | oacc: 0.389
layer:     0 | step:    22 | loss: 63.251 | tacc: 0.520 | oacc: 0.395
layer:     0 | step:    23 | loss: 62.886 | tacc: 0.527 | oacc: 0.395
layer:     0 | step:    24 | loss: 62.806 | tacc: 0.534 | oacc: 0.400
layer:     0 | step:    25 | loss: 62.474 | tacc: 0.548 | oacc: 0.406
layer:     0 | step:    26 | loss: 62.239 | tacc: 0.552 | oacc: 0.406
layer:     0 | step:    27 | loss: 62.093 | tacc: 0.555 | oacc: 0.405
layer:     0 | step:    28 | loss: 61.955 | tacc: 0.558 | oacc: 0.405
layer:     0 | step:    29 | loss: 61.629 | tacc: 0.566 | oacc: 0.406
layer:     0 | step:    30 | loss: 61.565 | tacc: 0.566 | oacc: 0.405
layer:     0 | step:    31 | loss: 61.384 | tacc: 0.566 | oacc: 0.400
layer:     0 | step:    32 | loss: 61.181 | tacc: 0.566 | oacc: 0.396
layer:     0 | step:    33 | loss: 61.055 | tacc: 0.567 | oacc: 0.394
layer:     0 | step:    34 | loss: 60.646 | tacc: 0.572 | oacc: 0.390
layer:     0 | step:    35 | loss: 60.553 | tacc: 0.570 | oacc: 0.386
layer:     0 | step:    36 | loss: 60.455 | tacc: 0.573 | oacc: 0.386
layer:     0 | step:    37 | loss: 60.310 | tacc: 0.575 | oacc: 0.384
layer:     0 | step:    38 | loss: 60.051 | tacc: 0.579 | oacc: 0.382
layer:     0 | step:    39 | loss: 60.080 | tacc: 0.580 | oacc: 0.381
layer:     0 | step:    40 | loss: 59.823 | tacc: 0.582 | oacc: 0.380
layer:     0 | step:    41 | loss: 59.631 | tacc: 0.582 | oacc: 0.375
layer:     0 | step:    42 | loss: 59.450 | tacc: 0.585 | oacc: 0.373
layer:     0 | step:    43 | loss: 59.547 | tacc: 0.584 | oacc: 0.375
layer:     0 | step:    44 | loss: 59.060 | tacc: 0.588 | oacc: 0.369
layer:     0 | step:    45 | loss: 59.214 | tacc: 0.586 | oacc: 0.370
layer:     0 | step:    46 | loss: 59.021 | tacc: 0.590 | oacc: 0.368
layer:     0 | step:    47 | loss: 58.908 | tacc: 0.589 | oacc: 0.366
layer:     0 | step:    48 | loss: 58.615 | tacc: 0.593 | oacc: 0.364
layer:     0 | step:    49 | loss: 58.484 | tacc: 0.596 | oacc: 0.364
layer:     0 | step:    50 | loss: 58.497 | tacc: 0.597 | oacc: 0.365
layer:     0 | step:    51 | loss: 58.487 | tacc: 0.600 | oacc: 0.368
layer:     0 | step:    52 | loss: 58.323 | tacc: 0.605 | oacc: 0.369
layer:     0 | step:    53 | loss: 57.320 | tacc: 0.616 | oacc: 0.358
layer:     0 | step:    54 | loss: 57.851 | tacc: 0.606 | oacc: 0.360
layer:     0 | step:    55 | loss: 57.002 | tacc: 0.622 | oacc: 0.357
layer:     0 | step:    56 | loss: 57.699 | tacc: 0.607 | oacc: 0.358
layer:     0 | step:    57 | loss: 57.685 | tacc: 0.606 | oacc: 0.356
layer:     0 | step:    58 | loss: 57.518 | tacc: 0.608 | oacc: 0.355
layer:     0 | step:    59 | loss: 57.214 | tacc: 0.612 | oacc: 0.353
layer:     0 | step:    60 | loss: 57.408 | tacc: 0.612 | oacc: 0.356
layer:     0 | step:    61 | loss: 56.642 | tacc: 0.626 | oacc: 0.354
layer:     0 | step:    62 | loss: 56.978 | tacc: 0.616 | oacc: 0.352
layer:     0 | step:    63 | loss: 56.879 | tacc: 0.622 | oacc: 0.355
  0%|                                                                                                              | 0/16 [00:00<?, ?it/s]/pfs/rl-train/wenhaoli/spotlight/train.py:406: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  length = torch.tensor(length, dtype=torch.int64, device=local_rank)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [01:14<00:00,  4.65s/it]
layer:     0 | step:    64 | loss: 56.834 | tacc: 0.622 | oacc: 0.353
layer:     0 | step:    65 | loss: 55.947 | tacc: 0.636 | oacc: 0.350
layer:     0 | step:    66 | loss: 56.580 | tacc: 0.625 | oacc: 0.351
layer:     0 | step:    67 | loss: 56.509 | tacc: 0.626 | oacc: 0.351
layer:     0 | step:    68 | loss: 56.406 | tacc: 0.627 | oacc: 0.350
layer:     0 | step:    69 | loss: 56.374 | tacc: 0.626 | oacc: 0.348
layer:     0 | step:    70 | loss: 55.998 | tacc: 0.631 | oacc: 0.345
layer:     0 | step:    71 | loss: 56.160 | tacc: 0.627 | oacc: 0.345
layer:     0 | step:    72 | loss: 55.894 | tacc: 0.631 | oacc: 0.343
layer:     0 | step:    73 | loss: 55.785 | tacc: 0.632 | oacc: 0.342
layer:     0 | step:    74 | loss: 56.012 | tacc: 0.630 | oacc: 0.344
layer:     0 | step:    75 | loss: 55.442 | tacc: 0.637 | oacc: 0.340
layer:     0 | step:    76 | loss: 55.599 | tacc: 0.635 | oacc: 0.341
layer:     0 | step:    77 | loss: 55.551 | tacc: 0.635 | oacc: 0.340
layer:     0 | step:    78 | loss: 55.382 | tacc: 0.639 | oacc: 0.341
layer:     0 | step:    79 | loss: 55.255 | tacc: 0.641 | oacc: 0.340
layer:     0 | step:    80 | loss: 55.327 | tacc: 0.641 | oacc: 0.341
layer:     0 | step:    81 | loss: 55.472 | tacc: 0.638 | oacc: 0.342
layer:     0 | step:    82 | loss: 54.877 | tacc: 0.645 | oacc: 0.335
layer:     0 | step:    83 | loss: 54.948 | tacc: 0.645 | oacc: 0.338
layer:     0 | step:    84 | loss: 54.920 | tacc: 0.646 | oacc: 0.337
layer:     0 | step:    85 | loss: 54.313 | tacc: 0.661 | oacc: 0.340
layer:     0 | step:    86 | loss: 54.602 | tacc: 0.648 | oacc: 0.334
layer:     0 | step:    87 | loss: 54.552 | tacc: 0.648 | oacc: 0.332
layer:     0 | step:    88 | loss: 54.392 | tacc: 0.651 | oacc: 0.332
layer:     0 | step:    89 | loss: 54.450 | tacc: 0.650 | oacc: 0.333
layer:     0 | step:    90 | loss: 54.380 | tacc: 0.650 | oacc: 0.331
layer:     0 | step:    91 | loss: 54.203 | tacc: 0.659 | oacc: 0.336
layer:     0 | step:    92 | loss: 54.247 | tacc: 0.652 | oacc: 0.330
layer:     0 | step:    93 | loss: 54.380 | tacc: 0.651 | oacc: 0.331
layer:     0 | step:    94 | loss: 54.037 | tacc: 0.656 | oacc: 0.330
layer:     0 | step:    95 | loss: 54.125 | tacc: 0.654 | oacc: 0.330
layer:     0 | step:    96 | loss: 54.118 | tacc: 0.654 | oacc: 0.329
layer:     0 | step:    97 | loss: 53.973 | tacc: 0.658 | oacc: 0.330
layer:     0 | step:    98 | loss: 53.834 | tacc: 0.657 | oacc: 0.328
layer:     0 | step:    99 | loss: 53.895 | tacc: 0.657 | oacc: 0.328
layer:     0 | step:   100 | loss: 53.438 | tacc: 0.662 | oacc: 0.324
layer:     0 | step:   101 | loss: 53.815 | tacc: 0.656 | oacc: 0.326
layer:     0 | step:   102 | loss: 52.488 | tacc: 0.673 | oacc: 0.317
layer:     0 | step:   103 | loss: 53.509 | tacc: 0.658 | oacc: 0.322
layer:     0 | step:   104 | loss: 53.492 | tacc: 0.661 | oacc: 0.323
layer:     0 | step:   105 | loss: 53.353 | tacc: 0.664 | oacc: 0.324
layer:     0 | step:   106 | loss: 53.264 | tacc: 0.665 | oacc: 0.323
layer:     0 | step:   107 | loss: 52.980 | tacc: 0.670 | oacc: 0.323
layer:     0 | step:   108 | loss: 52.991 | tacc: 0.666 | oacc: 0.320
layer:     0 | step:   109 | loss: 53.128 | tacc: 0.667 | oacc: 0.323
layer:     0 | step:   110 | loss: 52.996 | tacc: 0.669 | oacc: 0.321
layer:     0 | step:   111 | loss: 51.766 | tacc: 0.686 | oacc: 0.316
layer:     0 | step:   112 | loss: 53.235 | tacc: 0.664 | oacc: 0.322
layer:     0 | step:   113 | loss: 52.674 | tacc: 0.673 | oacc: 0.319
layer:     0 | step:   114 | loss: 52.711 | tacc: 0.675 | oacc: 0.321
layer:     0 | step:   115 | loss: 53.046 | tacc: 0.669 | oacc: 0.323
layer:     0 | step:   116 | loss: 52.810 | tacc: 0.673 | oacc: 0.321
layer:     0 | step:   117 | loss: 52.772 | tacc: 0.670 | oacc: 0.318
layer:     0 | step:   118 | loss: 52.583 | tacc: 0.672 | oacc: 0.316
layer:     0 | step:   119 | loss: 52.846 | tacc: 0.666 | oacc: 0.317
layer:     0 | step:   120 | loss: 51.729 | tacc: 0.679 | oacc: 0.308
layer:     0 | step:   121 | loss: 52.501 | tacc: 0.674 | oacc: 0.316
layer:     0 | step:   122 | loss: 52.369 | tacc: 0.675 | oacc: 0.315
layer:     0 | step:   123 | loss: 52.193 | tacc: 0.676 | oacc: 0.313
layer:     0 | step:   124 | loss: 51.860 | tacc: 0.681 | oacc: 0.312
layer:     0 | step:   125 | loss: 52.232 | tacc: 0.678 | oacc: 0.316
layer:     0 | step:   126 | loss: 50.936 | tacc: 0.697 | oacc: 0.310
layer:     0 | step:   127 | loss: 52.083 | tacc: 0.680 | oacc: 0.315
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [01:14<00:00,  4.65s/it]
layer:     0 | step:   128 | loss: 52.146 | tacc: 0.681 | oacc: 0.317
layer:     0 | step:   129 | loss: 51.855 | tacc: 0.682 | oacc: 0.312
layer:     0 | step:   130 | loss: 51.810 | tacc: 0.690 | oacc: 0.319
layer:     0 | step:   131 | loss: 52.215 | tacc: 0.675 | oacc: 0.313
layer:     0 | step:   132 | loss: 51.750 | tacc: 0.679 | oacc: 0.308
layer:     0 | step:   133 | loss: 51.557 | tacc: 0.685 | oacc: 0.309
layer:     0 | step:   134 | loss: 51.408 | tacc: 0.684 | oacc: 0.306
layer:     0 | step:   135 | loss: 51.245 | tacc: 0.688 | oacc: 0.306
layer:     0 | step:   136 | loss: 51.541 | tacc: 0.685 | oacc: 0.309
layer:     0 | step:   137 | loss: 51.587 | tacc: 0.689 | oacc: 0.314
layer:     0 | step:   138 | loss: 51.198 | tacc: 0.689 | oacc: 0.306
layer:     0 | step:   139 | loss: 50.660 | tacc: 0.696 | oacc: 0.304
layer:     0 | step:   140 | loss: 51.101 | tacc: 0.691 | oacc: 0.306
layer:     0 | step:   141 | loss: 51.512 | tacc: 0.686 | oacc: 0.309
layer:     0 | step:   142 | loss: 51.228 | tacc: 0.690 | oacc: 0.308
layer:     0 | step:   143 | loss: 50.578 | tacc: 0.691 | oacc: 0.299
layer:     0 | step:   144 | loss: 50.903 | tacc: 0.697 | oacc: 0.308
layer:     0 | step:   145 | loss: 51.287 | tacc: 0.688 | oacc: 0.307
layer:     0 | step:   146 | loss: 50.192 | tacc: 0.690 | oacc: 0.291
layer:     0 | step:   147 | loss: 50.255 | tacc: 0.701 | oacc: 0.302
layer:     0 | step:   148 | loss: 51.016 | tacc: 0.689 | oacc: 0.303
layer:     0 | step:   149 | loss: 50.991 | tacc: 0.691 | oacc: 0.304
layer:     0 | step:   150 | loss: 50.373 | tacc: 0.702 | oacc: 0.303
layer:     0 | step:   151 | loss: 49.594 | tacc: 0.701 | oacc: 0.289
layer:     0 | step:   152 | loss: 50.763 | tacc: 0.693 | oacc: 0.302
layer:     0 | step:   153 | loss: 50.883 | tacc: 0.693 | oacc: 0.304
layer:     0 | step:   154 | loss: 50.825 | tacc: 0.694 | oacc: 0.305
layer:     0 | step:   155 | loss: 50.519 | tacc: 0.698 | oacc: 0.302
layer:     0 | step:   156 | loss: 51.138 | tacc: 0.691 | oacc: 0.307
layer:     0 | step:   157 | loss: 50.898 | tacc: 0.691 | oacc: 0.303
layer:     0 | step:   158 | loss: 50.133 | tacc: 0.707 | oacc: 0.304
layer:     0 | step:   159 | loss: 50.476 | tacc: 0.701 | oacc: 0.304
layer:     0 | step:   160 | loss: 50.426 | tacc: 0.698 | oacc: 0.300
layer:     0 | step:   161 | loss: 50.790 | tacc: 0.692 | oacc: 0.302
layer:     0 | step:   162 | loss: 50.528 | tacc: 0.696 | oacc: 0.300
layer:     0 | step:   163 | loss: 50.436 | tacc: 0.699 | oacc: 0.301
layer:     0 | step:   164 | loss: 50.176 | tacc: 0.702 | oacc: 0.299
layer:     0 | step:   165 | loss: 49.018 | tacc: 0.716 | oacc: 0.294
layer:     0 | step:   166 | loss: 50.203 | tacc: 0.703 | oacc: 0.300
layer:     0 | step:   167 | loss: 50.049 | tacc: 0.703 | oacc: 0.298
layer:     0 | step:   168 | loss: 50.345 | tacc: 0.701 | oacc: 0.301
layer:     0 | step:   169 | loss: 50.149 | tacc: 0.703 | oacc: 0.299
layer:     0 | step:   170 | loss: 50.138 | tacc: 0.704 | oacc: 0.300
layer:     0 | step:   171 | loss: 50.061 | tacc: 0.706 | oacc: 0.300
layer:     0 | step:   172 | loss: 49.944 | tacc: 0.707 | oacc: 0.300
layer:     0 | step:   173 | loss: 50.002 | tacc: 0.702 | oacc: 0.297
layer:     0 | step:   174 | loss: 49.568 | tacc: 0.709 | oacc: 0.294
layer:     0 | step:   175 | loss: 49.178 | tacc: 0.714 | oacc: 0.294
layer:     0 | step:   176 | loss: 49.793 | tacc: 0.703 | oacc: 0.294
layer:     0 | step:   177 | loss: 49.908 | tacc: 0.703 | oacc: 0.295
layer:     0 | step:   178 | loss: 49.935 | tacc: 0.701 | oacc: 0.294
layer:     0 | step:   179 | loss: 49.927 | tacc: 0.701 | oacc: 0.294
layer:     0 | step:   180 | loss: 49.775 | tacc: 0.702 | oacc: 0.292
layer:     0 | step:   181 | loss: 49.504 | tacc: 0.707 | oacc: 0.291
layer:     0 | step:   182 | loss: 49.455 | tacc: 0.709 | oacc: 0.292
layer:     0 | step:   183 | loss: 49.588 | tacc: 0.710 | oacc: 0.295
layer:     0 | step:   184 | loss: 49.792 | tacc: 0.704 | oacc: 0.294
layer:     0 | step:   185 | loss: 49.723 | tacc: 0.706 | oacc: 0.295
layer:     0 | step:   186 | loss: 49.409 | tacc: 0.710 | oacc: 0.292
layer:     0 | step:   187 | loss: 49.379 | tacc: 0.714 | oacc: 0.296
layer:     0 | step:   188 | loss: 49.264 | tacc: 0.711 | oacc: 0.291
layer:     0 | step:   189 | loss: 49.489 | tacc: 0.708 | oacc: 0.292
layer:     0 | step:   190 | loss: 49.367 | tacc: 0.708 | oacc: 0.290
layer:     0 | step:   191 | loss: 48.944 | tacc: 0.719 | oacc: 0.293
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [01:14<00:00,  4.66s/it]
layer:     0 | step:   192 | loss: 48.410 | tacc: 0.724 | oacc: 0.289
layer:     0 | step:   193 | loss: 49.231 | tacc: 0.711 | oacc: 0.290
layer:     0 | step:   194 | loss: 49.252 | tacc: 0.713 | oacc: 0.292
layer:     0 | step:   195 | loss: 49.102 | tacc: 0.715 | oacc: 0.291
layer:     0 | step:   196 | loss: 49.150 | tacc: 0.711 | oacc: 0.288
layer:     0 | step:   197 | loss: 48.987 | tacc: 0.712 | oacc: 0.286
layer:     0 | step:   198 | loss: 48.164 | tacc: 0.723 | oacc: 0.284
layer:     0 | step:   199 | loss: 48.891 | tacc: 0.713 | oacc: 0.285
layer:     0 | step:   200 | loss: 49.264 | tacc: 0.708 | oacc: 0.288
layer:     0 | step:   201 | loss: 49.246 | tacc: 0.709 | oacc: 0.289
layer:     0 | step:   202 | loss: 49.097 | tacc: 0.712 | oacc: 0.288
layer:     0 | step:   203 | loss: 48.716 | tacc: 0.718 | oacc: 0.287
layer:     0 | step:   204 | loss: 49.110 | tacc: 0.714 | oacc: 0.291
layer:     0 | step:   205 | loss: 48.194 | tacc: 0.729 | oacc: 0.289
layer:     0 | step:   206 | loss: 48.375 | tacc: 0.723 | oacc: 0.285
layer:     0 | step:   207 | loss: 48.532 | tacc: 0.720 | oacc: 0.285
layer:     0 | step:   208 | loss: 48.949 | tacc: 0.715 | oacc: 0.288
layer:     0 | step:   209 | loss: 48.565 | tacc: 0.719 | oacc: 0.285
layer:     0 | step:   210 | loss: 48.609 | tacc: 0.718 | oacc: 0.285
layer:     0 | step:   211 | loss: 48.689 | tacc: 0.717 | oacc: 0.285
layer:     0 | step:   212 | loss: 48.137 | tacc: 0.727 | oacc: 0.286
layer:     0 | step:   213 | loss: 48.486 | tacc: 0.717 | oacc: 0.282
layer:     0 | step:   214 | loss: 48.723 | tacc: 0.714 | oacc: 0.284
layer:     0 | step:   215 | loss: 48.723 | tacc: 0.716 | oacc: 0.285
layer:     0 | step:   216 | loss: 48.409 | tacc: 0.720 | oacc: 0.283
layer:     0 | step:   217 | loss: 48.457 | tacc: 0.719 | oacc: 0.283
layer:     0 | step:   218 | loss: 48.456 | tacc: 0.721 | oacc: 0.284
layer:     0 | step:   219 | loss: 48.531 | tacc: 0.720 | oacc: 0.285
layer:     0 | step:   220 | loss: 48.871 | tacc: 0.716 | oacc: 0.288
layer:     0 | step:   221 | loss: 48.350 | tacc: 0.724 | oacc: 0.285
layer:     0 | step:   222 | loss: 48.398 | tacc: 0.722 | oacc: 0.284
layer:     0 | step:   223 | loss: 48.624 | tacc: 0.718 | oacc: 0.286
layer:     0 | step:   224 | loss: 48.329 | tacc: 0.723 | oacc: 0.284
layer:     0 | step:   225 | loss: 48.202 | tacc: 0.723 | oacc: 0.281
layer:     0 | step:   226 | loss: 48.062 | tacc: 0.723 | oacc: 0.280
layer:     0 | step:   227 | loss: 47.806 | tacc: 0.725 | oacc: 0.279
layer:     0 | step:   228 | loss: 48.508 | tacc: 0.716 | oacc: 0.282
layer:     0 | step:   229 | loss: 48.321 | tacc: 0.719 | oacc: 0.281
layer:     0 | step:   230 | loss: 47.554 | tacc: 0.733 | oacc: 0.282
layer:     0 | step:   231 | loss: 48.160 | tacc: 0.722 | oacc: 0.281
layer:     0 | step:   232 | loss: 48.292 | tacc: 0.719 | oacc: 0.280
layer:     0 | step:   233 | loss: 48.391 | tacc: 0.720 | oacc: 0.282
layer:     0 | step:   234 | loss: 47.851 | tacc: 0.727 | oacc: 0.279
layer:     0 | step:   235 | loss: 48.290 | tacc: 0.720 | oacc: 0.281
layer:     0 | step:   236 | loss: 48.097 | tacc: 0.726 | oacc: 0.282
layer:     0 | step:   237 | loss: 47.945 | tacc: 0.722 | oacc: 0.277
layer:     0 | step:   238 | loss: 47.660 | tacc: 0.727 | oacc: 0.276
layer:     0 | step:   239 | loss: 48.062 | tacc: 0.723 | oacc: 0.280
layer:     0 | step:   240 | loss: 47.802 | tacc: 0.725 | oacc: 0.277
layer:     0 | step:   241 | loss: 47.246 | tacc: 0.721 | oacc: 0.266
layer:     0 | step:   242 | loss: 47.908 | tacc: 0.730 | oacc: 0.283
layer:     0 | step:   243 | loss: 48.201 | tacc: 0.725 | oacc: 0.283
layer:     0 | step:   244 | loss: 47.910 | tacc: 0.730 | oacc: 0.282
layer:     0 | step:   245 | loss: 48.141 | tacc: 0.727 | oacc: 0.284
layer:     0 | step:   246 | loss: 47.752 | tacc: 0.730 | oacc: 0.280
layer:     0 | step:   247 | loss: 47.731 | tacc: 0.729 | oacc: 0.278
layer:     0 | step:   248 | loss: 47.658 | tacc: 0.729 | oacc: 0.278
layer:     0 | step:   249 | loss: 47.690 | tacc: 0.728 | oacc: 0.277
layer:     0 | step:   250 | loss: 47.708 | tacc: 0.726 | oacc: 0.276
layer:     0 | step:   251 | loss: 47.635 | tacc: 0.727 | oacc: 0.276
layer:     0 | step:   252 | loss: 47.360 | tacc: 0.730 | oacc: 0.273
layer:     0 | step:   253 | loss: 47.729 | tacc: 0.723 | oacc: 0.274
layer:     0 | step:   254 | loss: 47.666 | tacc: 0.731 | oacc: 0.279
layer:     0 | step:   255 | loss: 47.370 | tacc: 0.731 | oacc: 0.274
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [01:14<00:00,  4.66s/it]
