/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████| 3/3 [00:12<00:00,  4.08s/it]
RANK-0 training started !
/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████| 3/3 [00:04<00:00,  1.46s/it]
  0%|                                                                                                              | 0/16 [00:00<?, ?it/s]/pfs/rl-train/wenhaoli/spotlight/train.py:406: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  length = torch.tensor(length, dtype=torch.int64, device=local_rank)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [01:16<00:00,  4.78s/it]
[rank0]:W1219 16:02:37.356000 2129212 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0] Graph break from `Tensor.item()`, consider setting:
[rank0]:W1219 16:02:37.356000 2129212 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0]     torch._dynamo.config.capture_scalar_outputs = True
[rank0]:W1219 16:02:37.356000 2129212 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0] or:
[rank0]:W1219 16:02:37.356000 2129212 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1
[rank0]:W1219 16:02:37.356000 2129212 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0] to include these operations in the captured graph.
[rank0]:W1219 16:02:37.356000 2129212 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0]
[rank0]:W1219 16:02:37.356000 2129212 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0] Graph break: from user code at:
[rank0]:W1219 16:02:37.356000 2129212 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0]   File "/pfs/rl-train/wenhaoli/spotlight/train.py", line 182, in compute_attn_supervise_loss
[rank0]:W1219 16:02:37.356000 2129212 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0]     return top_acc.item(), oth_fp.item(), loss
[rank0]:W1219 16:02:37.356000 2129212 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0]
[rank0]:W1219 16:02:37.356000 2129212 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0]
layer:     0 | step:     0 | loss: 50.127 | tacc: 0.635 | oacc: 0.579
layer:     0 | step:     1 | loss: 50.086 | tacc: 0.632 | oacc: 0.577
layer:     0 | step:     2 | loss: 50.050 | tacc: 0.629 | oacc: 0.573
layer:     0 | step:     3 | loss: 49.921 | tacc: 0.628 | oacc: 0.571
layer:     0 | step:     4 | loss: 49.802 | tacc: 0.619 | oacc: 0.561
layer:     0 | step:     5 | loss: 49.682 | tacc: 0.611 | oacc: 0.552
layer:     0 | step:     6 | loss: 49.510 | tacc: 0.600 | oacc: 0.543
layer:     0 | step:     7 | loss: 49.247 | tacc: 0.595 | oacc: 0.531
layer:     0 | step:     8 | loss: 49.026 | tacc: 0.577 | oacc: 0.512
layer:     0 | step:     9 | loss: 48.802 | tacc: 0.570 | oacc: 0.500
layer:     0 | step:    10 | loss: 48.554 | tacc: 0.553 | oacc: 0.480
layer:     0 | step:    11 | loss: 48.306 | tacc: 0.540 | oacc: 0.462
layer:     0 | step:    12 | loss: 48.251 | tacc: 0.525 | oacc: 0.445
layer:     0 | step:    13 | loss: 48.143 | tacc: 0.511 | oacc: 0.429
layer:     0 | step:    14 | loss: 48.025 | tacc: 0.504 | oacc: 0.418
layer:     0 | step:    15 | loss: 47.845 | tacc: 0.494 | oacc: 0.404
layer:     0 | step:    16 | loss: 47.556 | tacc: 0.497 | oacc: 0.399
layer:     0 | step:    17 | loss: 47.467 | tacc: 0.492 | oacc: 0.392
layer:     0 | step:    18 | loss: 47.369 | tacc: 0.490 | oacc: 0.387
layer:     0 | step:    19 | loss: 47.132 | tacc: 0.498 | oacc: 0.390
layer:     0 | step:    20 | loss: 46.843 | tacc: 0.505 | oacc: 0.390
layer:     0 | step:    21 | loss: 46.798 | tacc: 0.508 | oacc: 0.392
layer:     0 | step:    22 | loss: 46.598 | tacc: 0.519 | oacc: 0.397
layer:     0 | step:    23 | loss: 46.397 | tacc: 0.521 | oacc: 0.393
layer:     0 | step:    24 | loss: 46.248 | tacc: 0.536 | oacc: 0.406
layer:     0 | step:    25 | loss: 46.060 | tacc: 0.541 | oacc: 0.407
layer:     0 | step:    26 | loss: 45.812 | tacc: 0.545 | oacc: 0.403
layer:     0 | step:    27 | loss: 45.784 | tacc: 0.553 | oacc: 0.410
layer:     0 | step:    28 | loss: 45.699 | tacc: 0.556 | oacc: 0.410
layer:     0 | step:    29 | loss: 45.447 | tacc: 0.559 | oacc: 0.406
layer:     0 | step:    30 | loss: 45.377 | tacc: 0.561 | oacc: 0.406
layer:     0 | step:    31 | loss: 45.358 | tacc: 0.561 | oacc: 0.405
layer:     0 | step:    32 | loss: 45.070 | tacc: 0.566 | oacc: 0.403
layer:     0 | step:    33 | loss: 44.993 | tacc: 0.567 | oacc: 0.399
layer:     0 | step:    34 | loss: 44.827 | tacc: 0.567 | oacc: 0.395
layer:     0 | step:    35 | loss: 44.622 | tacc: 0.567 | oacc: 0.389
layer:     0 | step:    36 | loss: 44.530 | tacc: 0.568 | oacc: 0.388
layer:     0 | step:    37 | loss: 44.353 | tacc: 0.574 | oacc: 0.387
layer:     0 | step:    38 | loss: 43.995 | tacc: 0.577 | oacc: 0.381
layer:     0 | step:    39 | loss: 44.284 | tacc: 0.568 | oacc: 0.380
layer:     0 | step:    40 | loss: 44.019 | tacc: 0.572 | oacc: 0.377
layer:     0 | step:    41 | loss: 44.078 | tacc: 0.570 | oacc: 0.376
layer:     0 | step:    42 | loss: 44.031 | tacc: 0.572 | oacc: 0.376
layer:     0 | step:    43 | loss: 43.863 | tacc: 0.577 | oacc: 0.376
layer:     0 | step:    44 | loss: 43.692 | tacc: 0.579 | oacc: 0.373
layer:     0 | step:    45 | loss: 43.591 | tacc: 0.580 | oacc: 0.372
layer:     0 | step:    46 | loss: 43.628 | tacc: 0.581 | oacc: 0.374
layer:     0 | step:    47 | loss: 43.433 | tacc: 0.587 | oacc: 0.373
layer:     0 | step:    48 | loss: 43.215 | tacc: 0.588 | oacc: 0.368
layer:     0 | step:    49 | loss: 43.159 | tacc: 0.587 | oacc: 0.366
layer:     0 | step:    50 | loss: 43.168 | tacc: 0.588 | oacc: 0.368
layer:     0 | step:    51 | loss: 43.042 | tacc: 0.590 | oacc: 0.366
layer:     0 | step:    52 | loss: 43.087 | tacc: 0.592 | oacc: 0.368
layer:     0 | step:    53 | loss: 42.746 | tacc: 0.596 | oacc: 0.363
layer:     0 | step:    54 | loss: 42.698 | tacc: 0.596 | oacc: 0.362
layer:     0 | step:    55 | loss: 42.549 | tacc: 0.599 | oacc: 0.359
layer:     0 | step:    56 | loss: 42.558 | tacc: 0.598 | oacc: 0.360
layer:     0 | step:    57 | loss: 42.404 | tacc: 0.602 | oacc: 0.359
layer:     0 | step:    58 | loss: 42.431 | tacc: 0.603 | oacc: 0.360
layer:     0 | step:    59 | loss: 42.137 | tacc: 0.608 | oacc: 0.358
layer:     0 | step:    60 | loss: 42.250 | tacc: 0.608 | oacc: 0.361
layer:     0 | step:    61 | loss: 41.716 | tacc: 0.620 | oacc: 0.357
layer:     0 | step:    62 | loss: 41.962 | tacc: 0.612 | oacc: 0.357
layer:     0 | step:    63 | loss: 41.874 | tacc: 0.611 | oacc: 0.354
  0%|                                                                                                              | 0/16 [00:00<?, ?it/s]/pfs/rl-train/wenhaoli/spotlight/train.py:406: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  length = torch.tensor(length, dtype=torch.int64, device=local_rank)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [01:14<00:00,  4.66s/it]
layer:     0 | step:    64 | loss: 41.887 | tacc: 0.612 | oacc: 0.355
layer:     0 | step:    65 | loss: 41.748 | tacc: 0.614 | oacc: 0.353
layer:     0 | step:    66 | loss: 41.594 | tacc: 0.617 | oacc: 0.351
layer:     0 | step:    67 | loss: 41.840 | tacc: 0.615 | oacc: 0.356
layer:     0 | step:    68 | loss: 41.406 | tacc: 0.624 | oacc: 0.352
layer:     0 | step:    69 | loss: 41.443 | tacc: 0.619 | oacc: 0.349
layer:     0 | step:    70 | loss: 41.177 | tacc: 0.624 | oacc: 0.346
layer:     0 | step:    71 | loss: 41.461 | tacc: 0.619 | oacc: 0.350
layer:     0 | step:    72 | loss: 41.086 | tacc: 0.623 | oacc: 0.344
layer:     0 | step:    73 | loss: 41.229 | tacc: 0.621 | oacc: 0.345
layer:     0 | step:    74 | loss: 41.211 | tacc: 0.620 | oacc: 0.344
layer:     0 | step:    75 | loss: 40.958 | tacc: 0.625 | oacc: 0.341
layer:     0 | step:    76 | loss: 40.873 | tacc: 0.627 | oacc: 0.341
layer:     0 | step:    77 | loss: 40.841 | tacc: 0.630 | oacc: 0.343
layer:     0 | step:    78 | loss: 40.812 | tacc: 0.629 | oacc: 0.342
layer:     0 | step:    79 | loss: 40.722 | tacc: 0.632 | oacc: 0.343
layer:     0 | step:    80 | loss: 40.443 | tacc: 0.640 | oacc: 0.341
layer:     0 | step:    81 | loss: 40.822 | tacc: 0.630 | oacc: 0.343
layer:     0 | step:    82 | loss: 40.563 | tacc: 0.633 | oacc: 0.339
layer:     0 | step:    83 | loss: 40.458 | tacc: 0.635 | oacc: 0.338
layer:     0 | step:    84 | loss: 40.343 | tacc: 0.635 | oacc: 0.335
layer:     0 | step:    85 | loss: 40.415 | tacc: 0.635 | oacc: 0.337
layer:     0 | step:    86 | loss: 40.226 | tacc: 0.640 | oacc: 0.336
layer:     0 | step:    87 | loss: 40.209 | tacc: 0.642 | oacc: 0.338
layer:     0 | step:    88 | loss: 40.186 | tacc: 0.640 | oacc: 0.336
layer:     0 | step:    89 | loss: 40.154 | tacc: 0.640 | oacc: 0.335
layer:     0 | step:    90 | loss: 39.971 | tacc: 0.642 | oacc: 0.332
layer:     0 | step:    91 | loss: 40.194 | tacc: 0.639 | oacc: 0.336
layer:     0 | step:    92 | loss: 39.941 | tacc: 0.644 | oacc: 0.333
layer:     0 | step:    93 | loss: 39.986 | tacc: 0.642 | oacc: 0.332
layer:     0 | step:    94 | loss: 39.705 | tacc: 0.649 | oacc: 0.331
layer:     0 | step:    95 | loss: 39.825 | tacc: 0.648 | oacc: 0.334
layer:     0 | step:    96 | loss: 39.712 | tacc: 0.646 | oacc: 0.330
layer:     0 | step:    97 | loss: 39.564 | tacc: 0.646 | oacc: 0.324
layer:     0 | step:    98 | loss: 39.630 | tacc: 0.648 | oacc: 0.329
layer:     0 | step:    99 | loss: 39.472 | tacc: 0.656 | oacc: 0.332
layer:     0 | step:   100 | loss: 39.312 | tacc: 0.652 | oacc: 0.325
layer:     0 | step:   101 | loss: 39.561 | tacc: 0.648 | oacc: 0.328
layer:     0 | step:   102 | loss: 39.014 | tacc: 0.657 | oacc: 0.322
layer:     0 | step:   103 | loss: 39.381 | tacc: 0.652 | oacc: 0.326
layer:     0 | step:   104 | loss: 39.365 | tacc: 0.651 | oacc: 0.325
layer:     0 | step:   105 | loss: 39.078 | tacc: 0.658 | oacc: 0.324
layer:     0 | step:   106 | loss: 39.170 | tacc: 0.655 | oacc: 0.324
layer:     0 | step:   107 | loss: 38.967 | tacc: 0.659 | oacc: 0.322
layer:     0 | step:   108 | loss: 39.319 | tacc: 0.653 | oacc: 0.325
layer:     0 | step:   109 | loss: 39.104 | tacc: 0.657 | oacc: 0.324
layer:     0 | step:   110 | loss: 38.880 | tacc: 0.659 | oacc: 0.320
layer:     0 | step:   111 | loss: 38.689 | tacc: 0.663 | oacc: 0.319
layer:     0 | step:   112 | loss: 39.096 | tacc: 0.657 | oacc: 0.324
layer:     0 | step:   113 | loss: 38.738 | tacc: 0.662 | oacc: 0.320
layer:     0 | step:   114 | loss: 38.692 | tacc: 0.665 | oacc: 0.321
layer:     0 | step:   115 | loss: 38.923 | tacc: 0.658 | oacc: 0.321
layer:     0 | step:   116 | loss: 38.772 | tacc: 0.662 | oacc: 0.321
layer:     0 | step:   117 | loss: 38.676 | tacc: 0.662 | oacc: 0.318
layer:     0 | step:   118 | loss: 38.556 | tacc: 0.663 | oacc: 0.316
layer:     0 | step:   119 | loss: 38.674 | tacc: 0.662 | oacc: 0.318
layer:     0 | step:   120 | loss: 38.325 | tacc: 0.673 | oacc: 0.319
layer:     0 | step:   121 | loss: 38.354 | tacc: 0.667 | oacc: 0.315
layer:     0 | step:   122 | loss: 38.485 | tacc: 0.665 | oacc: 0.315
layer:     0 | step:   123 | loss: 38.358 | tacc: 0.664 | oacc: 0.311
layer:     0 | step:   124 | loss: 38.017 | tacc: 0.672 | oacc: 0.311
layer:     0 | step:   125 | loss: 38.360 | tacc: 0.668 | oacc: 0.315
layer:     0 | step:   126 | loss: 37.928 | tacc: 0.674 | oacc: 0.310
layer:     0 | step:   127 | loss: 38.161 | tacc: 0.672 | oacc: 0.314
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [01:14<00:00,  4.65s/it]
layer:     0 | step:   128 | loss: 38.139 | tacc: 0.674 | oacc: 0.315
layer:     0 | step:   129 | loss: 38.026 | tacc: 0.675 | oacc: 0.313
layer:     0 | step:   130 | loss: 38.152 | tacc: 0.673 | oacc: 0.314
layer:     0 | step:   131 | loss: 38.161 | tacc: 0.673 | oacc: 0.314
layer:     0 | step:   132 | loss: 38.038 | tacc: 0.676 | oacc: 0.314
layer:     0 | step:   133 | loss: 37.869 | tacc: 0.674 | oacc: 0.309
layer:     0 | step:   134 | loss: 37.672 | tacc: 0.676 | oacc: 0.306
layer:     0 | step:   135 | loss: 37.575 | tacc: 0.678 | oacc: 0.305
layer:     0 | step:   136 | loss: 37.503 | tacc: 0.682 | oacc: 0.307
layer:     0 | step:   137 | loss: 37.752 | tacc: 0.674 | oacc: 0.306
layer:     0 | step:   138 | loss: 37.191 | tacc: 0.688 | oacc: 0.305
layer:     0 | step:   139 | loss: 37.753 | tacc: 0.674 | oacc: 0.306
layer:     0 | step:   140 | loss: 37.546 | tacc: 0.679 | oacc: 0.305
layer:     0 | step:   141 | loss: 37.682 | tacc: 0.678 | oacc: 0.307
layer:     0 | step:   142 | loss: 37.490 | tacc: 0.681 | oacc: 0.307
layer:     0 | step:   143 | loss: 37.117 | tacc: 0.693 | oacc: 0.308
layer:     0 | step:   144 | loss: 37.553 | tacc: 0.682 | oacc: 0.309
layer:     0 | step:   145 | loss: 37.586 | tacc: 0.680 | oacc: 0.307
layer:     0 | step:   146 | loss: 37.252 | tacc: 0.683 | oacc: 0.302
layer:     0 | step:   147 | loss: 37.442 | tacc: 0.678 | oacc: 0.302
layer:     0 | step:   148 | loss: 37.334 | tacc: 0.682 | oacc: 0.302
layer:     0 | step:   149 | loss: 37.279 | tacc: 0.686 | oacc: 0.304
layer:     0 | step:   150 | loss: 36.713 | tacc: 0.696 | oacc: 0.301
layer:     0 | step:   151 | loss: 37.058 | tacc: 0.690 | oacc: 0.303
layer:     0 | step:   152 | loss: 37.031 | tacc: 0.691 | oacc: 0.303
layer:     0 | step:   153 | loss: 37.113 | tacc: 0.686 | oacc: 0.301
layer:     0 | step:   154 | loss: 37.058 | tacc: 0.687 | oacc: 0.301
layer:     0 | step:   155 | loss: 36.914 | tacc: 0.691 | oacc: 0.301
layer:     0 | step:   156 | loss: 37.386 | tacc: 0.679 | oacc: 0.301
layer:     0 | step:   157 | loss: 37.206 | tacc: 0.682 | oacc: 0.300
layer:     0 | step:   158 | loss: 36.917 | tacc: 0.689 | oacc: 0.299
layer:     0 | step:   159 | loss: 37.038 | tacc: 0.687 | oacc: 0.300
layer:     0 | step:   160 | loss: 36.964 | tacc: 0.689 | oacc: 0.300
layer:     0 | step:   161 | loss: 36.708 | tacc: 0.698 | oacc: 0.302
layer:     0 | step:   162 | loss: 36.275 | tacc: 0.700 | oacc: 0.294
layer:     0 | step:   163 | loss: 36.934 | tacc: 0.688 | oacc: 0.298
layer:     0 | step:   164 | loss: 36.556 | tacc: 0.696 | oacc: 0.297
layer:     0 | step:   165 | loss: 36.674 | tacc: 0.693 | oacc: 0.297
layer:     0 | step:   166 | loss: 36.597 | tacc: 0.693 | oacc: 0.295
layer:     0 | step:   167 | loss: 36.483 | tacc: 0.695 | oacc: 0.294
layer:     0 | step:   168 | loss: 36.747 | tacc: 0.692 | oacc: 0.297
layer:     0 | step:   169 | loss: 36.632 | tacc: 0.694 | oacc: 0.296
layer:     0 | step:   170 | loss: 36.135 | tacc: 0.704 | oacc: 0.294
layer:     0 | step:   171 | loss: 36.547 | tacc: 0.695 | oacc: 0.295
layer:     0 | step:   172 | loss: 36.402 | tacc: 0.698 | oacc: 0.295
layer:     0 | step:   173 | loss: 36.481 | tacc: 0.696 | oacc: 0.295
layer:     0 | step:   174 | loss: 35.698 | tacc: 0.700 | oacc: 0.280
layer:     0 | step:   175 | loss: 36.504 | tacc: 0.696 | oacc: 0.295
layer:     0 | step:   176 | loss: 36.330 | tacc: 0.697 | oacc: 0.293
layer:     0 | step:   177 | loss: 36.538 | tacc: 0.695 | oacc: 0.296
layer:     0 | step:   178 | loss: 36.533 | tacc: 0.694 | oacc: 0.294
layer:     0 | step:   179 | loss: 36.367 | tacc: 0.698 | oacc: 0.294
layer:     0 | step:   180 | loss: 36.331 | tacc: 0.697 | oacc: 0.293
layer:     0 | step:   181 | loss: 36.246 | tacc: 0.699 | oacc: 0.292
layer:     0 | step:   182 | loss: 36.139 | tacc: 0.700 | oacc: 0.291
layer:     0 | step:   183 | loss: 35.758 | tacc: 0.709 | oacc: 0.290
layer:     0 | step:   184 | loss: 36.138 | tacc: 0.704 | oacc: 0.294
layer:     0 | step:   185 | loss: 36.241 | tacc: 0.696 | oacc: 0.289
layer:     0 | step:   186 | loss: 36.165 | tacc: 0.698 | oacc: 0.290
layer:     0 | step:   187 | loss: 36.244 | tacc: 0.697 | oacc: 0.289
layer:     0 | step:   188 | loss: 36.054 | tacc: 0.701 | oacc: 0.289
layer:     0 | step:   189 | loss: 36.016 | tacc: 0.703 | oacc: 0.290
layer:     0 | step:   190 | loss: 36.071 | tacc: 0.699 | oacc: 0.288
layer:     0 | step:   191 | loss: 36.064 | tacc: 0.702 | oacc: 0.291
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [01:14<00:00,  4.64s/it]
layer:     0 | step:   192 | loss: 35.870 | tacc: 0.707 | oacc: 0.289
layer:     0 | step:   193 | loss: 35.897 | tacc: 0.705 | oacc: 0.289
layer:     0 | step:   194 | loss: 35.851 | tacc: 0.706 | oacc: 0.288
layer:     0 | step:   195 | loss: 35.981 | tacc: 0.705 | oacc: 0.290
layer:     0 | step:   196 | loss: 35.880 | tacc: 0.707 | oacc: 0.290
layer:     0 | step:   197 | loss: 35.805 | tacc: 0.706 | oacc: 0.288
layer:     0 | step:   198 | loss: 35.596 | tacc: 0.709 | oacc: 0.285
layer:     0 | step:   199 | loss: 35.539 | tacc: 0.711 | oacc: 0.285
layer:     0 | step:   200 | loss: 35.969 | tacc: 0.705 | oacc: 0.290
layer:     0 | step:   201 | loss: 35.792 | tacc: 0.706 | oacc: 0.287
layer:     0 | step:   202 | loss: 35.534 | tacc: 0.703 | oacc: 0.279
layer:     0 | step:   203 | loss: 35.063 | tacc: 0.724 | oacc: 0.286
layer:     0 | step:   204 | loss: 35.700 | tacc: 0.708 | oacc: 0.286
layer:     0 | step:   205 | loss: 35.581 | tacc: 0.707 | oacc: 0.283
layer:     0 | step:   206 | loss: 35.371 | tacc: 0.712 | oacc: 0.282
layer:     0 | step:   207 | loss: 35.408 | tacc: 0.711 | oacc: 0.282
layer:     0 | step:   208 | loss: 35.608 | tacc: 0.708 | oacc: 0.284
layer:     0 | step:   209 | loss: 35.389 | tacc: 0.712 | oacc: 0.283
layer:     0 | step:   210 | loss: 35.420 | tacc: 0.711 | oacc: 0.283
layer:     0 | step:   211 | loss: 35.521 | tacc: 0.710 | oacc: 0.284
layer:     0 | step:   212 | loss: 35.424 | tacc: 0.710 | oacc: 0.283
layer:     0 | step:   213 | loss: 35.446 | tacc: 0.712 | oacc: 0.284
layer:     0 | step:   214 | loss: 35.570 | tacc: 0.708 | oacc: 0.284
layer:     0 | step:   215 | loss: 35.399 | tacc: 0.711 | oacc: 0.282
layer:     0 | step:   216 | loss: 35.236 | tacc: 0.717 | oacc: 0.284
layer:     0 | step:   217 | loss: 35.228 | tacc: 0.715 | oacc: 0.282
layer:     0 | step:   218 | loss: 35.331 | tacc: 0.714 | oacc: 0.282
layer:     0 | step:   219 | loss: 35.383 | tacc: 0.713 | oacc: 0.284
layer:     0 | step:   220 | loss: 35.484 | tacc: 0.709 | oacc: 0.282
layer:     0 | step:   221 | loss: 35.264 | tacc: 0.712 | oacc: 0.280
layer:     0 | step:   222 | loss: 35.277 | tacc: 0.710 | oacc: 0.279
layer:     0 | step:   223 | loss: 35.348 | tacc: 0.710 | oacc: 0.280
layer:     0 | step:   224 | loss: 35.247 | tacc: 0.716 | oacc: 0.283
layer:     0 | step:   225 | loss: 35.044 | tacc: 0.716 | oacc: 0.278
layer:     0 | step:   226 | loss: 34.892 | tacc: 0.720 | oacc: 0.278
layer:     0 | step:   227 | loss: 35.212 | tacc: 0.713 | oacc: 0.279
layer:     0 | step:   228 | loss: 35.244 | tacc: 0.712 | oacc: 0.280
layer:     0 | step:   229 | loss: 35.148 | tacc: 0.715 | oacc: 0.279
layer:     0 | step:   230 | loss: 35.035 | tacc: 0.717 | oacc: 0.279
layer:     0 | step:   231 | loss: 35.066 | tacc: 0.718 | oacc: 0.281
layer:     0 | step:   232 | loss: 34.983 | tacc: 0.720 | oacc: 0.281
layer:     0 | step:   233 | loss: 35.158 | tacc: 0.714 | oacc: 0.280
layer:     0 | step:   234 | loss: 34.363 | tacc: 0.725 | oacc: 0.271
layer:     0 | step:   235 | loss: 35.153 | tacc: 0.715 | oacc: 0.279
layer:     0 | step:   236 | loss: 34.900 | tacc: 0.717 | oacc: 0.276
layer:     0 | step:   237 | loss: 34.905 | tacc: 0.714 | oacc: 0.273
layer:     0 | step:   238 | loss: 34.784 | tacc: 0.718 | oacc: 0.273
layer:     0 | step:   239 | loss: 34.946 | tacc: 0.716 | oacc: 0.276
layer:     0 | step:   240 | loss: 34.705 | tacc: 0.720 | oacc: 0.273
layer:     0 | step:   241 | loss: 34.563 | tacc: 0.725 | oacc: 0.275
layer:     0 | step:   242 | loss: 34.909 | tacc: 0.717 | oacc: 0.276
layer:     0 | step:   243 | loss: 34.992 | tacc: 0.719 | oacc: 0.279
layer:     0 | step:   244 | loss: 34.910 | tacc: 0.717 | oacc: 0.275
layer:     0 | step:   245 | loss: 34.972 | tacc: 0.717 | oacc: 0.278
layer:     0 | step:   246 | loss: 34.678 | tacc: 0.720 | oacc: 0.273
layer:     0 | step:   247 | loss: 34.697 | tacc: 0.722 | oacc: 0.275
layer:     0 | step:   248 | loss: 34.308 | tacc: 0.729 | oacc: 0.273
layer:     0 | step:   249 | loss: 34.689 | tacc: 0.722 | oacc: 0.275
layer:     0 | step:   250 | loss: 34.701 | tacc: 0.721 | oacc: 0.275
layer:     0 | step:   251 | loss: 34.613 | tacc: 0.722 | oacc: 0.273
layer:     0 | step:   252 | loss: 34.438 | tacc: 0.725 | oacc: 0.272
layer:     0 | step:   253 | loss: 34.726 | tacc: 0.719 | oacc: 0.273
layer:     0 | step:   254 | loss: 34.784 | tacc: 0.720 | oacc: 0.276
layer:     0 | step:   255 | loss: 34.490 | tacc: 0.726 | oacc: 0.274
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [01:14<00:00,  4.67s/it]
layer:     0 | step:   256 | loss: 34.317 | tacc: 0.726 | oacc: 0.269
