/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:12<00:00,  4.30s/it]
RANK-0 training started !
/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:05<00:00,  1.69s/it]
  0%|                                                                                                                                                                                                 | 0/16 [00:00<?, ?it/s]/pfs/rl-train/wenhaoli/spotlight/train.py:339: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  length = torch.tensor(length, dtype=torch.int64, device=local_rank)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [01:16<00:00,  4.77s/it]
[rank0]:W1223 02:10:28.772000 2137547 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0] Graph break from `Tensor.item()`, consider setting:
[rank0]:W1223 02:10:28.772000 2137547 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0]     torch._dynamo.config.capture_scalar_outputs = True
[rank0]:W1223 02:10:28.772000 2137547 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0] or:
[rank0]:W1223 02:10:28.772000 2137547 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1
[rank0]:W1223 02:10:28.772000 2137547 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0] to include these operations in the captured graph.
[rank0]:W1223 02:10:28.772000 2137547 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0]
[rank0]:W1223 02:10:28.772000 2137547 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0] Graph break: from user code at:
[rank0]:W1223 02:10:28.772000 2137547 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0]   File "/pfs/rl-train/wenhaoli/spotlight/train.py", line 85, in compute_attn_supervise_loss
[rank0]:W1223 02:10:28.772000 2137547 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0]     sparsity = is_top.sum(-1).float().mean().item() / is_top.shape[-1] / 2
[rank0]:W1223 02:10:28.772000 2137547 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0]
[rank0]:W1223 02:10:28.772000 2137547 site-packages/torch/_dynamo/variables/tensor.py:1047] [0/0]
layer:     0 | step:     0 | loss: 7.938 | thresh: 0.000 | sparse: 0.001 | tacc: 0.550 | oacc: 0.468
layer:     0 | step:     1 | loss: 7.878 | thresh: 0.000 | sparse: 0.001 | tacc: 0.554 | oacc: 0.466
layer:     0 | step:     2 | loss: 7.913 | thresh: 0.000 | sparse: 0.001 | tacc: 0.551 | oacc: 0.466
layer:     0 | step:     3 | loss: 7.941 | thresh: 0.000 | sparse: 0.001 | tacc: 0.550 | oacc: 0.467
layer:     0 | step:     4 | loss: 7.958 | thresh: 0.000 | sparse: 0.001 | tacc: 0.558 | oacc: 0.471
layer:     0 | step:     5 | loss: 7.864 | thresh: 0.000 | sparse: 0.001 | tacc: 0.555 | oacc: 0.467
layer:     0 | step:     6 | loss: 7.808 | thresh: 0.000 | sparse: 0.001 | tacc: 0.557 | oacc: 0.465
layer:     0 | step:     7 | loss: 7.895 | thresh: 0.000 | sparse: 0.001 | tacc: 0.553 | oacc: 0.468
[rank0]:W1223 02:11:15.080000 2137547 site-packages/torch/_dynamo/convert_frame.py:1016] [1/8] torch._dynamo hit config.recompile_limit (8)
[rank0]:W1223 02:11:15.080000 2137547 site-packages/torch/_dynamo/convert_frame.py:1016] [1/8]    function: 'torch_dynamo_resume_in_compute_attn_supervise_loss_at_85' (/pfs/rl-train/wenhaoli/spotlight/train.py:85)
[rank0]:W1223 02:11:15.080000 2137547 site-packages/torch/_dynamo/convert_frame.py:1016] [1/8]    last reason: 1/7: ___as_tensor(___stack0).item() == 62.6761474609375  # (unknown source ___as_tensor(___stack0).item(), please file a bug)
[rank0]:W1223 02:11:15.080000 2137547 site-packages/torch/_dynamo/convert_frame.py:1016] [1/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank0]:W1223 02:11:15.080000 2137547 site-packages/torch/_dynamo/convert_frame.py:1016] [1/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
layer:     0 | step:     8 | loss: 7.873 | thresh: 0.000 | sparse: 0.001 | tacc: 0.552 | oacc: 0.466
layer:     0 | step:     9 | loss: 7.873 | thresh: 0.000 | sparse: 0.001 | tacc: 0.553 | oacc: 0.466
layer:     0 | step:    10 | loss: 7.938 | thresh: 0.000 | sparse: 0.001 | tacc: 0.551 | oacc: 0.466
layer:     0 | step:    11 | loss: 7.936 | thresh: 0.000 | sparse: 0.001 | tacc: 0.552 | oacc: 0.466
layer:     0 | step:    12 | loss: 7.857 | thresh: 0.000 | sparse: 0.001 | tacc: 0.557 | oacc: 0.468
layer:     0 | step:    13 | loss: 7.838 | thresh: 0.000 | sparse: 0.001 | tacc: 0.554 | oacc: 0.465
layer:     0 | step:    14 | loss: 7.839 | thresh: -0.000 | sparse: 0.001 | tacc: 0.557 | oacc: 0.467
layer:     0 | step:    15 | loss: 7.831 | thresh: -0.001 | sparse: 0.001 | tacc: 0.556 | oacc: 0.467
layer:     0 | step:    16 | loss: 7.791 | thresh: -0.001 | sparse: 0.001 | tacc: 0.557 | oacc: 0.466
layer:     0 | step:    17 | loss: 7.846 | thresh: -0.002 | sparse: 0.001 | tacc: 0.556 | oacc: 0.467
layer:     0 | step:    18 | loss: 7.885 | thresh: -0.004 | sparse: 0.001 | tacc: 0.555 | oacc: 0.467
layer:     0 | step:    19 | loss: 7.880 | thresh: -0.005 | sparse: 0.001 | tacc: 0.555 | oacc: 0.467
layer:     0 | step:    20 | loss: 7.839 | thresh: -0.005 | sparse: 0.001 | tacc: 0.560 | oacc: 0.468
layer:     0 | step:    21 | loss: 7.808 | thresh: -0.005 | sparse: 0.001 | tacc: 0.553 | oacc: 0.463
layer:     0 | step:    22 | loss: 7.870 | thresh: -0.001 | sparse: 0.001 | tacc: 0.556 | oacc: 0.467
layer:     0 | step:    23 | loss: 7.840 | thresh: 0.006 | sparse: 0.001 | tacc: 0.559 | oacc: 0.468
layer:     0 | step:    24 | loss: 7.833 | thresh: 0.016 | sparse: 0.001 | tacc: 0.555 | oacc: 0.467
layer:     0 | step:    25 | loss: 7.850 | thresh: 0.030 | sparse: 0.001 | tacc: 0.556 | oacc: 0.468
layer:     0 | step:    26 | loss: 7.812 | thresh: 0.042 | sparse: 0.001 | tacc: 0.558 | oacc: 0.467
layer:     0 | step:    27 | loss: 7.805 | thresh: 0.051 | sparse: 0.001 | tacc: 0.562 | oacc: 0.470
layer:     0 | step:    28 | loss: 7.742 | thresh: 0.067 | sparse: 0.001 | tacc: 0.561 | oacc: 0.470
layer:     0 | step:    29 | loss: 7.771 | thresh: 0.082 | sparse: 0.001 | tacc: 0.564 | oacc: 0.472
layer:     0 | step:    30 | loss: 7.707 | thresh: 0.094 | sparse: 0.001 | tacc: 0.568 | oacc: 0.470
layer:     0 | step:    31 | loss: 7.788 | thresh: 0.106 | sparse: 0.001 | tacc: 0.559 | oacc: 0.468
layer:     0 | step:    32 | loss: 7.751 | thresh: 0.115 | sparse: 0.001 | tacc: 0.563 | oacc: 0.469
Traceback (most recent call last):
  File "/pfs/rl-train/wenhaoli/spotlight/train.py", line 515, in <module>
    train(args)
  File "/pfs/rl-train/wenhaoli/spotlight/train.py", line 390, in train
    _, extra_rets = layer(
                    ^^^^^^
  File "/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/rl-train/wenhaoli/spotlight/spotlight/monkey_patches/hash_train.py", line 258, in attention_forward
    k_hash, _ = self.key_hash(key_states)
                ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/rl-train/wenhaoli/spotlight/spotlight/monkey_patches/utils/hash_utils.py", line 107, in forward
    if self.return_thresh:
            ^^^^^^^^^^^
  File "/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/nn/modules/container.py", line 244, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/rl-train/wenhaoli/spotlight/spotlight/monkey_patches/utils/hash_utils.py", line 75, in forward
    out = out + x
                ^^
  File "/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/functional.py", line 422, in einsum
    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/pfs/rl-train/wenhaoli/spotlight/train.py", line 515, in <module>
[rank0]:     train(args)
[rank0]:   File "/pfs/rl-train/wenhaoli/spotlight/train.py", line 390, in train
[rank0]:     _, extra_rets = layer(
[rank0]:                     ^^^^^^
[rank0]:   File "/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/pfs/rl-train/wenhaoli/spotlight/spotlight/monkey_patches/hash_train.py", line 258, in attention_forward
[rank0]:     k_hash, _ = self.key_hash(key_states)
[rank0]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/pfs/rl-train/wenhaoli/spotlight/spotlight/monkey_patches/utils/hash_utils.py", line 107, in forward
[rank0]:     if self.return_thresh:
[rank0]:             ^^^^^^^^^^^
[rank0]:   File "/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/nn/modules/container.py", line 244, in forward
[rank0]:     input = module(input)
[rank0]:             ^^^^^^^^^^^^^
[rank0]:   File "/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/pfs/rl-train/wenhaoli/spotlight/spotlight/monkey_patches/utils/hash_utils.py", line 75, in forward
[rank0]:     out = out + x
[rank0]:                 ^^
[rank0]:   File "/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/functional.py", line 422, in einsum
[rank0]:     return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: KeyboardInterrupt
