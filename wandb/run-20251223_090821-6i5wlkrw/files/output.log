/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:11<00:00,  3.78s/it]
RANK-0 training started !
/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:04<00:00,  1.65s/it]
  0%|                                                                                                                        | 0/16 [00:00<?, ?it/s]/pfs/rl-train/wenhaoli/spotlight/train.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  length = torch.tensor(length, dtype=torch.int64, device=local_rank)
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [01:15<00:00,  4.75s/it]
✅
/pfs/rl-train/wenhaoli/miniconda3/envs/mirorl-v2-copy/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1481: UserWarning: Dynamo does not know how to trace the builtin `_thread.allocate_lock.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
  torch._dynamo.utils.warn_once(explanation + "\n" + "\n".join(hints))
[rank0]:W1223 09:12:10.455000 3583538 site-packages/torch/_dynamo/variables/tensor.py:1047] [3/0] Graph break from `Tensor.item()`, consider setting:
[rank0]:W1223 09:12:10.455000 3583538 site-packages/torch/_dynamo/variables/tensor.py:1047] [3/0]     torch._dynamo.config.capture_scalar_outputs = True
[rank0]:W1223 09:12:10.455000 3583538 site-packages/torch/_dynamo/variables/tensor.py:1047] [3/0] or:
[rank0]:W1223 09:12:10.455000 3583538 site-packages/torch/_dynamo/variables/tensor.py:1047] [3/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1
[rank0]:W1223 09:12:10.455000 3583538 site-packages/torch/_dynamo/variables/tensor.py:1047] [3/0] to include these operations in the captured graph.
[rank0]:W1223 09:12:10.455000 3583538 site-packages/torch/_dynamo/variables/tensor.py:1047] [3/0]
[rank0]:W1223 09:12:10.455000 3583538 site-packages/torch/_dynamo/variables/tensor.py:1047] [3/0] Graph break: from user code at:
[rank0]:W1223 09:12:10.455000 3583538 site-packages/torch/_dynamo/variables/tensor.py:1047] [3/0]   File "/pfs/rl-train/wenhaoli/spotlight/train.py", line 93, in torch_dynamo_resume_in_compute_attn_supervise_loss_at_74
[rank0]:W1223 09:12:10.455000 3583538 site-packages/torch/_dynamo/variables/tensor.py:1047] [3/0]
[rank0]:W1223 09:12:10.455000 3583538 site-packages/torch/_dynamo/variables/tensor.py:1047] [3/0]
