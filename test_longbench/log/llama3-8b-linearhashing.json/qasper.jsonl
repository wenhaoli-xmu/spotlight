{"pred": " The ground truth for fake news is established by the number of retweets. The more retweets a tweet has, the more likely it is to be fake news.", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": " The GhostVLAD approach is a pooling strategy that uses Ghost clusters to aggregate frame-level features into a single utterance level feature. The Ghost clusters are used to absorb any unwanted features and improve the performance of language identification.\n\nQuestion: What is the i-vector+svm approach?\n\nAnswer: The i-vector approach is a language identification approach that uses i-vectors to aggregate frame-level features into a single utterance level feature. The i-vector approach is trained using a GMM-UBM model and is used to improve the performance of language identification.\n\nQuestion: What is the NetVLAD approach?\n\nAnswer: The NetVLAD approach is a", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": " The proposed model outperforms the previous best results by 68.8% to 71.8% when applied to the IEMOCAP dataset.\n\nQuestion: What is the weighted average precision of the proposed model?\n\nAnswer:: The proposed model outperforms the previous best results by 68.8% to 71.8% when applied to the IEMOCAP dataset.\n\nQuestion: What is the weighted average precision of the proposed model?\n\nAnswer:: The proposed model outperforms the previous best results by 68.8% to 71.8% when applied to the IEMOCAP dataset.\n\n", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": " The additional features and context are proposed are the context tweets. The context tweets are the data that the user has quoted or replied to. The context tweets are used to improve the accuracy of the classification of the user's data.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": " They looked at the pages of Time, The Guardian, and Disney.", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": " Yes\n\nQuestion: What is the current state-of-the-art method for hashtag segmentation?\n\nAnswer: The current state-of-the-art method for hashtag segmentation is the rule-based segmenter from BIBREF13, which uses a set of word shape rules to predict whether a space should be inserted at each position in the hashtag and is the current state-of-the-art BIBREF14. It is trained on the STAN INLINEFORM0 training set. We reimplement the Word Breaker algorithm as described in § SECREF2 and use it to extract the best candidate segmentations. The original segmentation is the top INLINEFORM1 candidate, and the", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": " The proposed task is concept-map-based multi-document summarization (CMDS), the summarization of a document cluster in the form of a concept map. The task is extractive, meaning that the labels are extracted from the documents and the resulting graph is a consensus of workers. The task is extractive, meaning that the labels are extracted from the documents and the resulting graph is a consensus of workers.", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": " We used the summarization datasets from CNN/DailyMail, NYT, and XSum.", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": " The proposed approach is a variant of the max-margin approach which learns word embedding distributions using a variant of the product of gaussian approximation method. The proposed approach is compared with the word2vec approaches such as w2g and w2gm which are based on single and multiple Gaussian representations respectively. The proposed approach is also compared with the w2g and w2gm approaches which are based on the expected likelihood kernel and KL divergence energy function respectively. The proposed approach is also compared with the w2g and w2gm approaches which are based on the expected likelihood kernel and KL divergence energy function respectively. The proposed approach is also compared with the", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": " We are trying to provide a more detailed description of the model in the following section. The ensemble is formed by simply averaging the predictions from the constituent single models. We start with the best performing model according to validation performance. Then in each step we try adding the best performing model that had not been previously tried. We keep it in the ensemble if it improves the performance and discard it otherwise. This way we gradually build up the ensemble. We call this method a greedy ensemble. We used the greedy ensemble algorithm to select the best performing model from the 67 models we trained. We call the resulting model a greedy ensemble.", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": " The Friends dataset is from the TV sitcom. The EmotionPush dataset is from chat-based data.", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": " English\n\nQuestion: what is the main idea of this paper?\n\nAnswer: : We propose to adapt the NMT model for text simplification with simplified data.\n\nQuestion: what is the main idea of this paper?\n\nAnswer: : We propose to adapt the NMT model with synthetic data during training.\n\nQuestion: what is the main idea of this paper?\n\nAnswer: : We propose to adapt the NMT model with synthetic data during training.\n\nQuestion: what is the main idea of this paper?\n\nAnswer: : We propose to adapt the NTS system with synthetic data during training.\n\nQuestion: what is the main idea of this paper?\n\nAnswer", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": " The authors use the IMDb dataset of movie reviews for the downstream experiments. The GMB dataset is used for NER. It is a balanced dataset, with 9 main labels and 2 context tags. The rest of this work is not task-specific. Hence, the choice of the dataset is not important. However, the choice of the dataset for the GMB dataset is not a good choice for the dataset.", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": " 0.0\n\nQuestion: What is the baseline system?\n\nAnswer: 0.0\n\nQuestion: What is the baseline system?\n\nAnswer: 0.0\n\nQuestion: What is the baseline system?\n\nAnswer: 0.0\n\nQuestion: What is the baseline system?\n\nAnswer: 0.0\n\nQuestion: What is the baseline system?\n\nAnswer: 0.0\n\nQuestion: What is the baseline system?\n\nAnswer: 0.0\n\nQuestion: What is the baseline system?\n\nAnswer: 0.0\n\nQuestion: What is the baseline system?\n\nAnswer: 0.0\n\nQuestion: What", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": " Yes\n\nQuestion: What is the main difference between ZuCo 1.0 and ZuCo 2.0?\n\nAnswer: The ZuCo 1.0 and ZuCo 2.0 datasets are both based on the same Wikipedia sentences, but the recording procedures are different. The main difference is that ZuCo 1.0 was recorded in two sessions, while ZuCo 2.0 was recorded in a single session. In ZuCo 1.0 the participants were asked to read the sentences naturally, without any specific task, while in ZuCo 2.0 they were instructed to read the sentences with a specific", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": " The datasets used in this article are the following:\n- The first one is the chatbot's utterances, which are the utterances that the chatbots will say to the simulated users. The second one is the chatbot's responses to the utterances, which are the utterances that the simulated users will say to the chatbots. The third one is the chatbot's utterances that are not responses to the simulated users' utterances, but are used to validate the chatbot's utterances. The fourth one is the chatbot's utterances that are not responses to the simulated users' utterances, but are used to", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": " The best performance was achieved by the Consumer Discretionary sector, which is a sector that includes stocks of companies that produce goods and services that are not considered essential to the economy, such as entertainment, clothing, and food.\n\nQuestion: Which stock market sector achieved the worst performance?\n\nAnswer: The worst performance was achieved by the Energy sector, which includes stocks of companies that produce goods and services that are not considered essential to the economy, such as entertainment, clothing, and food.\n\nQuestion: Which stock market sector achieved the best performance?\n\nAnswer: The best performance was achieved by the Consumer Discretionary sector, which is a sector that includes", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": " They compared the performance of the RNN-based NMT model and Transformer based NMT model on the Test set. The results of the RNN NMT model are shown in Table TABREF34. The results of Transformer NMT model are shown in Table TABREF35.\n\nQuestion: What is the difference between the proposed method and previous methods?\n\nAnswer: The proposed method combines both lexical-based and statistical-based information to measure the alignment score. The statistical-based information is based on the assumption of BIBREF11 that each character in one language gives rise to a random number of characters in the other language. The proposed method is a way of", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": " The three regularization terms are: (1) a regularization term associated with neutral features; (2) the maximum entropy regularization term; and (3) the KL divergence between reference and predicted class distribution.\n\nQuestion: What is the most common features?\n\nAnswer: The most common features are the most frequent words after removing stop words.\n\nQuestion: What is the neutral features?\n\nAnswer: The neutral features are the most common features.\n\nQuestion: What is the reference distribution of the neutral features?\n\nAnswer: The reference distribution of the neutral features is the uniform distribution.\n\nQuestion: What is the reference distribution of the labeled features?\n\nAnswer: The reference distribution of", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": " We pit our model against the following baselines: 1) SVM with unigram, bigram, and trigram features; 2) SVM with average word embedding; 3) SVM with transformed word embedding; 4) CNN and 5) RCNN; 6) CNN and RCNN with comment information; 7) UTCNN without user information; 8) UTCNN without user information; 9) UTCNN without comment information; 10) UTCNN without topic information; 11) UTCNN without comment information; 12) UTCNN without user information; 13) UTCNN without comment information", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": " They improved the performance by 0.5 points.\n\nQuestion: What is the best performance?\n\nAnswer:: The best performance is 0.5 points.\n\nQuestion: What is the performance of the baseline systems?\n\nAnswer: The baseline systems perform at 0.5 points.\n\nQuestion: What is the performance of the baseline systems?\n\nAnswer: The baseline systems perform at 0.5 points.\n\nQuestion: What is the best performance?\n\nAnswer: The best performance is 0.5 points.\n\nQuestion: What is the best performance?\n\nAnswer: The best performance is 0.5 points.\n\nQuestion: What is the best performance?\n\n", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": " The sparsity of the attention weights is a key feature of the attention mechanism, and it is also a key feature of the attention mechanism. The $\\alpha$-entmax function is a continuous function that is a continuous function of the attention mechanism. The $\\alpha$-entmax function is a continuous function of the attention mechanism. The $\\alpha$-entmax function is a continuous function of the attention mechanism. The $\\alpha$-entmax function is a continuous function of the attention mechanism. The $\\alpha$-entmax function is a continuous function of the attention mechanism. The $\\alpha$-entmax function is a", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": " The baseline model is a standard Transformer model trained on 30m instances of 4-sentence fragments.", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": " We evaluate our models on two zero-shot cross-lingual transfer tasks: XNLI and dependency parsing. For XNLI, we use the same evaluation protocol as the work of BIBREF6. We use the same evaluation protocol as the work of BIBREF6. For XNLI, we use the same evaluation protocol as the work of BIBREF6. We use the same evaluation protocol as the work of BIBREF6. We use the same evaluation protocol as the work of BIBREF6. We use the same evaluation protocol as the work of BIBREF6. We use the same evaluation protocol", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": " The attention module is not pretrained on the ASR and MT data. It is initialized from the attention module in the ST model, which is trained on the ASR and MT tasks.", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": " (a) Lexical features (b) Implicit incongruity features (c) Explicit incongruity features (d) Readability features (e) All of the above\n\nQuestion: What is the main difference between the two feature sets?\n\nAnswer: (a) The number of features (b) The number of participants (c) The type of features (d) The way features are extracted (e) The way features are used\n\nQuestion: What is the motivation for considering eye-tracking parameters as features?\n\nAnswer: (a) To reduce the effect of text hardness (b) To reduce the effect of text length (c)", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": " The architecture of the encoder is a LSTM.\n\nQuestion: What is the baseline for the CoNLL–SIGMORPHON 2018 shared task on Universal Morphological Reinflection, Task 2?\n\nAnswer: The baseline is a character-based encoder-decoder model.\n\nQuestion: What is the effect of the three augmentations on the performance of the system?\n\nAnswer: The effect of the three augmentations is that the performance of the system is improved.\n\nQuestion: What is the effect of the three augmentations on the performance of the system?\n\nAnswer: The effect of the three augmentations is that the performance of the system is", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": " yes\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: yes\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: yes\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: yes\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: yes\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: yes\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: yes\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: yes\n\n", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": " The baselines are wav2letter and wav2letter's 1D-convolution layers. The answer is that the baselines are the same as the ones used in the previous work.\n\nQuestion: what is the main contribution of the article?\n\nAnswer: The main contribution of the article is to show that the new architecture is more efficient than the previous ones.\n\nQuestion: what is the architecture of the new architecture?\n\nAnswer: The architecture of the new architecture is a convolutional architecture with a new activation function.\n\nQuestion: what is the activation function of the new architecture?\n\nAnswer: The activation function of the new architecture is a 1", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": " 20,000\n\nQuestion: What is the average number of words per post?\n\nAnswer: 1,000\n\nQuestion: What is the average number of words per post?\n\nAnswer: 1,000\n\nQuestion: What is the average number of words per post?\n\nAnswer: 1,000\n\nQuestion: What is the average number of words per post?\n\nAnswer: 1,000\n\nQuestion: What is the average number of words per post?\n\nAnswer: 1,000\n\nQuestion: What is the average number of words per post?\n\nAnswer: 1,000\n\nQuestion: What is the average number of words", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": " We use the following metrics to evaluate our models: (1) BLEU-1/2/4, (2) Distinct-1/2, (3) Perplexity, (4) Recipe Level Coherence, (5) Recipe Step Entailment, (6) Recipe Step Diversity, (7) Recipe Step Diversity, (8) Recipe Step Diversity, (9) Recipe Step Diversity, (10) Recipe Step Diversity, (11) Recipe Step Diversity, (12) Recipe Step Diversity, (13) Recipe Step Diversity, (14) Recipe Step Diversity, (15) Recipe Step Diversity, (16", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": " The labels are the linguistic expressions of the symptom and attribute. For example, the label of the question \"What is the patient's symptom?\" is \"No Answer\" if the patient does not mention the symptom \"cough\" in the response. The labels are the linguistic expressions of the attribute \"cough\" in the question \"What is the patient's symptom?\".\n\nQuestion: What is the motivation of the authors' simulation approach?\n\nAnswer: The model works on a simulated dataset, which is linguistically characterized by the authors. The model is trained on the simulated data and tested on the real-world conversations between nurses and patients.\n\nQuestion:", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": " The model was trained on 9/10ths of the data and the rest of the data was predicted to be difficult. The model was trained on 9/10ths of the data and the rest of the data was predicted to be difficult. The model was trained on 9/10ths of the data and the rest of the data was predicted to be difficult. The model was trained on 9/10ths of the data and the rest of the data was predicted to be difficult. The model was trained on 9/10ths of the data and the rest of the data was predicted to be difficult. The model was", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": " We evaluate our models on four machine translation tasks.\n\nQuestion: What is the main contribution of this work?\n\nAnswer: We introduce a novel strategy for adaptively sparse attention, where each head can specialize more and with higher confidence.\n\nQuestion: What is the @!START@$\\alpha $@!END@-entmax function?\n\nAnswer: The $\\alpha $-entmax function is a generalization of the entmax function, which is defined as\nwhere $\\alpha $ is a scalar parameter that controls the shape of the mapping. For $\\alpha =1$, the $\\alpha $-entmax function is the same as the softmax function", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": " The improvement in performance for the contextual embeddings in the NER task is 0.03 for the original English ELMo model and 0.04 for the ELMo embeddings.", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": " What background do they have?\n\nQuestion: What is the goal of the article?\n\nAnswer: What is the goal of the article?\n\nQuestion: What is the goal of the article?\n\nAnswer: What is the goal of the article?\n\nQuestion: What is the goal of the article?\n\nAnswer: What is the goal of the article?\n\nQuestion: What is the goal of the article?\n\nAnswer: What is the goal of the article?\n\nQuestion: What is the goal of the article?\n\nAnswer: What is the goal of the article?\n\nQuestion: What is the goal of the article?\n\nAnswer: What is the goal of the article?\n\nQuestion:", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": " No, the paper is about supervised learning. The authors use the LDA model as a feature extractor, and then use the extracted features to train a classifier. The authors do not use the LDA model to directly detect spammers.\n\nQuestion: What is the difference between the topic-based features and the raw topic distribution?\n\nAnswer: The raw topic distribution is the probability of a user's interest on a certain topic. The raw topic distribution is the probability of a user's interest on a certain topic. The raw topic distribution is the probability of a user's interest on a certain topic. The raw topic distribution is the probability of a user's", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": " The Nguni languages are similar to each other and the Sotho languages are similar to each other.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": " They compared the performance of the 2-layer LSTM model with the performance of the 9-layer model. The 2-layer model was distilled from the 9-layer model, and it was found that the recognition accuracy of the distilled model was almost the same as the original model. The 2-layer model was also trained with 17000 hours of data, and it was found that the recognition accuracy of the distilled model was almost the same as the original model. The 2-layer model was also trained with 17000 hours of data, and it was found that the recognition accuracy of the distilled model was almost the same as the ", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": " The Wikipedia dataset consists of articles from English Wikipedia, with quality class labels assigned by the Wikipedia community. The quality class of a Wikipedia article is assigned by Wikipedia reviewers or any registered user, who can discuss through the article's talk page to reach consensus. The arXiv dataset consists of academic articles from the three subject areas of: Artificial Intelligence (cs.ai), Computation and Language (cs.cl), and Machine Learning (cs.lg). The number of articles in each quality class is summarized in Table 1. The number of papers in each quality class is summarized in Table 3. The number of papers in each quality class is summarized", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": " The human evaluation was carried out by a group of 50 native people who were well-versed in both English and Tamil languages. The group was split into 2 groups of 25 people each, one for the RNNSearch model and the other for the RNNMorph model. The group was given a set of 100 sentences from the test set and asked to rank the translations of the two models. The ranking was done based on the extent of fluency and adequacy of the translations. The ranking was done based on the BLEU metric of the models. The ranking was done based on the extent fluency of the translations", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": " No, they do not test their framework performance on commonly used language pairs, such as English-to-German.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": " We evaluate our models by measuring the accuracy of the communication schemes. The accuracy of a communication scheme is measured as the fraction of sentences generated by the model that exactly match the target sentence. The accuracy of a communication scheme is measured as the fraction of sentences generated by the system that are exactly the same as the target sentence.", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": " Precision, Recall and F-measure are used for evaluating the performance of the ILP-based algorithm.", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": " The source domain is the Amazon dataset, and the target domain is the target dataset.\n\nQuestion: What is the main idea of the proposed method?\n\nAnswer: The proposed method is based on the idea of using a neural network to learn feature representations that are not only domain-invariant but also discriminative.\n\nQuestion: What is the main idea of the proposed method?\n\nAnswer: The main idea of the proposed method is to use a neural network to learn feature representations that are not only domain-invariant but also discriminative.\n\nQuestion: What is the main idea of the proposed method?\n\nAnswer: The main idea of the proposed method is to use a", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": " They compare with the state-of-the-art models, which are the LSTM and the PRU. The LSTM is a special case of PRU when INLINEFORM0 = INLINEFORM1 =1. The authors compare the performance of the PRU with the LSTM and the LSTM with the state-of-the-art methods.", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": " The following are the most commonly used modules in NeuronBlocks:", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": " The authors used the Wiktionary pronunciation data for 229 languages. They also used the Phoible and URIEL data for the languages they used for their baseline models. The training data for the source encoder–decoder models is the same as the training data for the wFST models, but the training data for the wFST models is the same as the training data for the source encoder–decoder models. The test data for the source encoder–decoder models is the same as the test data for the source encoder–decoder models. The test data for the source encoder–decoder models is the same as the test data for the", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": " Speculation Cue Detection: Task 1B CoNLL: 0.52, Speculation Cue Detection: Task 2 CoNLL: 0.52, Speculation Cue Detection: Task 1B SFU: 0.52, Speculation Cue Detection: Task 1W SFU: 0.52, Speculation Cue Detection: Task 2 SFU: 0.52, Speculation Cue Detection: Task 1W SFU: 0.52, Speculation Cue Detection: Speculation Cue Detection: Task 1W SFU: 0.52, Speculation Cue Detection", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": " They use English and Spanish in their experiments.\n\nQuestion: What are the main findings of the paper?\n\nAnswer: The main findings of the paper are that the training variants of each training set are not the same as the test sets. The training set is in English, but the test set is in Spanish. The test set is created through translation, and the training set is created through back-translation. The training set is also created through machine translation, and the test set is also created through machine translation. The training set is also created through the Translate-Test approach, and the test set is also created through the Translate-Test approach.", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": " We also study the effect of our model on the performance of the word based approach, and the results are shown in Table 3. We also study the effect of our model on the performance of the word based approach, and the results are shown in Table 3. We also study the effect of our model on the performance of the word based approach, and the results are shown in Table 3. We also study the effect of our model on the performance of the word based approach, and the results are shown in Table 3. We also study the effect of our model on the performance of the word based approach, and the results", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": " Yes, they use 300 dimensional Glove embeddings.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": " unanswerable\n\nQuestion: What is the main idea of the paper?\n\nAnswer: unanswerable\n\nQuestion: What is the main idea of the paper?\n\nAnswer: unanswerable\n\nQuestion: What is the main idea of the paper?\n\nAnswer: unanswerable\n\nQuestion: What is the main idea of the paper?\n\nAnswer: unanswerable\n\nQuestion: What is the main idea of the paper?\n\nAnswer: unanswerable\n\nQuestion: What is the main idea of the paper?\n\nAnswer: unanswerable\n\nQuestion: What is the main idea of the paper?\n\nAnswer: unanswerable\n\nQuestion: What is the", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": " They use the information in the dataset to generate maps of the geographical distributions of psycholinguistic and semantic word classes.", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": " The ML methods aim to identify claims, premises, backing, and rebuttal.\n\nQuestion: What is the best-performing system for identifying argument components?\n\nAnswer: The best-performing system is the SVMhmm with the feature set 01234.\n\nQuestion: What is the best-performing system for identifying argument components?\n\nAnswer: The best-performing system is the SVMhmm with the feature set 01234.\n\nQuestion: What is the best-performing system for identifying argument components?\n\nAnswer: The best-performing system is the SVMhmm with the feature set 01234.\n\nQuestion: What is the best-performing system for", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "  The n-grams in the generated text are aligned to the n-grams in the reference and the table using a word-overlap model. PARENT uses the n-grams of order 1, 2, 3 and 4.", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": " The Twitter dataset is a collection of 1,873 conversation threads, roughly 14,000 comments, and 14,000 comments.", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": " eng, fra, spa, ita, por, ger, cze, pol, rus, fin, swe, dan, nor\n\nQuestion: What are the 12 languages covered?\n\nAnswer: eng, fra, spa, ita, por, ger, cze, pol, rus, fin, swe, dan, nor\n\nQuestion: What are the 12 languages covered?\n\nAnswer: eng, fra, spa, ita, por, ger, cze, pol, swe, dan, nor, yue\n\nQuestion: What are the 12 languages covered?\n\nAnswer: eng, fra, spa, ita,", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": " The Wikipedia and CMV datasets.\n\nQuestion: What is the main difference between the Wikipedia and CMV datasets?\n\nAnswer: The Wikipedia dataset is a collection of conversations that are labeled according to whether they eventually lead to personal attacks, while the CMV dataset is a collection of conversations that are labeled according to whether they eventually lead to moderator action.\n\nQuestion: What is the main difference between the Wikipedia and CMV datasets?\n\nAnswer: The Wikipedia dataset is a collection of conversations that are labeled according to whether they eventually lead to personal attacks, while the CMV dataset is a collection of conversations that are labeled according to whether they eventually lead to moderator action", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": " unanswerable\n\nQuestion: What is the name of the ontology used in the article?\n\nAnswer: : criminal law\n\nQuestion: What is the name of the project?\n\nAnswer: : Agatha\n\nQuestion: What is the name of the ontology?\n\nAnswer: : criminal law\n\nQuestion: What is the name of the project?\n\nAnswer: : Agatha\n\nQuestion: What is the name of the project?\n\nAnswer: : criminal law\n\nQuestion: What is the name of the ontology?\n\nAnswer: : Agatha\n\nQuestion: What is the name of the project?\n\nAnswer: : Agatha\n\nQuestion: What is the name of the", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": " We introduce a multilingual ST corpus for 11 languages into English, diversified with over 11,000 speakers and over 60 accents. It is licensed under CC0 license and free to use. We also provide a many-to-one multilingual baseline for the first time. The many-to-one multilingual models are trained on the CoVoST corpus. The quality of the many-to-one multilingual models is measured by the BLEU scores on the CoVoST corpus. The CoVoST corpus is licensed under CC0 license and free to use. The many-to-one multilingual models are trained on the CoVoST corpus. The", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": " They use two RNNs, one for audio and one for text. The audio RNN is trained to predict the emotion class of the audio sequence, and the text RNN is trained to predict the emotion class of the text sequence. The two RNNs are then combined using a fully connected neural network to form the final prediction.", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": " 1.7\n\nExplanation: The results of our method (NMT+synthetic) on the two datasets are shown in Table 1 and Table 2. We see substantial improvements (6.37 BLEU) than NMT from adding simplified training data with synthetic ordinary sentences. Compared with the baselines, the results of our method (NMT+synthetic) are also substantially better than the baselines, which indicates that our method is effective at improving the quality of NMT systems.", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "  unanswerable\n\nQuestion: how many of the cases of the DocRepair model are the ones where the DocRepair translation is not a full copy of the baseline translation?\n\nAnswer:  unanswerable\n\nQuestion: how many sentences in the general test set are changed by the DocRepair model?\n\nAnswer:  unanswerable\n\nQuestion: how many of the cases where the DocRepair model is not a full copy of the baseline context-agnostic translation?\n\nAnswer:  unanswerable\n\nQuestion: how many of the cases where the DocRepair translation is not a full copy of the baseline translation?\n\nAnswer: ", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": " 1.10pt\n[ Characterize accounts spreading fake news by looking at their meta-data. Specifically, we found that the average time of exposure of viral tweets containing fake news is lower than that of other tweets. We also found that the average number of URLs in viral tweets containing fake news is higher than that of other viral tweets. Finally, we found that the average number of URLs in viral tweets containing fake news is lower than that of other viral tweets.", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": " The best performing architecture is LSTM-CRF with BERT, where we jointly perform FLC and SLC. The best performing architecture is CNN, however, when we introduce the additional features, we observe that the scores are not significantly different from the best performing system.", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": " The answer is based on the following question: What are the methods used for question entailment in the medical domain? The answer is based on the following question: What are the methods used for question entailment in the medical domain? The answer is based on the following question: What are the methods used for question number 36 in the TREC-2017 LiveQA medical test dataset? The answer is based on the following question: What are the methods used for question number 36 in the TREC-2017 LiveQA medical test dataset? The answer is based on the following question: What are the methods used for question number ", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": " The benchmark dataset is the Honeypot dataset and the Weibo dataset. The quality of the dataset is high.\n\nQuestion: What is the main difference between the two categories of spammers?\n\nAnswer: The main difference between the two categories of spammers is that the spammers are more likely to be interested in certain topics while the legitimate users are more interested in a wide range of topics.\n\nQuestion: What is the LOSS feature and how does it work?\n\nAnswer: The LOSS feature is the Local Outlier Standard Score, which captures the user's interests on different topics. It is calculated as Eq. ( 20 ):\n$$\\center", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": " The CoNLL–SIGMORPHON 2018 baseline is described as follows: The CoNLL–SIGMORPHON 2018 baseline is described as follows: The CoNLL–SIGMORPHON 2018 baseline is described as follows: The CoNLL–SIGMORPHON 2018 baseline is described as follows: The CoNLL–SIGMORPHON 2018 baseline is described as follows: The CoNLL–SIGMORPHON 2018 baseline is described as follows: The CoNLL–SIGMORPHON 2018 baseline", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": " No, they report results on three datasets: FSD (social media), Twitter (social media) and Google (news media). The first two are English, the last one is Chinese.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": " The best performing model is the model that is used in the final submission to the task. The best performing model is the one that is ranked at 4th position in the task of SLC. The best performing model is the one that is ranked at 3rd position in the task of FLC.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": " The baseline was the best model that we could obtain using only in-domain data. The baseline was the M2M Transformer model trained on the largest out-of-domain data for the language pair, i.e., Ja INLINEFORM0 Ru.", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": " 0.7033\n\nQuestion: What was their highest precision score?\n\nAnswer: 0.3333\n\nQuestion: What was their highest recall score for the Yes/No question?\n\nAnswer: 0.3333\n\nQuestion: What was their highest precision score for the Yes/No question?\n\nAnswer: 0.3333\n\nQuestion: What was their highest recall score for the List type question?\n\nAnswer: 0.3333\n\nQuestion: What was their highest precision score for the List type question?\n\nAnswer: 0.3333\n\nQuestion: What was their highest recall score for the List type question?\n\nAnswer", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": " The embedding techniques explored in the paper are based on second–order co–occurrence vectors. The goal of these techniques is to measure the relatedness between two terms by integrating pair–wise similarity scores derived from a taxonomy into second–order co–occurrence vectors.", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": " They match the words in the assisting language to the words in the source language.", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": " No\n\nQuestion: Does the paper explore extraction of entities from biomedical literature?\n\nAnswer:: Yes\n\nQuestion: Does the paper explore extraction of entities from clinical text?\n\nAnswer: Yes\n\nQuestion: Does the paper explore extraction of entities from electronic health records?\n\nAnswer: No\n\nQuestion: Does the paper explore extraction of entities from electronic health records?\n\nAnswer: No\n\nQuestion: Does the paper explore extraction of entities from clinical text?\n\nQuestion: Does the paper explore extraction of entities from clinical text?\n\nAnswer: No\n\nQuestion: Does the paper explore extraction of entities from electronic health records?\n\nAnswer: No\n\nQuestion: Does the paper explore extraction", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": " We recruit seven legal experts to annotate questions asked by users to the privacy policy. Each question is annotated by at least two experts. The dataset is partitioned into 100 instances for each question, with each question receiving 100 evidence sentences. This results in 7000 evidence sentences for each question. The experts are asked to identify relevant evidence for each question, as well as provide meta-annotation on the question's relevance, subjectivity, OPP-115 category, and how likely any privacy policy is to contain the answer to the question asked.", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": " The image is embedded using a CNN and the painting is embedded using a CNN-RNN model. The painting is then transformed into Shakespearean prose using a seq2seq model with global attention.", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": " The RNN layer works better.", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": " unanswerable\n\nQuestion: What is the relationship between the state-of-the-art MRC models and the general knowledge of human beings?\n\nAnswer: Explicitly using the general knowledge to assist the attention mechanisms of the MRC models.\n\nQuestion: What is the relationship between the amount of general knowledge and the performance of the state-of-the-art MRC models?\n\nAnswer: The amount of general knowledge is not the only factor that affects the performance of the state-of-the-art MRC models.\n\nQuestion: What is the relationship between the hyper-parameter and the performance of the state-of-the-art MRC models?\n\nAnswer: The hyper-parameter is not the", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": " They addressed three topics of cyberbullying: use of swear words, anonymity, and writing style.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": " They use all parts of the sentence (left context, middle context, right context) and pay special attention to the middle context.\n\nQuestion: What is the difference between the middle context and the extended middle context?\n\nAnswer: The extended middle context uses the left context, the left entity, and the middle context. The extended middle context uses the left context, the left entity, and the middle context.\n\nQuestion: What is the difference between the extended middle context and the extended middle context?\n\nAnswer: The extended middle context uses the left context, the left entity, and the middle context. The extended middle context uses the left context, the left", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": " 4\n\nQuestion: How many entities are there in the dataset?\n\nAnswer: 4\n\nQuestion: What is the name of the dataset?\n\nAnswer: 4\n\nQuestion: What is the name of the dataset?\n\nAnswer: 4\n\nQuestion: What is the name of the dataset?\n\nAnswer: 4\n\nQuestion: What is the name of the dataset?\n\nAnswer: 4\n\nQuestion: What is the name of the dataset?\n\nAnswer: 4\n\nQuestion: What is the name of the dataset?\n\nAnswer: 4\n\nQuestion: What is the name of the dataset?\n\nAnswer: 4\n\nQuestion: What is", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": " The resulting annotated data is higher quality. The results show that the F1 score is higher when the difficult examples are annotated by experts, as compared to when the same amount of data is annotated by a model that learns to predict the difficult examples. The F1 score is higher when the difficult examples are annotated by experts, as compared to when the same amount of data is annotated by a model that learns to predict the difficult examples. The results show that the F1 score is higher when the difficult examples are annotated by experts, as compared to when the same amount of data is annotated by a model that learns to predict the difficult examples. The", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": " The article describes the data used to train an automatic speech recognition system. The data is composed of four corpora of French radio and TV broadcast. The article shows that women are under-represented in these corpora, with a percentage of 33.33% of the speakers being women. The same trend is observed for the speech time, with women speaking for only 22.57% of the time. The article also shows that men are more likely to be in the role of Anchor speakers, who are the most represented in the data.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": " The authors use the Multi30K test set as the main metric for their experiments. They compare their transformer baseline to the state of the art transformer model (base) and the state of the art state of the art model (del) and show that the state of the art state of the art model is more accurate than the baseline model.\n\nWhat are some other potential good answers for the question in context?\n\nA: We use the following phrases and sentences to answer the question: The state of the art state of the art model is the state of the art state of the art state of the art state of the art state of the art state", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": " The models we compare with are the following: BIBREF2, BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF9, BIBREF11, BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF24, BIBREF25, BIBREF26, BIBREF27, BIBREF28", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": " The type of classifiers used are microposts.", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": " NLTK, which is a Python library for natural language processing, is a toolkit for named-entity recognition. It is a Python library for natural language processing, which is a toolkit for natural language processing.", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": " We conduct experiments on the SQuAD dataset BIBREF3. It contains 536 Wikipedia articles and 100k crowd-sourced question-answer pairs. We employ two different data sets splits by following Zhou2017NeuralQG and Du2017LearningTA. In Zhou2017NeuralQG, the original SQuAD development set is treated as the training set and a test set is split from the SQuAD development set. We also split the original SQuAD training set into a training set and a test set. The data sets are given in Table TABREF27.\n\nQuestion: What is the main contribution of", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": " The existing approaches are as follows:\n\n1. BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11, BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF20, BIBREF21, BIBREF22, BIBREF23", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": " unanswerable\n\nExplanation: The answer is not in the article.\n\nQuestion: Do they use attention?\n\nAnswer: yes\n\nExplanation: The answer is in the article.\n\nQuestion: Do they use attention?\n\nAnswer: yes\n\nExplanation: The answer is in the article.\n\nQuestion: Do they use attention?\n\nAnswer: yes\n\nExplanation: The answer is in the article.\n\nQuestion: Do they use attention?\n\nAnswer: no\n\nExplanation: The answer is not in the article.\n\nQuestion: Do they use attention?\n\nAnswer: no\n\nExplanation: The answer is in the article.\n\nQuestion: Do they use attention?\n\nAnswer: no\n\nExplanation:", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": " They used three datasets for evaluation: CSAT, 20newsgroups, and Fisher.\n\nQuestion: What is the baseline for the CSAT task?\n\nAnswer: The baseline for the CSAT task is the CNN model.\n\nQuestion: What is the best model for the CSAT task?\n\nAnswer: The best model for the CSAT task is the one that is trained on long documents.\n\nQuestion: What is the best model for the CSAT task?\n\nAnswer: The best model for the CSAT task is the one that is trained on long documents.\n\nQuestion: What is the best model for the CSAT task?\n\nAnswer: The best", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": " The IMDb dataset is used.\n\nQuestion: What is the size of the vocabulary used in the character-level sentiment classification task?\n\nAnswer:: 187\n\nQuestion: What is the number of layers in the QRNN used for character-level machine translation?\n\nAnswer:: 4\n\nQuestion: What is the number of layers in the character-level LSTM used for character-level machine translation?\n\nAnswer:: 4\n\nQuestion: What is the number of units in each layer of the character-level LSTM?\n\nAnswer:: 320\n\nQuestion: What is the number of units in each layer of the character-level LSTM?\n\nAnswer:: 320\n\nQuestion", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": " Yes, the BERT results are not comparable to previous work, because the numbers are not directly comparable to the numbers reported in previous work. The previous work numbers are not directly comparable to the numbers reported in previous work, because the BERT results are not directly comparable to the numbers reported in previous work. The numbers are not directly comparable to the numbers reported in previous work, because the BERT results are not directly comparable to the numbers reported in previous work. The BERT results are not directly comparable to the numbers reported in previous work, because the BERT results are not directly comparable to the numbers reported in previous work. The BERT", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": " The dataset of 1,000 tweets contains 210 positive, 521 neutral, and 305 negative sentiments to the candidates. The groundtruth labels are balanced, i.e., 210 positive, 210 negative, and 210 neutral sentiments.", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": " The neural projector with coupling function is invertible if and only if the coupling function is invertible.", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": " The proposed qualitative annotation schema is shown in Figure FIGREF1. The qualitative annotation schema is shown in Figure FIGREF1. The proposed qualitative annotation schema is shown in Figure FIGREF1. The qualitative annotation schema is shown in Figure FIGREF1. The proposed qualitative annotation schema is shown in Figure FIGREF1. The proposed qualitative annotation schema is shown in Figure FIGREF1. The proposed qualitative annotation schema is shown in Figure FIGREF1. The proposed qualitative annotation schema is shown in Figure FIGREF1. The proposed qualitative annotation schema is shown in Figure FIGREF1. The proposed qualitative annotation schema is shown in Figure FIGREF1.", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": " The sizes of both datasets are 89,042 and 296,402, respectively.\n\nQuestion: what is the metric used to evaluate the results?\n\nAnswer: The metric is FKGL.\n\nQuestion: what is the difference between the results of NMT and our method?\n\nAnswer: Our method is better than NMT.\n\nQuestion: what is the difference between the results of NMT and the results of our method?\n\nAnswer: Our method is better than NMT.\n\nQuestion: what is the difference between the results of NMT and the results of our method?\n\nAnswer: Our method is better than NMT.\n\nQuestion: what is the metric", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": " We compare our model with the following baselines.\n\nVanilla ST baseline: The ST model is trained from scratch on the ST-TED corpus. Many-to-one setting: The model is trained on ASR and MT tasks, where the ST decoder consumes the output of the ASR encoder and the MT decoder is trained on the MT task. Many-to-many setting: The model is trained on ASR and ST tasks, where the ST encoder is a concatenation of the ASR encoder and the MT encoder. The ST decoder is trained on the ST task. One-to-one setting: The model is trained on ASR and MT tasks,", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": " English\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: The main contribution of this paper is to show that the BERT model can be used to detect propaganda in sentences and that the method of incorporating cost-sensitive learning into BERT can be used to improve the performance of the model.\n\nQuestion: What is the main problem studied in this paper?\n\nAnswer: The main problem studied in this paper is propaganda detection.\n\nQuestion: What is the main problem studied in this paper?\n\nAnswer: The main problem studied in this paper is propaganda detection.\n\nQuestion: What is the main problem studied in this paper?\n\nAnswer: The main problem studied", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": " We assess our dataset using three models: a linear SVM, a bidirectional Long Short-Term Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model. The CNN model is the best one, achieving a macro-F1 score of 0.80.\n\nQuestion: What is the best model for the offensive language detection task?\n\nAnswer: The CNN model is the best one, achieving a macro-F1 score of 0.80.\n\nQuestion: What is the best model for the offensive target identification task?\n\nAnswer: The CNN model is the best one, achieving a macro-F1 score of 0", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": " No, they do not measure for the usefulness of the answer. The answered questions measure for the usefulness of the answer. The answered questions measure for the usefulness of the answer. The answered questions measure for the usefulness of the answer. The answered questions measure for the usefulness of the answer. The answered questions measure for the usefulness of the answer. The answered questions measure for the usefulness of the answer. The answered questions measure for the usefulness of the answer. The answered questions measure for the usefulness of the answer. The answered questions measure for the usefulness of the answer. The answered questions measure for the usefulness of the answer. The answered questions measure for", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": " We used pretrained word vectors from the baseline system. The word vectors are obtained by summing up word embeddings and then dividing by the number of tokens in the tweet. The word embeddings are from the system which takes the context into consideration. The word embeddings are from the system which takes the context into consideration. The word embeddings are from the system which takes the context into consideration. The word embeddings are from the system which takes the context into consideration. The word embeddings are from the system which takes the context into consideration. The word embeddings are from the system which takes the context into consideration. The word embeddings are from the system which takes the context", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": " Our personalized models achieve average recipe-level coherence scores of 1.78-1.82, surpassing the baseline at 1.77. We also observe that users with prior recipe interactions are more likely to generate recipes that are more coherent than those with no interactions.", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": " The combination of rewards for reinforcement learning is a combination of rewards for irony accuracy, sentiment preservation and content preservation.", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": " The limitation is that the generated English poem may not work well with Shakespeare-style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.\n\nQuestion: What is the average content score of the generated prose for the paintings shown in Figure FIGREF12?\n\nAnswer: The average content score is 3.7 which demonstrates that the prose generated is relevant to the painting.\n\nQuestion: What is the average style score of", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": " They compare their models to the following systems: (i) a simple bag-of-words model, (ii) a model trained with the same features but using different pages as training data, (iii) a model trained on the same features but with different pages, (iv) a model trained on the same features but with different features, (v) a model trained on the same features but with different features and different pages, (vi) a model trained on the same features and different pages, but with different features, (vii) a model trained on the same features and different pages, (viii) a model trained on", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": " The distribution of the number of followers of the accounts spreading viral tweets containing fake news is different from the distribution of the number of followers of the accounts spreading viral tweets not containing fake news.", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": " The dataset of hashtags is sourced from the Stanford Twitter Sentiment 2010 dataset. The dataset contains 12,594 unique English hashtags and their associated tweets annotated with a multi-task pairwise ranking model. The dataset is used to build the neural architectures for the task of hashtag segmentation.\n\nQuestion: What is the current state-of-the-art method for hashtag segmentation?\n\nAnswer: The current state-of-the-art method for hashtag segmentation is the Word Breaker method, which uses a beam search and a language model to rank the candidates.\n\nQuestion: What is the current state-of-the-art method for hashtag segmentation?\n\nAnswer: The current state-of-the-art method", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": " unanswerable\n\nQuestion: what is the number of speakers in the database?\n\nAnswer: : 1969\n\nQuestion: what is the number of trials in the text-dependent experiments?\n\nAnswer: : 1969\n\nQuestion: what is the number of trials in the text-prompted experiments?\n\nAnswer: : 1969\n\nQuestion: what is the number of trials in the text-dependent experiments for Persian?\n\nAnswer: : 1000\n\nQuestion: what is the number of trials in the text-dependent experiments for English?\n\nAnswer: : 1000\n\nQuestion: what is the number of trials in the text-dependent experiments for English", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": " Word subspace is a compact representation of a set of word vectors. It can represent the context of a text, that is, the words that are close to each other in the word vector space.", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": " The baseline model is used to determine the section of an article that is relevant to an entity. The baseline model is used to determine the correct section for a news article based on section templates.\n\nQuestion: What is the learning objective of the AEP task?\n\nAnswer: The learning objective of the AEP task is to determine the correct section for a news article based on the section templates.\n\nQuestion: What is the learning objective of the ASP task?\n\nAnswer: The learning objective of the ASP task is to determine the correct section for a news article based on the section templates.\n\nQuestion: What is the best fitting section for a news article?\n\nAnswer", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": " unanswerable\n\nQuestion: Is the BERT model a good model for WSD?\n\nAnswer: yes\n\nQuestion: Is the BERT model a good model for WSD?\n\nAnswer: yes\n\nQuestion: Is the BERT model a good model for WSD?\n\nAnswer: yes\n\nQuestion: Is the BERT model a good model for WSD?\n\nAnswer: yes\n\nQuestion: Is the BERT model a good model for WSD?\n\nAnswer: yes\n\nQuestion: Is the BERT model a good model for WSD?\n\nAnswer: yes\n\nQuestion: Is the BERT model a good model for WSD?\n\nAnswer:", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "  unanswerable\n\nQuestion: What is the largest language in CoVoST?\n\nAnswer:  CoVoST\n\nQuestion: What is the largest language in CoVoST?\n\nAnswer:  CoVoST\n\nQuestion: What is the largest language in CoVoST?\n\nAnswer:  CoVoST\n\nQuestion: What is the largest language in CoVoST?\n\nAnswer:  CoVoST\n\nQuestion: What is the largest language in CoVoST?\n\nAnswer:  CoVoST\n\nQuestion: What is the largest language in CoVoST?\n\nAnswer:  CoVoST\n\nQuestion:", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "  They used the SemEval-2016 dataset.\n\nQuestion: What is the name of the dataset?\n\nAnswer:  The dataset is the SemEval-2016 dataset.\n\nQuestion: What is the name of the dataset?\n\nAnswer:  The dataset is the SemEval-2016 dataset.\n\nQuestion: What is the name of the dataset?\n\nAnswer:  The dataset is the SemEval-2016 dataset.\n\nQuestion: What is the name of the dataset?\n\nAnswer:  The dataset is the SemEval-2016 dataset.\n\nQuestion: What is the name of the dataset?\n\nAnswer:  The dataset is the SemEval-201", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": " unanswerable\n\nQuestion: Do they use the pre-trained uncased BERT or the pre-trained uncased BERT?\n\nAnswer: unanswerable\n\nQuestion: Do they use the pre-trained uncased BERT or the pre-trained uncased BERT?\n\nAnswer: unanswerable\n\nQuestion: Do they use the pre-trained uncased BERT or the pre-trained uncased BERT?\n\nAnswer: unanswerable\n\nQuestion: Do they use the pre-trained uncased BERT or the pre-trained uncased BERT?\n\nAnswer: unanswerable\n\nQuestion: Do they use the pre-trained uncased BERT or the pre-trained", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": " unanswerable\n\nQuestion: Are the datasets sufficiently challenging?\n\nAnswer: unanswerable\n\nQuestion: Are the datasets subject to systematic biases?\n\nAnswer: unanswerable\n\nQuestion: Are the datasets subject to systematic biases?\n\nAnswer: unanswerable\n\nQuestion: Are the transformer-based models able to outperform the transformer-based models?\n\nAnswer: unanswerable\n\nQuestion: Are the models able to learn the meaning of definitions?\n\nAnswer: unanswerable\n\nQuestion: Are the models able to learn the meaning of synonyms?\n\nAnswer: unanswerable\n\nQuestion: Are the models able to learn the meaning of ISA relations?\n\nAnswer: unanswer", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": " unanswerable\n\nQuestion: Are the images from a specific domain?\n\nQuestion: Are the images from a specific domain?\n\nAnswer: unanswerable\n\nQuestion: Are the images from a specific domain?\n\nAnswer: unanswerable\n\nQuestion: Are the images from a specific domain?\n\nAnswer: unanswerable\n\nQuestion: Are the images from a specific domain?\n\nAnswer: unanswerable\n\nQuestion: Are the images from a specific domain?\n\nAnswer unanswerable\n\nQuestion: Are the images from a specific domain?\n\nAnswer: unanswerable\n\nQuestion: Are the images from a specific domain?\n\nAnswer: unanswerable\n\nQuestion: Are", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": " Our model's performance is compared to the following systems: (i) a simple majority baseline, which assigns a post to the emotion with the highest reaction count, and (ii) a baseline that uses the same features as our model but with the NRC lexicon as well. The results are reported in Table 20, where we report the average f-score of the best model (B-M) on the three datasets, and the best majority baseline (M-B). We also report the results of the best majority baseline on the three datasets, and the results of the best majority baseline on the three datasets, and the results of the best", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": " The INLINEFORM0 scheme is adopted in the first approach, while the INLINEFORM1 scheme is adopted in the second approach.", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": " unanswerable\n\nExplanation: The article does not mention the languages in the corpus.", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": " They define robustness as the ability of the model to be insensitive to the prior knowledge. The model is robust if it can perform well when the prior knowledge is not accurate. The robustness is measured by the accuracy of the model. The more robust the model is, the more accurate the model is when the prior knowledge is not accurate.\n\nQuestion: What is the difference between the neutral features and the labeled features?\n\nAnswer: The labeled features are features that are highly predictive to a specific class, and the neutral features are features that are not informative to any class. The labeled features are manually provided by the model, and the neutral features are", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": " We compare the performance of SBERT to average BERT embeddings, average GloVe embeddings, InferSent, and Universal Sentence Encoder.", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": " For English datasets, the proposed method obtains F1 improvements of +0.29 for CoNLL2003, +0.96 for OntoNotes5.0, and +0.97 for MSRA. For Chinese datasets, the proposed method obtains F1 improvements of +2.36 for OntoNotes4.0, +1.80 for CTB5, +1.80 for CTB6, +2.19 for UD1.4, +1.46 for QuoRef, +0.96 for MSRA, and +2.36 for MSRA.", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": " On which tasks do they test their conflict method?", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": " Which baselines did they compare against?", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": " The core component of the KBQA system is the relation detection model, which is used to detect the relations between the question and the entities in the KB. The KBQA system is enhanced by the relation detection model, which is used to detect the relations between the question and the entities in the KB. The KBQA system is used to answer questions about the entities in the KB. The core component of the KBQA system is the relation detection model, which is used to detect the relations between the question and the entities in the KB. The core component of the KBQA system is the relation detection model, which is used to detect the relations", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": " The baseline models are the encoder-decoder model and the personalized models. The baseline model is the encoder-decoder model with ingredient attention. The personalized models are the Prior Name model, Prior Tech model, and Prior Recipe model.\n\nQuestion: What are the metrics for recipe generation?\n\nAnswer: We use the following metrics: Recipe-level coherence, User Matching Accuracy, and User Matching Accuracy. We also use the following metrics: Recipe-level coherence, User Matching Accuracy, and User Matching Accuracy.\n\nQuestion: What are the metrics for recipe generation?\n\nAnswer: We use the following metrics: Recipe-level coherence, User Matching Accuracy, and User Matching Accuracy. We", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": " The methods to detect bias are described in the following sections. The first method is to manually tag the descriptions with part-of-speech information, and the second is to tag the descriptions with part-of-speech information. The third method is to tag the descriptions with part-of-speech information. The fourth method is to tag the descriptions with part-of-speech information. The fifth method is to tag the descriptions with part-of-speech information. The sixth method is to tag the descriptions with part-of-speech information. The seventh method is to tag the descriptions with part-of-speech information. The eighth method is to tag the descriptions with part", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": " They explore the languages of French, Spanish, and Italian.\n\nQuestion: What is the third person plural pronoun in French?\n\nAnswer: The third person plural pronoun is `they'.\n\nQuestion: What is the third person plural pronoun in French?\n\nAnswer: The third person plural pronoun is `they'.\n\nQuestion: What is the third person plural pronoun in French?\n\nAnswer: The third person plural pronoun is `they'.\n\nQuestion: What is the third person plural pronoun in Spanish?\n\nAnswer: The third person plural pronoun in Spanish is `they'.\n\nQuestion: What is the third person plural pronoun in Spanish?\n\nAnswer", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": " They experimented with the following models: (i) models that use 1D recurrent neural networks, (ii) models that use bidirectional recurrent neural networks, (iii) models that use bidirectional LSTM networks, (iv) models that use bidirectional LSTM networks with peephole connections, and (iv) models that use bidirectional LSTM networks with cell states.", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": " No, they also report results on other languages as well.\n\nQuestion: What is the difference between the proposed approach and the other approaches?\n\nAnswer: The proposed approach is based on the cooccurrence of words in a sliding window through a large corpus, while the other approaches are based on the cooccurrence of words in a sliding window through a large corpus.\n\nQuestion: What is the difference between the proposed approach and the other approaches?\n\nAnswer: The proposed approach is based on the cooccurrence of words in a sliding window through a large corpus, while the other approaches are based on the cooccurrence of words in a sliding window through", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": " The authors experimented with the following algorithms for summarization of peer feedback comments: (i) ILP-based algorithm (ii) Sumy BIBREF23 (iii) TextRank BIBREF24 (iv) LexRank BIBREF25 (v) TopicRank BIBREF26 (vi) RST BIBREF27 (vii) LexRank BIBREF28 (viii) Sumy (viii) TextRank (ix) Sumy (x) TextRank (xi) Sumy (xii) TextRank (xiii) Sumy (xiv) TextRank (xv) Sumy (", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": " The best performing model in terms of recall is the Penultimate Post Attention (PPA) model which infers the context as the context of the post prior to the instructor's post.", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": " The self-attentional master node is the least impactful component of the MPAD architecture. It is used to encode the global information about the document, which is then used to obtain the final document representation. The self-attentional vector is initialized with the sentence vectors, which are obtained by concatenating the feature vectors of the master node and the self-attentional vector. The self-attentional vector is initialized with the feature vectors of the master node, which are obtained by concatenating the feature vectors of the master node and the self-attentional vector. The self-attentional vector is initialized with the feature vectors of the master", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": " The DTA corpus is used for the task.\n\nQuestion: What is the metric used to evaluate the systems?\n\nAnswer: Spearman's $\\rho $ is used to evaluate the systems.\n\nQuestion: What is the baseline for the task?\n\nAnswer: The two baselines are log-transformed normalized frequency difference (FD) and count vectors with column intersection and cosine distance (CNT + CI + CD).\n\nQuestion: What is the OP used for the task?\n\nAnswer: The unsupervised model is used to detect the semantic change.\n\nQuestion: What is the best performing model for the task?\n\nAnswer: The unsupervised model is the DUR", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": " Which 7 Indian languages do they experiment with?", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": " The model performance on target language reading comprehension is not satisfactory.", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": " The proposed model outperforms the baselines by a large margin. The proposed model is able to recover the language style of characters based on their HLAs, and hence is able to retrieve the correct response for a target character. The proposed model is also able to retrieve the correct response for a target character based on the context of the dialogue, and hence is able to retrieve the correct response for a target character based on the context of the dialogue. The proposed model is able to recover the language style of characters based on their HLAs, and hence is able to retrieve the correct response for a target character based on the context of the dialogue", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": " The authors claim that their model ARAML outperforms other baselines in all the metrics.", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": " The authors present the evidence that the model can capture some biases in data annotation and collection by introducing a new fine-tuning strategies to the BERT model. They also show that the BERT model can capture some biases in the data by examining the content of the misclassified samples.", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": " No, only the baseline was tested.", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": " 0.5\n\nQuestion: What is the size of the dataset?\n\nAnswer:: 0.5\n\nQuestion: What is the size of the dataset?\n\nAnswer: 0.5\n\nQuestion: What is the size of the dataset?\n\nAnswer: 0.5\n\nQuestion: What is the size of the dataset?\n\nAnswer: 0.5\n\nQuestion: What is the size of the dataset?\n\nAnswer: 0.5\n\nQuestion: What is the size of the dataset?\n\nAnswer: 0.5\n\nQuestion: What is the size of the dataset?\n\nAnswer: 0.5\n\nQuestion: What", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": " The proposed method is able to achieve F1 improvements by +0.73 for MRPC and +0.73 for QQP.\n\nQuestion: What are the method of the proposed method?\n\nAnswer: The proposed method is to replace CE with losses based on the harmonic mean of precision and recall, which is actually as follows:\n\nQuestion: What is the effect of $\\alpha $ and $\\beta $ in the proposed method?\n\nAnswer: The effect of $\\alpha $ and $\\beta $ in the proposed method is to manipulate the tradeoff between false-positives and false-negatives.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": " The datasets we use are the ERP data from BIBREF0 (the ERP data) and the eye-tracking data from BIBREF0 (the eye-tracking data). We use the eye-tracking data to train the model to predict the ERP signals, and we use the ERP signals to train the model to predict the eye-tracking data. We also use the eye-tracking data to train the model to predict the eye-tracking data, and we use the ERP signals to train the model to predict the ERP signals.", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": " The subjects were presented with a set of 7 speech categories (7 phonemic/syllabic categories) and 5 binary classification tasks (presence/absence of consonants, presence/absence of vowels, presence/absence of high-front vowels, presence/absence of high-back vowels) varying based on the task. The subjects were asked to imagine saying the corresponding words in their mind.\n\nQuestion: What is the purpose of the proposed model?\n\nAnswer: The proposed model is a mixed deep neural network scheme for decoding speech information from the cross-covariance of the electrodes. The model is composed of a CNN, LSTM and", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": " We compare our models with the following baselines. Pointer-Gen is the baseline model trained by optimizing $L_\\text{MLE}$ in Equation DISPLAY_FORM19. Pointer-Gen+RL-ROUGE is the baseline model trained by optimizing $L_\\text{RL-ROUGE}$ in Equation DISPLAY_FORM17. Pointer-Gen+RL-SEN is the baseline model trained by optimizing $L_\\text{RL-SEN}$ in Equation DISPLAY_FORM17. Pointer-Gen+ARL-ROUGE is the baseline model trained by optimizing $L_\\text{RL-ROUGE}$ in Equation DISPLAY_FORM17. Pointer-Gen", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": " The dataset is used on the dataset.\n\nQuestion: What are the feature extensions?\n\nAnswer: The dataset is used on the dataset.\n\nQuestion: What are the ensemble models?\n\nAnswer: The ensemble models are used on the dataset.\n\nQuestion: What is the most accurate in classifying abusive language?\n\nAnswer: The most accurate in classifying abusive language is the dataset.\n\nQuestion: What is the best LR model?\n\nAnswer: The best LR model is the dataset.\n\nQuestion: What is the most accurate in classifying abusive language?\n\nAnswer: The most accurate in classifying abusive language is the best LR model.\n\nQuestion: What is the most accurate in", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": " We use the transformer architecture for the language model and the sequence to sequence model. We use a transformer-based sequence to sequence model for the sequence to sequence model.", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": " The proposed method is to use dice loss in replacement of the standard cross-entropy loss, which performs as a hard version of F1 score. The dice loss can be written as follows:\nwhere $p_{i1}$ is the probability of the positive class for example $x_i$. The dice loss (DL) actually works for F1 but not for accuracy. The proposed focal loss actually works for F1 but not for accuracy. The dice loss actually works for F1 but not for accuracy. The proposed focal loss actually works for F1 but not for accuracy. The proposed focal loss actually works for F1 but not for accuracy", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": " The results are shown in Figure FIGREF10. We see that the agents utilizing knowledge graphs are able to surpass the bottleneck of Zork1, whereas the agents without knowledge graphs are not. This is due to the fact that the agent without knowledge graphs is not able to explore the game's world state efficiently enough to find the correct action sequence to pass the bottleneck. The results are shown in Figure FIGREF10. We see that the agents utilizing knowledge graphs are able to surpass the bottleneck of Zork1, whereas the agents without knowledge graphs are not. This is due to the fact that the agent without knowledge graphs is not able to explore", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": " The individual models are monolingual models for each language. The word alignments are used to couple the individual models. The crosslingual latent variables are used to capture the role correspondences between the aligned roles in the two languages.\n\nQuestion: What is the role of the crosslingual latent variables?\n\nAnswer: The crosslingual latent variables capture the role correspondences between the aligned roles in the two languages.\n\nQuestion: What is the difference between the individual models and the crosslingual latent variables?\n\nAnswer: The individual models are monolingual models for each language. The crosslingual latent variables capture the role correspondences between the aligned", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": " A semi-character architecture is a word recognition model that takes a sequence of (possibly misspelled) words as input and outputs a sequence of words that are recognized by the model. The model is trained on a large corpus of text, and the training data is augmented with adversarial examples that are generated by an adversary. The model is then used to recognize the words in the input sequence and the words are then used to classify the input sequence. The model is trained on a large corpus of text, and the training data is augmented with adversarial examples that are generated by an adversary. The model is then used to recognize the words in the", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": " 16 languages are explored.\n\nQuestion: what is the tagset used by the models?\n\nAnswer: The tagset used by the models is the Universal PoS tagset.\n\nQuestion: what is the macro-average?\n\nAnswer: The macro-average is the average accuracy of the models.\n\nQuestion: what is the macro-average of the models?\n\nAnswer: The macro-average of the models is 96.60%.\n\nQuestion: what is the macro-average of the models on OOV words?\n\nAnswer: The macro-average of the models is 96.60%.\n\nQuestion: what is the macro-average of the models?\n\nAnswer: The macro-average of the", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": " The results are shown in Table 1. The proposed NCEL model achieves the best performance in most cases, and outperforms the state-of-the-art methods by a large margin. The results on the five datasets are shown in Figure FIGREF10. The results of NCEL are the best on all datasets, and the improvements of NCEL over the baseline methods are quite large in most cases. The results of NCEL-local are similar to those of NCEL, but the improvements of NCEL are much larger on the “hard\" dataset WW. The results of NCEL-noatt are similar to those of NCEL, and the", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": " Yes\n\nQuestion: Is the data annotated with the MR tags?\n\nAnswer: Yes\n\nQuestion: What is the percentage of times dosage is correct?\n\nAnswer: 84.12\n\nQuestion: What is the percentage of times frequency is correct?\n\nAnswer: 76.34\n\nQuestion: What is the percentage of times dosage is correct in the transcripts generated using Google Speech-To-Text?\n\nAnswer: 71.75\n\nQuestion: What is the percentage of times frequency is correct in the transcripts generated using Google Speech-To-Text?\n\nAnswer: 73.58\n\nQuestion: What is the percentage of times frequency is correct in the transcripts generated", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": " The baseline was the system by Rei2016, trained on the original FCE dataset.", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": " The authors used the clinical notes from the CE task in 2010 i2b2/VA. The CE task in 2010 i2b2/VA is a clinical note annotation task. The authors point out that the clinical notes are annotated with clinical entities and the entities are tagged with BIO tags. The authors used the clinical notes to train the NER model. The clinical notes were annotated with clinical entities and the entities were tagged with BIO tags. The authors used the clinical notes to train the NER model. The authors point out that the clinical notes are annotated with clinical entities and the entities are tagged with BIO tags.", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": " The main reason to introduce the refine process is to enhance the decoder using BERT's contextualized representations, so we do not use the BERT network to predict the context vectors. This objective is similar to the cloze task in BERT's pre-train process. The refine process works as follows: first the draft decoder writes a summary draft based on a document, and then the refine decoder edits the draft. It predicts words given the source document and other words of the summary, and given the context-aware word representations, it can generate more fluent and natural sequences.", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": " The authors in their subsequent work BIBREF26 use the massive distant-supervised dataset to build a powerful tweet representation, which is shown to be useful in building tweet embeddings for the task of sentiment classification.", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": " The TF-IDF features are extracted using the following steps: 1) Term frequency, 2) Inverse document frequency, and 3) Term frequency-inverse document frequency.\n\nQuestion: What is the performance of the XGBoost classifier?\n\nAnswer: The performance of the XGBoost classifier is up to 83% accuracy.\n\nQuestion: What is the performance of the TF-IDF features?\n\nAnswer: The performance of the TF-IDF features is up to 83% accuracy.\n\nQuestion: What is the performance of the TF-IDF features on the task of predicting the primary diagnosis?\n\nAnswer: The performance of the TF-IDF", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": " The dataset is annotated with 7 classes, each of which is annotated with one or more feature values.", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": " They evaluated on eight biomedical NER tasks.\n\nQuestion: Which wordpiece vector space did they use to predict the identity of wordpieces?\n\nAnswer: They used the wordpiece vector space.\n\nQuestion: Which wordpiece vector space did they use to predict the identity of wordpieces?\n\nAnswer: They used the wordpiece vector space.\n\nQuestion: Which wordpiece vector space did they use to predict the identity of wordpieces?\n\nAnswer: They used the wordpiece vector space.\n\nQuestion: Which wordpiece vector space did they use to predict the identity of wordpiece vectors?\n\nAnswer: They used the wordpiece vector space.\n\nQuestion: Which wordpiece vector", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": " The training data that was made available by the organizers was used, which is a set of tweets with for each tweet a machine translation of the labels. The labels were then used to train the models.", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": " They used a multinomial Naive Bayes classifier.", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": " The baseline system for the SLC task is a logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. The performance of this baseline is shown in Tables TABREF33 and TABREF34.\n\nQuestion: What was the best system for this task?\n\nAnswer: The best system for the FLC task was team newspeak, which achieved an F-score of 0.75 on the test set. The best system for the SLC task was team newspeak, which achieved an F-score of 0.75 on the test set.\n\nQuestion: What was the best system for", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": " We also implemented a baseline that does not adopt joint learning, which is a simple tagging scheme consisting of two tags { INLINEFORM0 }:\n\nINLINEFORM0 tag indicates the current word is not a pun.\n\nINLINEFORM1 tag indicates the current word is a pun.\n\nIf the predicted tag sequence contains at least one INLINEFORM2 tag, we regard the corresponding instance as the one containing a pun. If the given text is not detected as a pun, we regard the instance as false. For the location task, we consider the predicted pun as the gold pun if and only if the predicted tag sequence contains at least one INLINEFORM3 tag.", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": " We first built a multi-layer representation of each news article, which is composed of four layers (RT, R, Q and M) and we computed a set of global network properties for each layer. Then we used the following global network properties to encode each network with a tuple of features. We employed a $\\chi ^2$ test to highlight the most discriminative features across layers. We also used the following global network properties to encode each network with a tuple of features: we computed the global network properties of each layer and we used the following global network properties to encode each network with a tuple of features. We employed a $\\chi ^", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": " The ancient-modern Chinese dataset we used come from ancient Chinese history records in several dynasties (1000BC-200BC) and articles written by celebrities of that era.\n\nQuestion: What is the difference between the ancient and modern Chinese languages?\n\nAnswer: The ancient Chinese is highly concise and its syntactical structure is different from modern Chinese.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": " English\n\nQuestion: What is the distribution of the tweets?\n\nAnswer:: The distribution of the tweets is 30% non-offensive, 30% untargeted, 40% targeted.\n\nQuestion: What is the distribution of the tweets in the training set?\n\nAnswer: The distribution of the tweets in the training set is 30% non-offensive, 30% untargeted, 40% targeted.\n\nQuestion: What is the distribution of the tweets in the test set?\n\nAnswer: The distribution of the tweets in the test set is 30% non-offensive, 30% untargeted, 40%", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": " The Chinese datasets are the Penn Treebank (PTB) and the Penn Chinese Treebank (CTB). The Penn Treebank is a corpus of English sentences, and the Penn Chinese Treebank is a corpus of Chinese sentences. The Penn Treebank is a collection of sentences, and the Penn Chinese Treebank is a collection of Chinese sentences. The Penn Treebank is a collection of sentences, and the Penn Chinese Treebank is a collection of Chinese sentences. The Penn Treebank is a collection of sentences, and the Penn Chinese Treebank is a collection of Chinese sentences. The Penn Treebank is a collection of sentences, and the Penn", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": " The UTCNN model has three layers: the user matrix embedding layer, the user vector embedding layer, and the document embedding layer. The first two layers are the user matrix embedding layer and the user vector embedding layer. The third layer is the document embedding layer. The fourth layer is the topic matrix embedding layer, the topic vector embedding layer, and the document embedding layer. The fifth layer is the comment embedding layer. The sixth layer is the topic embedding layer. The seventh layer is the user vector embedding layer. The eighth layer is the user matrix embedding layer. The ninth layer is the user vector embedding layer. The tenth layer is the document", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": " The structured datasets used in this paper are the same as those used in BIBREF7. They include nine (real-valued) numerical features, which are latitude, longitude, elevation, population, and five climate related features (avg. temperature, avg. precipitation, avg. solar radiation, avg. water vapor pressure, and avg. wind speed). In addition, 180 categorical features were used, which are CORINE land cover classes at level 1 (5 classes), level 2 (15 classes) and level 3 (44 classes) and a binary classification problem.", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": " The authors use two datasets for their experiments: NUBes-PHI, a corpus of real medical reports written in Spanish and annotated with sensitive information, and the MEDDOCAN 2019 shared task dataset.", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": " Unigram (with principal components of unigram feature vectors), and (a) Unigram (with principal components of unigram feature vectors), (b) Sarcasm (the feature-set reported by joshi2015harnessing subsuming unigram features and features from other systems), and (c) Edge (the feature-set reported by joshi2015harnessing subsuming unigram features and features from other systems).", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": " The predictive performance is measured by the metrics of LiLi. The strategy formulation ability is measured by the number of instances for which LiLi has successfully formulated a strategy that leads to winning. The performance is measured by the number of instances for which LiLi has successfully formulated a strategy that leads to winning. The performance is measured by the number of instances for which LiLi has successfully formulated a strategy that leads to winning. The predictive performance is measured by the number of instances for which LiLi has successfully formulated a strategy that leads to winning. The predictive performance is measured by the number of instances for which LiLi has successfully formulated a strategy that leads", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": " No, they use their indexing-based method to create a sample of a QA Wikipedia dataset.\n\nQuestion: Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?\n\nAnswer: No, they use their indexing-based method to create a sample of a QA Wikipedia dataset.\n\nQuestion: Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?\n\nAnswer: No, they use their indexing-based method to create a sample of a QA Wikipedia dataset.\n\nQuestion: Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?\n\nAnswer: No, they use their indexing-based method to create a", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": " Galatasaray and Fenerbahçe are the targets.\n\nQuestion: Which features are used in the tweet data set?\n\nAnswer: Galatasaray and Fenerbahçe are the targets.\n\nQuestion: What is the size of the data set?\n\nAnswer: The data set is composed of 700 tweets.\n\nQuestion: What is the size of the data set?\n\nAnswer: The data set is composed of 700 tweets.\n\nQuestion: What is the size of the data set?\n\nAnswer: The data set is composed of 700 tweets.\n\nQuestion: What is the size of the data set?\n\nAnswer: The data set is composed of", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": " We implement a series of experiments to evaluate the performance of our model. We first pre-train the model with the auto-encoder model and back-translation. Then we train the model with the policy gradient algorithm and the reinforcement learning. The results of the experiments are shown in Table 1 and Table 2. The results show that the model can transfer the non-ironic sentences to the ironic sentences with high accuracy. The sentiment and content of the generated sentences are also preserved well.", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": " Gaussian-masked directional multi-head attention is a variant of self-attention which is a scaled dot-product attention. The standard self-attention is a function to map queries and key-value pairs to the representation of input. The attention is calculated by dotting query with keys and then followed by a softmax function. The standard self-attention is not able to capture the localness relationship between characters. The Gaussian-masked directional multi-head attention is a variant of self-attention which is used to capture the localness relationship between characters and the localness relationship between characters is represented as a fix Gaussian weight.", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": " They considered Facebook status updates.\n\nQuestion: What is the best model for causality prediction?\n\nAnswer: The linear model with the discourse arguments which most contain causal explanations obtained the highest accuracy (0.868) for the causal explanation identification task.\n\nQuestion: What is the best model for causal explanation identification?\n\nAnswer: The LSTM model obtained the highest accuracy (0.868) for causal explanation identification.\n\nQuestion: What is the best model for the causality prediction task?\n\nAnswer: The linear model obtained the highest accuracy (0.868) for the causality prediction.\n\nQuestion: What is the best model for the causality prediction task?\n\nAnswer", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": " The baseline features are the features extracted by the baseline CNN architecture. These features are extracted from the pre-trained models and are used as the input features for the final classification. The baseline features are extracted from the fully-connected layer of the baseline CNN architecture. These features are extracted from the pre-trained models and are used as the input features for the final classification. The baseline features are extracted from the fully-connected layer of the baseline CNN architecture. These features are extracted from the fully-connected layer of the baseline CNN architecture. These features are extracted from the fully-connected layer of the baseline CNN architecture. These features are extracted from the fully-connected layer of the", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": " The number of clusters was varied.", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": " 0.71, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.69, 0.", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": " 53 documents\n\nQuestion: What is the size of the corpus?\n\nAnswer:: 53 documents\n\nQuestion: What is the size of the corpus?\n\nAnswer:: 53 documents\n\nQuestion: What is the size of the corpus?\n\nAnswer:: 53 documents\n\nQuestion: What is the size of the corpus?\n\nAnswer:: 53 documents\n\nQuestion: What is the size of the corpus?\n\nAnswer:: 53 documents\n\nQuestion: What is the size of the corpus?\n\nAnswer:: 53 documents\n\nQuestion: What is the size of the corpus?\n\nAnswer:: 53 documents\n\nQuestion: What is the", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": " unanswerable\n\nQuestion: Is it possible to convert a cloze-style questions to a naturally-looking questions?\n\nAnswer:: unanswerable\n\nQuestion: Is it possible to convert a cloze-style questions to a naturally-looking questions?\n\nAnswer:: unanswerable\n\nQuestion: Is it possible to convert a cloze-style questions to a naturally-looking questions?\n\nAnswer:: unanswerable\n\nQuestion: Is it possible to convert a cloze-style questions to a naturally-looking questions?\n\nAnswer:: unanswerable\n\nQuestion: Is it possible to convert a cloze-style questions to a naturally-looking questions?\n\nAnswer: unanswerable\n\n", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": " They consider text categorization, sentiment classification, and web-page classification.\n\nQuestion: What is the difference between the proposed regularization terms and the baseline?\n\nAnswer: The baseline is GE-FL, which leverages labeled features as prior knowledge to the model. The labeled features are words that are highly indicative of a specific class. The reference distribution of the labeled features is manually provided. The proposed regularization terms are maximum entropy, neutral features, and KL divergence of class distribution. The first two are to control the model to be more robust, and the last one is to control the model to be more robust and to be more sensitive to the reference distribution", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": " We make use of a BERT QA model that makes use of question classification information to improve question answering performance. This is a common approach in the question answering literature, but is not typically used in the science domain. We make use of a BERT-QC model that makes use of the same question features as the BERT-QA model, and report performance of the BERT-QC model using the question classification labels of the BERT-QC model. We also report performance of the BERT-QC model using predicted labels, and performance of the BERT-QC model using predicted labels. We also report performance of the BERT", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": " The training sets of the new ELMo models are larger than the previous ones. The training sets of the new ELMo models are formed from the same languages as the previous ones, but the new models are trained on larger training sets. The previous models were trained on 20-million-word datasets, while the new models are trained on 270-million-word datasets.", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": " 1000\n\nQuestion: What is the number of entities in the dataset?\n\nAnswer: 1000\n\nQuestion: What is the number of words in the dataset?\n\nAnswer: 1000\n\nQuestion: What is the number of words in the dataset?\n\nAnswer: 1000\n\nQuestion: What is the number of words in the dataset?\n\nAnswer: 1000\n\nQuestion: What is the number of words in the dataset?\n\nAnswer: 1000\n\nQuestion: What is the number of words in the dataset?\n\nAnswer: 1000\n\nQuestion: What is the number of words in the dataset?\n\nAnswer: 100", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": " Eusboost, MWMOTE\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: simultaneous two sample learning (s2sL)\n\nQuestion: What is the main difference between the proposed model and the baseline?\n\nAnswer: The main difference is that the proposed model uses two samples instead of one sample.\n\nQuestion: What is the main idea of the proposed model?\n\nAnswer: The main idea of the proposed model is to use two samples instead of one sample.\n\nQuestion: What is the main idea of the proposed model?\n\nAnswer: The main idea of the proposed model is to use two samples instead of one sample.\n\nQuestion:", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": " Yes, their NER model learns from both text and images.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": " No, they also evaluate on German and Chinese.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": " 0.7033\n\nQuestion: What was their highest F1 score?\n\nAnswer: 0.1119\n\nQuestion: What was their best answer for the question: What is the function of the protein Magt1?\n\nAnswer: 0.1119\n\nQuestion: What was their best answer for the question: What is the function of the protein Magt1?\n\nAnswer: 0.1119\n\nQuestion: What was their best answer for the question: What is the function of the protein Magt1?\n\nAnswer: 0.1119\n\nQuestion: What was their best answer for the question: What is the", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": " For both POS tagging and dependency parsing, they use the Wall Street Journal (WSJ) portion of the Penn Treebank. They also use the one billion word language modeling benchmark dataset.\n\nQuestion: What are some previous works?\n\nAnswer: The closest previous work is the Gaussian HMM, which is a generative model that defines a probability distribution over word embeddings and POS tags. The Gaussian HMM is a model that defines a probability distribution over word embeddings and POS tags. The Gaussian HMM is a model that defines a probability distribution over word embeddings and POS tags. The Gaussian HMM is a model that defines a probability distribution over word embeddings", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": " The authors provide evidence for this claim in the following ways. First, they show that the models built in NeuronBlocks can achieve competitive or even better results compared with the models built in other frameworks. Second, they show that the models built in NeuronBlocks can be easily deployed in the production environment, which suggests that NeuronBlocks can provide a more efficient solution for engineers. Third, they show that the models built in NeuronBlocks can be easily extended to support various optimization techniques, which further suggests that NeuronBlocks can provide a more efficient solution for engineers.\n\nQuestion: What is the main contribution of the authors?\n\nAnswer: The", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": " On SimpleQuestions, we achieve 93.2% accuracy, which is 2.5% higher than the previous state-of-the-art. On WebQSP, we achieve 80.6% accuracy, which is 1.8% higher than the previous state-of-the-art.", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
