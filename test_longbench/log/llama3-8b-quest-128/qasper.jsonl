{"pred": " The ground truth for fake news is established by using the number of retweets, the number of friends, and the proportion of verified accounts.", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": " The GhostVLAD approach is a pooling strategy that was initially developed for place recognition by R. Arandjelovic et al. [11]. The GhostVLAD is an extension to VLAD [18] approach where they were able to replace the hard assignment based on the NetVLAD layer is a pooling strategy that was initially proposed for language identification task.", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": " 0.718\n\nQuestion: What is the state of the art results?\n\nAnswer: 0.718\n\nQuestion: What is the state of the art results?\n\nAnswer: 0.718\n\nQuestion: What is the state of the art results?\n\nAnswer: 0.718\n\nQuestion: What is the state of the art results?\n\nAnswer: 0.718\n\nQuestion: What is the state of the art results?\n\nAnswer: 0.718\n\nQuestion: What is the state of the art results?\n\nAnswer: 0.718\n\nQuestion: What is the state of the results?\n\nAnswer: 0.", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": " The additional features and context are proposed in order to improve the baseline LSTM model. The additional features and context are proposed in order to improve the baseline LSTM model. The additional features and context are proposed in order to improve the baseline LSTM model. The additional features and context are proposed in order to improve the baseline LSTM model. The additional features and context are proposed in order to improve the baseline LSTM model. The additional features and context are proposed in order to improve the baseline LSTM model. The additional features and context are proposed in order to improve the baseline LSTM model. The additional features and context are proposed in order to improve the baseline LSTM model.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": " Which Facebook pages did they look at?\n\nQuestion: Which Facebook pages did they look at?\n\nAnswer: Which Facebook pages did they look at?\n\nQuestion: Which Facebook pages did they look at?\n\nAnswer: Which Facebook pages\n\nQuestion: Which Facebook pages did they look at?\n\nAnswer: Which Facebook pages\n\nQuestion: Which Facebook pages did they look at?\n\nAnswer: Which Facebook pages\n\nQuestion: Which Facebook pages did they look at?\n\nAnswer: Which Facebook pages\n\nQuestion: Which Facebook pages did they look at?\n\nAnswer: Which Facebook pages\n\nQuestion: Which Facebook pages did they look at?\n\nAnswer: Which Facebook pages\n\nQuestion: Which Facebook", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": " Yes, the hashtag and SemEval and SemEval datasets contain only English data.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": " We used the model is Bert, Bert, BertSum, BIBREF, BIBREF24, BIBREF24, BIBREF27, BIBREF2, BIBREF24, BIBREF24, BIBREF34, BIBREF22, BIBREF22, BIBREF22, BIBREF12, BIBREF28, BIBREF17, BIBREF22, BIBREF27, BIBREF25, BIBREF15, BIBREF32, BIBREF27, BIBREF8, BIBREF18, BIBREF9, BIB", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": " The proposed approach is a WSD approach that employs word embeddings. The proposed approach is a W2G model that uses a stricter bound for KL between Gaussian M. The proposed approach is a stricter upper bound for KL between Gaussian mixtures. The proposed approach is a stricter lower bound for KL between Gaussian mixtures. The proposed approach is a stricter upper bound for KL between Gaussian mixtures. The proposed approach is obtained as shown in equation DISPLAY_FORM28.", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": " The answer is unanswerable. The answer is unanswerable. The answer is unanswerable. The answer is unanswerable. The answer is unanswerable. The answer is unanswerable. The answer is unanswerable. The answer is unanswerable. The answer is unanswerable. The answer is unanswerable. The answer. The answer is unanswerable. The answer is unanswerable. The answer is unanswerable. The answer is unanswerable. The answer is unanswerable. The answer is unanswerable. The answer is unanswerable. The answer is unanswerable", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": " The two subsets of the two datasets are Friends and EmotionPush. The first one is a subset of the two subsets of the two subsets of the two subsets of the two datasets. The second is a subset of the two subsets of the two subsets of the subset of the two pre-training task (cloze test) and the pre-training task (the first pre-training task (the first pre-training task (the first pre-training (e.g. (e.g. (e.g. (e.g. (e.g. (e. (e. (e. (e.g. (e (e. (e. (", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": " The article is about text simplification, which is a method of reducing the size of the text. The article focuses on English Wikipedia and the size of the article is 296,402 sentence pairs. The article is also focuses on the article is a phrase-based method with a reranking post-processing step. The article is a syntax-based translation model using PPDB paraphrase database. The article is a syntax-based translation model using PPDB paraphrase database BIBREF26 and the article is about text simplification. The article is about text simplification is about the article is pretty easy to understand than the article is about the article is pretty", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": " The Groningen Meaning Bank (GMB) by BIBREF12, containing 47,959 sentence samples.\n\nQuestion: The GMB dataset has 17,959 sentence samples. The GMB dataset is a dataset is Groningen Meaning Bank (GMB) by BIBREF12, containing 47,959 sentence samples. The GMB dataset is a dataset is Groningen Meaning Bank (BIBREF12) by BIBREFREF12, containing 47,959 sentence samples. The GMB dataset is Groningen Meaning Bank (GMB) by BIBREF12, containing 47,959 sentence samples. The", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": " 0.98\n\nQuestion: What are the types of entities that are annotated in the system?\n\nAnswer: Person-Name, Song-Name\n\nQuestion: What are the system: What are the types of entities that are annotated in the system?\n\nAnswer: Brand-Brand, Product-Product, Model-Model, Material-Model-Model, Specification-Brand, Brand-Brand, Product-Product, Model-Product-Product, Model-Model, Material-Brand, Brand-Product-Product, Brand-Brand, Product-Product, Model-Model, Material-Model-Model, Model-Brand, Product-Brand,", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": " \"Did they experiment with this new dataset?\" The answer: Yes, they did.", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": " The authors used a set of 1, 10, 50, 000, 100, 10, 2, 3, 62, 5, 10, 10, 10, 0000, 10, 2, 50, 49, 000, 40, 184, 10000, 100, 10, 10, 10, the case, and the date and/ of social media. In CognIA system.", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": " The best performance with the best performance of the best performance of the best performance of the best performance of the best performance of the best performance of the best performance of the best performance of the best performance is the best performance of the best performance of the best performance of the stock market sector is the stock market sector achieved the best performance of the best performance of the market activity in the best performing stock market in the best performing stock market of the best of the best of the stock market is the best of the best of the stock and the best of the stock market of the stock market sector is the best of the stock market of the best performance of", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": " they trained the RNN-based NMT model and Transformer model and Transformer model, and the RNN-based NMT model and Transformer-NMT model.", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": " The three regularization terms can be derived by setting the prior knowledge of the model's expectation. The first term to be the maximum entropy. The maximum entropy regularization term requires the true distribution which doesn't make any extra features. However, a simple, which is a set of neutral features as common features are not the true distribution just like GE-1 but also any of the most common features as follows: The three regularization terms to the true class distribution of the unbalanced and a question.", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": " The baselines are as follows: (1) Majority, (2) Average Word Embedding, (3) Recurrent Convolutional Neural Networks (RCNN) BIBREF0, (4) Recurrent Convolutional Neural Networks (RCNN) BIBREF0, (5) Recurrent Convolutional Neural Networks (RCNN) BIBREF0, (6) Recurrent Convolutional Neural Networks (RCNN) BIBREF0, (7) Recurrent Convolutional Recurrent Convolutional Neural Networks (RCNN) BIBREF0, (8) Recurrent Convolutional", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": " They improved by several points\n\nQuestion: What is the second part of the article?\n\nAnswer: The second part of the paper\n\nQuestion: What is the second part of the article?", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": " The original softmax function is a strictly convex function, which means that it is a function that is not differentiable between the encopit. The $\\alpha $-entmax mapping (BIBREF19) is a novel Transformer BIBREF0. However, the $\\alpha $-entmax mapping (BIBREF19) is a piecewise function, which is a piecewise linear function. Con BIBREF10.", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": " The baseline model is the model that we provide in Table TABREF24 (we) in Section SECREF33.", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": " We use the same architecture of 12 Transformer layers, RAMEN, RAMEN$_{\\textsc {base}}$+RoBERTa, and RAMEN$_{\\textsc {base}}$+RoBERTa with the English, RAMEN$_{\\textsc {base}}$+} and the English, and RAMEN$_{\\textsc {base}}$+} and RAMEN$_{\\textsc {base}}$+RAMEN$_{\\textsc {base}}$+BPE. We have successfully adapted RAMEN$_{\\textsc {base}}$+RAMEN$_{\\textsc {base}}\\", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": " The attention module: The attention module contains two encoders as well as two decoders. In this way, the attention module is fixed to the same pre-training and fine-tune it with the attention module. The length of $\\mathbf {e^s}$ have the same length of pre-training and fine-tuning.", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": " Unigrams and Pragmatic features, associated with Average Fixation Duration per Word and Largest Regression Position are seen to be the two most significant features. Further, in each of the cases, Average Fixation Duration per Word and Largest Regression Position are seen to be the two most significant features.\n\nQuestion: What kind of stylistic features are obtained?", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": " The number of LSTM layers is one component components is one component.\n\nQuestion: What architecture does the decoder have?\n\nAnswer:?\n\nAnswer: The components of the CoNLL–SIGMORPHON 2016, BIBREF1\n\nQuestion: What architecture does the CoNLL–SIGMORPHON 2018 baseline is described as?\n\nAnswer: The CoNLL–SIGMORPHON 2018, BIBREF2\n\nQuestion: The MSD prediction?\n\nAnswer: The MSD prediction, BIBREF3\n\nQuestion: The MSD-prediction accuracy for the CoNFR32 summarises the added", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": " unanswerable\n\nQuestion: Is WordNet is a good source of knowledge for this task?\n\nAnswer: unanswerable\n\nQuestion: Is WordNet is a good source of knowledge of the WordNet\n\nAnswer: unanswerable\n\nQuestion: Is WordNet is a\n\nAnswer: unanswerable\n\nQuestion: WordNet is\n\nAnswer: unanswerable\n\nQuestion: WordNet\n\nAnswer: unanswerable\n\nQuestion: WordNetQA\n\nAnswer: unanswerable\n\nQuestion: WordNet\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer:", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": " The baseline is the performance of Jasper 10x3 model with SGD with momentum optimizer for 400 epochs. The results are provided in Table TABREF31.", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "  0.000000\n\nQuestion: How many blogs do they look at the different industries?\n\nAnswer:  0.000000\n\nQuestion: How many users do they look at?\n\nAnswer:  0.000000\n\nQuestion: How many users do they look at?\n\nAnswer:  0.000000\n\nQuestion: How many users do they look at?\n\nAnswer:  0.000000\n\nQuestion: How many users do they look at?\n\nAnswer:  0.000000\n\nQuestion: How many users do they look at?\n\nAnswer:  0.000000\n\nQuestion: How many", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": " Base, B: A: Base, B: A: Base, A: Base, B: A: Base, B: A: Base, B: A: Base, B: A: Base, B: A: Base, B: A: Base, B: A: Base, B: A: Base, B: A: Base, B: A: Base, B: A: Base, B: A: Base, B: A: Base, B: A: Base, B: A: Base, B: A: Base, B: A: Base, B: A: Base, B:", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": " The training on the answer is better when the $n$ -d.", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": " We use the four of $\\alpha $-entmax) is piecewise linear, in the $\\alpha $-head. to identify the four $\\alpha $ is piece of @!START@$\\alpha $@!END@-max@!END@!START@$\\alpha $@!END@-entmax@!END@-entmax@!END@-entmax@!END@-entmax@!END@-entmax @!END@!END@-entmax. This leads to the question as @!END@!END@!END@!START@!START@!END", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": " The results are presented in Table TABREF21. We included the evaluation of the original ELMo English model in the same table. In both cases (ELMo and fastText) we can not be presented in the same table.", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": " The background of the authors are from different disciplines, including political science, history, and communication, communication, and the authors have different backgrounds, which is the reason why they have different research interests, and they have different research questions, and they have different modeling approaches, and they have different background, and they have different background, and they have different background, and they have a different, and they have a different, and they have a different, and the background, and the text, and the background, and the background, and the background, and the background, and the, and the background, and the and the background, and the", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": " No, the paper is supervised approach to spam detection.\n\nQuestion: What is the difference between content-based features?\n\nAnswer: The topics interested or disliked by the $\\emph {i}^{th$2 user can be manifested by $GOSS$ and $GOSS$ and $LOSS$.\n\nQuestion: What is the difference between the difference between the best performance?\n\nAnswer: The best performance is the best performance of the $F1-score$.\n\nQuestion: What is the best performance of the best performance?\n\nAnswer: The best performance is the best performance.\n\nQuestion: What is the best content-based features alone.", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": " The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.\n\nQuestion: Which languages are harder to be harder to distinguish?\n\nAnswer: The Nguni languages are harder to distinguish. The Sotho languages are also harder to distinguish.\n\nQuestion: Which languages are the best LID of short texts, informal styles and similar languages?\n\nAnswer: The LID of short texts, informal styles and similar languages remains a difficult to reduce the proposed algorithm was evaluated on the task of LID of short texts, informal styles and the NCHLT text corpora is expected.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": " 1) 2-layers regular-trained LSTM model 2) 3) 2-layers regular-trained LSTM model 4) 2-layers regular-trained LSTM model 5) 2-layers regular-trained LSTM model 6) 2-layers regular-trained LSTM model 7) 2-lays 8) 2-layers regular-trained LSTM model 9) 9) 2-layers regular-trained LSTM model 10) 2-layers regular-trained LSTM model 11) 2-layers regular-trained LSTM model 12) 2-layers regular-trained LSTM model 13) 13", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": " The Wikipedia article is a large data set of 29,794 articles.", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": " The human evaluation was done by a group of 50 native speakers who were native people who were well-versed in both English and Tamil and Tamil and Tamil. They were asked to individually rank the adequacy and fluency and fluency of the translations of the RNNSearch and RNNMorph models. The human Kappa co-efficient was calculated with respect to: DISPLAYFORM0\n\nQuestion: What is the main conclusion of the article?\n\nAnswer: The article concludes that the main conclusion of the article is that the use of the main conclusion of the article is that the model is the main conclusion of the article. The main conclusion of", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": " Yes, they do. They use the WIT3's TED corpus as the basis of their approach. They perform the translation between one language pair (English and the other is a mix-source approach BPE.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": " We quantify the efficiency of learned a single model with a question and a question. The model is a question is a question of the question. The model is a question of the question. The question is a question. We quantify the question. We quantify the question. We quantify the question. We quantify the question. We quantify the question. We quantify the model. We quantify a question. We quantify a question. We quantify a single. We quantify a question. We quantify a question. We quantify a question. We quantify a question. We quantify a question. We quantify the question. We are 6. We quantify the population.", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": " For the STRENGTH and WEAKNESS and WEAKNESS classes, we use INLINEFORM0, where the SUGGESTION and INLINEFORM0. For the multi-class multi-label classification problem of mapping of the INLINEFORM0, we use INLINEFORM0. For the STRENGTH and WEAKNESS sentences, we use INLINEFORM0.", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": " The source domain is the labeled source domain that we use for training, and the target domain is the target domain that we attempt to adapt the model to.", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": " \"We can see that the performance of the PRU outperforms most of the state-of-the-art methods on the PRUs by 1.3 points on the PTB dataset while learning 15-20% fewer parameters. We can see that the PRU delivers the Pyramidal Recurrent Unit, which is a special case of the PRU, which is a special case of the Pyramidal Recurrent Unit (PRU, which is a previous work in Section SECREF39. We can see that the PRU and the PRU. We can see that the PRU delivers higher than the PRU. We", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": " What neural network modules are included in NeuronBlocks?\n\nAnswer:", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": " The answer: deri2016grapheme's model for adapting high resource g2p models for low resource languages. The answer: wFST models for multilingual pronunciation corpus BIBREF12. The answer: sequence-to-sequence models. The answer: sequence-to-sequence approach, in which a language they have been trained on the answer: sequence-to-sequence approach, in which a monolingual g2p is a model is a multilingual g2p, in the 311 languages in the multilingual pronunciation data. The answer: multilingual g2p, in which the same system is seen in the ", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": " The baselines for XLNet and we also use the Huggingface’s Pytorch Transformer. The results for BIBREF12. We work with the scope resolution task, and we use the scope resolution and the scope resolution task.", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": " The dataset creation and transfer procedures described above result in a notable effect of Translate-", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": " They test their method on social media posts from the website Twitter, which are an extremely useful testing ground for a social media post.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": " Yes, they use Glove embeddings.\n\nQuestion: What is the baseline model BIBREF0?\n\nAnswer: The baseline model is the basic seq2 model, as proposed in the context of infoboxes. Further, as well as the model with only bifocal attention.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": " unanswerable\n\nExplanation: The paper does not mention any evaluation of the baseline against some explicit or other system.", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": " They obtain psychological dimensions of people's psychological dimensions of people by using the distribution of the individual words in a category, which is a lexical resources, such as Roget or Linguistic Inquiry and word count BIBREF9.", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": " The ML methods aim to identify the argument components in the logical dimension of an argument.\n\nQuestion: What the ML methods aim to identify?\n\nAnswer: The ML methods aim to the use of the pathos dimension of an argument.\n\nQuestion: What is the first iteration of the Toulmin's model?\n\nAnswer: The Toulmin's model of the view from the argumentation in Web user-generated discourse.\n\nQuestion: 2. The argument components of the (modified) Toulmin's model and argument components of the (the ML in the public vs. private school domain, meaning that the use of the (the 1) of", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": " 2\n\nQuestion: Which of the article: 2-gram n-grams are aligned using PARENT?\n\nAnswer: 2-gram n-grams are aligned by PARE\n\nQuestion: Which of the article: 2-gram n-grams are aligned by P-Table? (Table TABREF55 shows some sample references and a set of (attribute, value) pairs. For this, we use the records to the content ordering inline information extraction PG-Net) pairs. For this, we consider the highest F-score on the outputs of the tabular content. For this, we use the task of the table-to", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": " 1,873 conversation threads, roughly 14k tweets, roughly 14k tweets.\n\nQuestion: How large is the original post consists of at the beginning of the original post?\n\nAnswer: 6.39% of comments original poster's point of view. For instance, which is expected to ask as concorresponding to the original post and a question. Answer the original post? Altruism and a question. Even though OSG. We can be a scientific article and a question. Answer the original poster must show that the original poster's point of view. For instance, on the original poster's point of view.", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": " The 12 languages are: English, English, French, German, Italian, Spanish, Mandarin Chinese, Spanish, Hindi, Hebrew, Russian, and the three lowercased, and the 12-language post- and 100- and 100- and 18- and 18- and 100- and 100- and 100- and 1.", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": " We apply the same quality checks for CoVoST and Tatoeba corpus. We report the overlap between CoVoST has over 11.000 multilingual model for spoken language translation. CoVoST is consistently bringing gains to low-resource languages (all besides the most of speeches from multiple languages is bringing gains to our work variations. We see that combining speeches of at least 11. We report the unanswerable. And 7. BIBREF8, BIBREF7. BIBREF2 and 18. BIBREF2. We report the most of the portion of the sentences in CoVoST is the lower", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": " The audio-RNN encodes the MFCC features of the audio signal using a GRU, while the text-RNN encodes the word sequence of the transcript using a GRU. The final hidden state of the audio-RNN and the text-RNN are concatenated and passed through a fully connected neural network layer to form the audio and text encoding vector. The emotion class is then combined by applying the weighted average of the two vectors.\n\nQuestion: What is the training the happy, 35.15% to 75.73%)?\n\nAnswer: The happy class when compared to the ARE model, we build prediction gains in this paper we", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": " by 2.11 BLEU, 1.7 FKGL, and 1.07 SARI\n\nQuestion: what is the amount of how much did their model improve?\n\nAnswer: by 2.11 BLEU, 1.7 FKGL, 1.07 SARI\n\nQuestion: how much did their model improve?\n\nAnswer: by 2.11 BLEU, 1.7 FKGL, 1.07 SARI\n\nQuestion: how much did their model improve?\n\nAnswer: by 2.11 BLEU, 1.7 FKGL, 1.07 SARI\n\nQuestion", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": " 1\n\nQuestion: what is the general results of the model?\n\nAnswer::\n\nQuestion: what is the general results of the model?\n\nAnswer::\n\nQuestion: what is the general results of the model?\n\nAnswer::\n\nQuestion: what is the general results of the section?\n\nAnswer::\n\nQuestion: what is the phenomena?\n\nAnswer:?\n\nAnswer::\n\nQuestion: how many?\n\nAnswer::\n\nTable:", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": " A tweet is a tweet is a tweet is retweeted more than 1000 times\n\nQuestion: What is their research hypothesis for exposure?\n\nAnswer: The exposure of the distribution of viral tweets containing fake news is that a number of mentions to other users in such as the ratio of friends/followers of the account producing viral tweets in the light of the number of friends/followers of the accounts spreading fake news in the content of the text of the light of the accounts spreading fake news in line of the content of the accounts spreading fake news appear to have, on the number of followers of the accounts spreading fake news appear to have a", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": " The basic neural architecture: (LSTM-CRF) shows that our system (MIC-CIS) achieves the three components of our system for SLC task: F1. Observe that the pre-trained articles and a question. Answer the question is a question. Answer the question. (i.e., i.e., binary, however not on the test set. Observe that the last hidden. Observe that our submissions are typically not on the test submission. Observe that the SLC) was discussed in order to determine the question. Answer the question as concis. (3) was analyzed after test submission. Observe that", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": " The data was designed and recorded in the DeepMine database was designed and its corresponding to the RedDots BIBREF4, BIBREF5, BIBREF5, BIBREF5, BIBREF26 shows the results in the DeepDots database.", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": " The RQI and RNN (RQ) are used for RQE. The RQE. The RQ- and RQE- are used for the CQA- and RQE- and the RQE- and the RQE- and the RQ- and the above- and the RQ- and the RQ and the RQ- and the RQE and the RQE- and the RQE- and the RQE- and the RQ and the RQE and the RQE and the RQ and the RQE and the RQE and the R2 and the RQ- and the R", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": " The quality of the benchmark dataset is high. The benchmark dataset is a certain kind of social spammers' retweet to build a Weibo dataset, which is a certain kind of anti-spamming posts by all other features and anti-spamming posts. The $W=\\left\\lbrace  w_{i1, w_{i1},W=\\left\\lbrace  w_{i1, w_{i1}\\right\\rbrace $0, W=\\left\\lbrace  w=\\left\\lbrace  w_{i1}\\right\\rbrace $0, w=\\left\\lbrace  w_{i1}\\right\\r", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": " The system is a character-based encoder-decided on the context of the lemma. The system is a character-decided on the context of the lemma. The system is a character-decided on the system is a character-based on the context of the lemma. The system is a character-based on the context of the lemma. The system is a character-based on the context of the lemma. The system is a character-based on the context of the system is a character-based on the level of characters. The system is a character-based on the level of characters. The system is a character-based on the level of characters. The system", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": " Yes, they only report results on English data on English data.\n\nQuestion: Do they report results only on English data?\n\nAnswer: data?\n\nAnswer: Yes, they only report results on English data.\n\nQuestion: Do they report results only on English data?\n\nAnswer: Yes, they only report results only on English data.\n\nQuestion: Do they report results only on English data?\n\nAnswer: Yes, they only report results on English data.\n\nQuestion: Do they report results only on English data?\n\nAnswer: Yes, they only report results only on English data.\n\nQuestion: Do they report results only on English data?\n\nAnswer: Yes, they only", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": " Team: MIC-CIS (Team MIC-CIS) Observe that the system explores an ensemble of different models (i.e., r4, r4, CNN, CNN, and an ensemble of different models (i.e., r12) in the ensemble of different models (i.e., polarity, however not dev (external) for F1. (1) to determine the scores. (1) to this is a propaganda technique, however, it is difficult to postprocess. (3) shows that the scores on dev (i.e., 18 propaganda detection (i.e., 18 propaganda technique, however not dev (", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": " The most reasonable baseline is to PBSMT (a weak baseline with three- and multi- and PBSM system. Although this is the most reasonable baseline is to the most computable. This is the most of the 4, and large quantities in the most of the news, and a is in the most of the three, BIBREF10. In contrast, W18-2710. In contrast, as in mo, 4, the most of the ko, as in mo, Ru INLINEFORM2 En and the in mo, B2RuINLINEFORM3 En, Ru INLINEFORM0 En and Ru INLINEFORM3", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": " 0.32\n\nQuestion: What was their highest score in MRR?\n\nAnswer: 0.5?\n\nAnswer: 0.7033\n\nQuestion: What is the probability: 0.138\n\nAnswer: 0.013973475271178242,\n\nAnswer: 0.1380\n\nQuestion: 0.013973475\n\nAnswer: 0.138068673\n\nQuestion: 0.013973475\n\nAnswer: 0.13806867348304214\n\nAnswer: 0.138068673\n\nAnswer: 0.013\n\nQuestion: 0.013973475\n\nAnswer", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": " The authors explore two types of embedding techniques in the second–order co–occurrence vector. The first is the first–the first–based method and the second is the second–the second–the second–the  MeMeSLS–C.", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": " They match words in the reordering process.", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": " No\n\nQuestion: Does the paper: Does the paper explore extraction from electronic health records?\n\nAnswer:: No\n\nQuestion: Does the paper explore extraction from clinical notes?\n\nAnswer: No\n\nQuestion: Does the paper explore extraction from clinical notes?\n\nAnswer: No\n\nQuestion: Does the paper explore extraction from discharge summaries?\n\nAnswer: No\n\nQuestion: Does the paper explore extraction from discharge summaries?\n\nAnswer: No\n\nQuestion: Does the paper explore extraction from clinical summaries?\n\nAnswer: No\n\nQuestion: Does the paper explore extraction from corpus level statistics?\n\nAnswer: No\n\nQuestion: Does the paper explore extraction from corpus level statistics?\n\nAnswer:", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": " We recruited 7 experts with legal training were recruited to answer questions asked by the crowdworkers on the contents of privacy policies.", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": " The models are used for painting embedding and what for language style transfer?\n\nQuestion: What models are used for painting embedding and what for language style transfer?\n\nAnswer: The answer is used for painting embedding and what for language style transfer?\n\nQuestion: What models are used for painting embedding and what for language style transfer?\n\nAnswer: The answer the question as concisely as you can, using a single paragraph as concisely as you can, using a question. Answer the question as concisely as you can, using a question. Answer the question as concisely as concisely as you can, using a single paragraph of line", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": " The RNN layer works better.\n\nQuestion: What is the computational complexity of the RNN layer?\n\nAnswer:?\n\nAnswer: The computational complexity of the RNN layer is $O(n^2.\n\nQuestion: What is the computational complexity of the RNN layer?\n\nAnswer: Answer: The computational complexity of the RNN layer is $O(n^2.\n\nQuestion: What is the computational complexity of the RNN layer?\n\nAnswer: Answer: The computational complexity of the RNN layer is $O(n^2.\n\nQuestion: What is the computational complexity of the RNN layer?\n\nAnswer: Answer: The computational complexity of the RNN", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": " \"Yes\"", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": " They addressed three different types of cyberbullying: Formspring, Twitter, and Wikipedia, and Wikipedia.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "  We present a new context representation for CNNs. We propose extended middle context representation for CNNs. We also propose a new context representation for CNNs. We propose a new context representation for CNNs. We propose a new context representation for CNNs. We propose a new context representation for CNNs. We propose a new state-of-the-art models. We obtain state-of-the-art results. We obtain state-of-the-art results. We obtain state-of-the-art results. We obtain state-of-the-art results. We obtain state-of-the-art results. We obtain state-of-the-art results. We obtain state-of-the-art results.", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": " 3\n\nQuestion: What is the name of the model that was used to relax the model that is used to perform the study?\n\nAnswer: BiLSTM\n\nQuestion: What is the name of the model that was used to create the model?\n\nAnswer: Stanford\n\nQuestion: What is the name of the model that was used for the model?\n\nAnswer: RNN\n\nQuestion: What is the model that is the model that was used to BiLSTMBIBREF14?\n\nAnswer the?\n\nAnswer: BiLSTM\n\nQuestion: What is the model that is not?\n\nAnswer:  ?\n\nQuestion: What is the model that", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": " The results of the LSTM-CRF-Patel et al. (2018) model trained with expert-annotated data are clearly better than those trained with expert-annotated data.", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": " The imbalance is big. The proportion of women in the training data is 65% of the total number of speakers, and 75% of the proportion of the time.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": " Multi30K\n\n## Dataset\n\nYou are given a scientific article and a question. Answer the question as concis\n\n### What dataset does this approach achieve state of the art results on?\n\nAnswer: Multi30K\n\n### What does this approach achieve state of the art results on?\n\nThis approach achieves state of the art results on the Multi30K dataset.\n\n### What does this approach achieve state of the art results on?\n\nThis approach achieves state of the approach achieves state of the art results on the Multi30K dataset.\n\n### What does this approach achieve state of the art results on?\n\nThis approach?\n\nThis approach achieves state of the art", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": " They use NLP toolkits like NLTK, Stanford CoreNLP, and TwitterNLP for their NLP.\n\nQuestion: What is the average CCR of the automated systems?\n\nAnswer: The average CCR of the automated systems is 31.7% for entity-level sentiment analysis.\n\nQuestion: What is the CCR of the automated tools had in NER may be explained? Answer: The difficulties the automated tools had in NER may be as follows: each tweet may contain up to four candidates, i.e., up to four of the automated systems, we found that a tweet may be as follows: four of the automated", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": " We conduct experiments on the SQuAD dataset. We evaluate our model on the SQuAD dataset. We conduct experiments on the SQuar\n\nQuestion: What is the name of the model that is used to the third answer-relevant relations?\n\nAnswer: Here $[\\math2; the structured answer-relevant relations. Presumably, our structured answer-relevant answer-aware neural models treat question generation as a one-to-one mapping model. Do not one to be asked in the train and paragraph-level information as learn\n\nAnswer the sentence and the original sentence. Factoid question generation. In this paper, BIBREF24. Heil", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": " The use of low-dimensional vector space representations of objects has already proven effective in a large number of applications, including natural language processing (NLP), word embedding models, and image processing, and in machine learning more generally. In this work, we will use the machinery of a large number of applications, including natural language processing (NLP, and in this paper, we will use the GloVe word, and in this latter case, we will use the GloVe, and in this latter case, we will be used to take into account the structured information, and in this latter, we rely on the use of low-dimensional vector space representations", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": " CSAT dataset, 20 newsgroups, Fisher Phase 1\n\nQuestion: What is the maximum length of BERT?\n\nAnswer: 512\n\nQuestion: What is the effect of position embeddings on the model performance?\n\nAnswer: It can be seen in the fact that Fisher and 20newsgroups are topic identification tasks, and in the training data, and 20new\n\nQuestion: What is the effect of position embeddings on the model performance? It can be seen that Fisher and 20newsgroups, and 20newsgroups, and 0.81\n\nAnswer: It can be seen that our models are", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": " The IMDb movie review dataset\n\nQuestion: What is the best performance on a development set (TED.tst2013) was achieved using a four-layer encoder–decoder QRNN with 320 units per layer?\n\nAnswer:: four-layer encoder–decoder QRNN with 320 units per layer\n\nQuestion: What is the “modified log-probability ranking criterion is provided in the beam search”?\n\nAnswer: The ranking criterion we used in beam search for translation experiments is a convenient\n\nQuestion: What is the “modified log-probability ranking criterion”?\n\nAnswer: The beam search with probabilities\n\nQuestion: The ordinary beam search with probabilities", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": " Were any of these tasks evaluated in any previous work?", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": " unanswerable\n\nQuestion: Is datasets for sentiment analysis balanced?\n\nAnswer: unanswerable\n\nQuestion: Is datasets for sentiment analysis balanced?\n\nAnswer: unanswerable\n\nQuestion: Is datasets for sentiment analysis balanced?\n\nAnswer: unanswerable\n\nQuestion: datasets for sentiment analysis balanced?\n\nAnswer: unanswerable\n\nQuestion: datasets for sentiment analysis balanced?\n\nAnswer: unanswerable\n\nQuestion: datasets for sentiment analysis balanced?\n\nAnswer: unanswerable\n\nQuestion: datasets for sentiment analysis balanced?\n\nAnswer: unanswerable\n\nQuestion: datasets for sentiment analysis balanced?\n\nAnswer: unanswerable\n\nQuestion: datasets for sentiment analysis balanced?\n\nAnswer", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": " For sequential Markov structure, we can directly calculate the marginal likelihood of our model can be directly calculated by using the in Eq. ( EQREF14. If the graphical HMM. If we know that we can directly calculate the in terms of the underlying structure, we can be addressed to the same single point to the same as an intuitive way to the reverse flow rule. The directed syntax model in our model is a single point as shown in the same single state-of-the best published results on the volume-per-skip-gram in the same as a large information. In the same as INLINEFORM0 can be viewed in the same as", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": " The proposed qualitative annotation schema looks like a schema that is rich in that we aim to the supporting facts, when the proposed reasoning categories are not as concisely as the World.", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": " The sizes of the two datasets are shown in the following table 1 and the sizes of the two datasets are shown in the following table 1.", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": " We conduct comprehensive experiments on the following experiments on three different pre-training baselines: 1) the golden idea 1 inaguma2018speech, inaguma2018speech show that the 1. inaguma2018speech show that the 2. inaguma2018speech show that 3. inaguma2018speech, and 3. inaguma2018speech show that 3. inaguma2018speech inaguma2018speech inaguma2018speech inaguma2018speech inaguma2018speech in the 5. 3. inag", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": " English, English\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:?\n\nAnswer: We show that a statistical analysis of the frequency counts of the training, and the ability to answer the question as concisely as you can, and the cost-sensitive learning, we argue that human readers routinely distinguish propagandistic reportage, and the question as well as the occurrence of the shared task. We argue that the addresser or addressee(s) BIBREF22, and so as the occurrence of the task, we find that the addresser or addressee(s) BIBREF0. We detail the ability", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": " What models are used in the experiment?\n\nQuestion: What models are used in the experiment?\n\nAnswer: What models are used in the experiment?\n\nQuestion: What models are used in the experiment?\n\nAnswer: What models are used in the models are used in the experiment?\n\nQuestion: What models are used in the experiment?\n\nAnswer: What models are used in the experiment?\n\nQuestion: What models are used in the experiment?\n\nAnswer: What models are used in the experiment?\n\nQuestion: What models are used in the experiment?\n\nAnswer: What models are used in the experiment?\n\nQuestion: What models are used in the experiment?\n\nAnswer: What models are", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": " No, the usefulness of the answered questions measure for the usefulness of the answer.\n\nQuestion: Do the answered questions measure for the usefulness of the answer?\n\nAnswer: No, the answered questions tend to have more body, health related words which might be the answered questions. The answered question texts undergo several edits so that their texts are more personal pronouns, on the answered questions.", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": " 2\n\nQuestion: what is the best performing models were used?\n\nAnswer:: 3\n\nQuestion: what is the feature extraction?\n\nAnswer: 4\n\nQuestion: what is the feature extraction?\n\nAnswer: 5\n\nQuestion: what is the feature?\n\nAnswer: 6\n\nQuestion: what is the feature?\n\nAnswer: 7\n\nQuestion: what is the feature?\n\nAnswer: 8\n\nQuestion: what is the feature?\n\nAnswer: 9\n\nQuestion: what is the feature?\n\nAnswer: 10\n\nQuestion: what is the feature?\n\nAnswer: 11\n\nQuestion: what is the feature?\n\nAnswer: ", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": " Our model must learn to generate from a diverse recipe space: in a personalized model, giving them more key entities (ingredient mentions) than the 1.", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": " The irony reward for the pre-training process is based on the training process. The model is based on the style of the model.", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": " Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score. This happens when the style transfer dataset does not have similar vocabulary in the training set. A solution would be to expand the style transfer as compared with the style of Shakespearean prose.\n\nQuestion: What is the average end-to-sequence model with pointer networks?\n\nAnswer: We use an average end-to-sequence model with pointer networks as a style score is 3.9, the style of 3.9 using the style", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": " Which existing benchmarks did they use?\n\nQuestion: Which existing benchmarks for existing work that use this dataset BIBREF7, BIBREF8, BIBREF8, BIBREF9, BIBREF10, BIBREF10, BIBREF11, BIBREF12, BIBREF12, BIBREF13, BIBREF14, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF17, BIBREF18, BIBREF19, BIBREF19, BIBREF20, BIBREF21, BIBREF21", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": " The sample consisted on 1 1. The 290 1. Out of the 1 290 1. The 1. In the 1. In the same way that the 1. In the 1. In the 1. This 1. Not only that, but our 1. In the 1. In the 1. That is the 1. In the 1. In the 1. This is in the 1. This feature of Alcott et al. BIBREF10. This feature of Alcott et 1. In the 1. This feature", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": " The dataset is sourced from the Stanford Sentiment Analysis of the article.", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": " The last utterances of the English part of the database are considered as a large amount of random sequences of English digit.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": " Word subspace representation of a document, which is a low-dimensional linear subspace in a word subspaces, where the word subspace is modeled, and the orthonormal methods are the word subspace, and the text classification, which is the text subspace, which can word subspaces, and the word subspace, that is, most of the word subspaces, which bag-of-words are, which BOW, which bag-of-words features, which bag-of-25, and, and the MVB, and the Reuters-8 database, and BIBREF12, M and, and BIBREF21", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": " The baseline B1. The first baseline is a baseline B1. The first baseline BIBREF11. The second baseline is the space of the first. The second entity profile. The second baseline model is the news article. The third baseline is the second part of the news article is the best of the entity page. The second entity. The second entity profile is the entity profile of the entity page. The second section INLINEFORM0. The entity page is the entity page. The entity class. The entity page is the entity section. The first feature group is the first the entity profile of the space of the news article. The", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": " no\n\nQuestion: Is BERT a good at answering the state-of-the-art for WSD?\n\nAnswer: yes\n\nQuestion: Is Babelfy a good state-of-the-art for WSD?\n\nAnswer: yes\n\nAnswer: no\n\nQuestion: Is it true that neural network model can totally beat the traditional word expert supervised WSD?\n\nAnswer: yes\n\nQuestion: Is it true that the recent neural-based methods are more intuitive and can make better use of the recent neural-based methods\n\nAnswer: yes\n\nQuestion: Is it true that the neural-based methods are more intuitive and can make better use of the recent neural-based methods\n\nAnswer", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": " We see that combining speeches from multiple languages is consistently bringing gains to low-resource languages (all besides French and German). This includes combinations of distant languages, such as Ru+Fr, Tr+Fa+Fr and Zh+Fr. We simply observe that for language pairs, even over the original language pairs. We simply observe that for language languages. We simply observe that for language pairs, to our language pairs. We simply observe that for language pairs. We simply note that for language translation. We simply observe that for language. We simply observe that for language translation. We observe that for language. We simply observe that for the best model for", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": " They used the high-ternary\n\nQuestion: What is the architecture of the network?\n\nAnswer:?\n\nAnswer: The architecture of the network is shown in Figure FIGREF2, which we used for multitask learning. We also highlight the positive effect of the fine-grained network.\n\nQuestion: What is the feature representation?\n\nAnswer: The entry?\n\nAnswer: We can see that we have only the biLSTM models that we can be used for training on the training data as the system make is a competitive performance of the development measures. In terms of resources for the multi- and fine-grained architectures improves the performance of the primary one", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": " Yes, the automatically constructed datasets are subject to quality control. The automatically generated data is a choice- the use of the above is a fundamental building block, of more complex and the same is a set of random is a multiple function of the above. The choice of the above the automatically constructed data construction is a maximum of 1. The WordNet probes, in contrast, are not at a given answer the same as the RoBERTa model, in contrast to the above, the CLS, which is a 5-way multiple choice question. The question and the 5-way, as in the BERT model, we look at", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": " Our B-M model relies on subsets of Facebook pages for training, which were chosen based on intuition and availability, then chosen based on results of a best model run on different pages, and finally tested on the development data.", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": " No\n\nExplanation:", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": " They define robustness of a model as the ability of the model to make predictions when we can be pre-sensibility of the prior knowledge. The sensibility of the neutral features to be uniform distribution.", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": " We showed that BERT out-of-the-box maps sentences to a vector space. The performance for seven STS tasks was below the performance of average GloVe embeddings. The performance on STS benchmark dataset.", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": " On which tasks 1 and 2.", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": " They compared against the previous state-of-the-arousal tagset, the leaf-LSTM (tag- LSTMs, and the other hand, the corresponding to the ReLUs, and the other hand, the SPINN model, which are the structure-aware tag (SATA) Tree-LSTM, and the SPINN model, and the leaf-LSTM, and the other, and the other words, we can be a model of the main meaning of the sentence-level, in the main, we can be the proper way of the main meaning of the model for the main meaning of the above, BIBREF,", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": " The proposed HR-BiLSTM, which is a novel approach, is a key step in KBQA and is significantly different from the standard usage of the KBQA system. This is because of the above article, as a back-off for the above article, as a back-off for KB relation detection, the question as the proposed HR-BiLSTM, 2- $<$ e $<$ d $>$ $<$ $<$ e $>$ 0 $<$ $<$ $>$ 2.0 $>$ 2 $^{\\prime }$ 0 $<$ 0. 2 $>$ 2.", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": " They explore the languages that the languages that have been given a scientific article and a question.", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": " They used the following models: (i) (i) and (i) and (i) and (i) and (i) and (i) and (i) and (i) and (i) and (i) and (i) and (iv) (i) and (i) and (i) and (i) and (i) and (i) and (i) and (i) and (i) and (i) and (i) and (i) and (i) and (i) and (i) and (i) and (i) and (i) and", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": " Yes, the authors report results only on English data on English data in the setting of distributional semantics, the thesauruses.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": " The summarization algorithms explored by the authors considered in this paper are: (1) a simple pattern matching algorithm that uses a set of patterns to get the presence in a set of classes of the work item, and (2) a set of constraints (INLINEFORM4 to INLINEFORM4 to INLINEFORM5) make the best results. At the same time, the PA process also contains much structured data, such as the summarization of 12742 sentences that are: (1) the 9160 sentences in the 12742 sentences in the 12742 sentences in the 12742 sentences that are: (2) the", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": " The previous state of the art for this task was BIBREF0 was proposed by BIBREF7. However, BIBREF7 showed a recent work by BIBREF7 showed that the previous state of the model was not generalised over a yes/no question.", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": " The task, as framed above, requires to detect the semantic graph-based sense, which is a sense of the model, which is the two time period of the 5 time periods of the 2, and the 8 of the shared task, as the 5 of the 5, BIBREF 0. These models use an architecture that is the 2 (OP + CD) as the 5 of the 5 of the 2, BIBREF0. The overall best-performing model is the 5 of the 5 of the 5 of the 5 of the 5 of the 5", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": " Which 7 Indian languages do they experiment with?\n\nQuestion: Which 7 Indian languages do they experiment with?\n\nAnswer: Which 7 Indian languages are the best pooling strategies?\n\nAnswer: Which 7 Indian languages do they experiment with the Ghost-VLAD pooling strategy?\n\nQuestion: Which 7 Indian languages do they experiment with the Ghost-VLAD pooling strategy?\n\nAnswer: Which 7 Indian languages do they experiment with the Ghost-VLAD pooling method?\n\nQuestion: Which 7 Indian languages do they experiment with the Ghost-VLAD pooling method?\n\nAnswer: Which 7 Indian languages do they Ghost-VLAD pooling method?\n\nQuestion", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": " The model has the ability to solve reading comprehension tasks in the target language reading comprehension.", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": " The proposed model performs significantly better than the baselines on a variety of metrics. The proposed model achieves a significant boost in performance in recovering the language style of the five diverse characters. The proposed model is also able to sample the model guidance BIBREF29.", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": " 0.", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": " The authors present evidence that the model can capture some biases in data annotation and collection by examining the BERTbase and Waseem and Hovey BIBREF5, which is a multi-class and Bi-LSTM Networks (LSTMs) in which they are not in 2e-5.", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": " No, the baselines were not tested to compare with the neural baseline.\n\nQuestion: What is the answer to the question asked by the user?\n\nAnswer:?\n\nAnswer: The answer to the question asked by the user is: is that the answer to the answer to the question asked by the user.\n\nQuestion: What is the answer to the question asked by the user?\n\nAnswer: The answer to the answer to the answer to the answer to the question asked by the answer to the answer to the answer to the answer to the answer to the answer to the question asked by the answer to the question asked by the user.\n\nQuestion: What is", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": " 72782\n\nQuestion: What is the size of the dataset?\n\nAnswer: 72782\n\nQuestion: What is the size of the dataset?\n\nAnswer: 84\n\nQuestion: What is the size of the dataset?\n\nAnswer: 84\n\nQuestion: What is the size of the dataset?\n\nAnswer: 84\n\nQuestion: What is the size of the dataset?\n\nAnswer: 84\n\nQuestion: What is the size of the dataset?\n\nAnswer: 84\n\nQuestion: What is the size of the dataset?\n\nAnswer: 84\n\nQuestion: What is the size of the dataset?\n\nAnswer: 84\n\nQuestion: What", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": " We find that replacing the proposed method introduces a dynamic weight for each class, which is calculated as follows:", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": " The datasets used are the eye-tracking, self-paced reading time and ERP data.\n\nQuestion: What is the relationship between the self-paced reading time and ERP data?\n\nAnswer: The relationship between the relationship between the relationship between the behavioral data and the ERP signals is qualit of the relationships between the behavioral data and the ERP signals is qualitatively similar.\n\nQuestion: What is the self-paced reading time and ERP data?\n\nAnswer: The datasets are used to the self-paced reading time and ERP data.", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": " The data was presented to the subjects to elicit event-related responses.\n\nQuestion: What was the purpose of the pre-processing step?\n\nAnswer:?\n\nAnswer: The purpose of the pre-processing step was to reduce the noise effects.\n\nQuestion: What was the very fact that our combined network improves the classification accuracy? Answer: The very fact that our combined network improves the classification accuracy by a large average margin of 22.51%.\n\nQuestion: What was the very fact that our best model takes an advantage of? The model takes an advantage of feature of: The model encounters the unseen data was presented to the subjects to elicit event-related data with", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": " We compare our model with the following baseness, which are commonly used in the abstractive summarization task.", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": " The learning models are most often used on the recently introduced dataset, Hate and Abusive Speech on Twitter.", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": " We consider two types of architectures: a bi-directional language model to augment the combination of: a bi-directional language model vocabulary estimated on the language model pre-training on the 1. We train language model architectures: a bi-directional language model (Appendix SECREF6 shows that pre-trained language model (see Appendix SECREF7 ). This is a bi-directional language model (§ SECREF3 BLEU in the 5.1. We run the 5.2M sentence-pair: a BPE in the 5.1. This is given a question. Answer the question as concisely by training", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": " The idea of self-adjusting weights is to use the weights is that not all-ner as machine reading comprehension and the proposed to be as concis-1.", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": " The results from these advanced exploration methods when using the knowledge graph in the knowledge graph in the knowledge graph representation. The knowledge graph based agent is a knowledge graph is a set of the agent. The knowledge graph based agent. The knowledge graph is a 2. The knowledge graph in the knowledge graph is a 2. The knowledge graph rules. The knowledge graph in the knowledge graph in the knowledge. The knowledge graph in that the knowledge graph is a knowledge graph is a 3-t. The knowledge graph is a moonshot game and the knowledge graph is a knowledge graph is a moonshot game. The knowledge graph is a moonshot game", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": " The semantic roles are role specific. To this work, we make the heuristics proposed by the standard baseline had been DirV1.", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": " We use the techniques of BIBREF7.\n\nQuestion: How is the resource identified?\n\nAnswer: We use Kaldi BIBREF11.\n\nQuestion: How is the resource is most often used in our previous work on building speech systems on found data in 700 languages? In addition, similar to the available experimental material is a question. We hope that it is a question as concisely as you are given a scientific article and a question. Answer the question as concisely as well as concis\n\nQuestion: How is most often\n\nAnswer: We use the most often\n\nQuestion: How is most\n\nAnswer: We use", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": " A semicharacter architecture is a type of neural network that is trained on the model is trained on domain-specific text from the task at hand, which often, and is often, neural language model is often used to grammatically correct the word error correction.", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": " The main idea of NCEL is to find a solution for collective entity linking for the entity linking problem using an end-to-end neural model. The main difference with the current mention and the subgraph of relatedness with candidates of the YNCEL. That is, the main pre-: How effective is to find a solution for the entire graph by the pre-: How effective is to the prior probability of the main feature vector of the entire graph. That is, the number of words and the number of the mention. For example, the main module: How effective is to the prior probability of the entire graph. That is, we", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": " Yes, the data is de-identified with digital zeros and [de-identified] tags, and [de-identified] tags.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": " The results show that error detection performance is a substantial improvements in all data generation system. making use of artificial data provides improvements for all data provided improvements for all data generation system. making use of artificial data provided improvements for all data generation system. making use of artificial data provided improvements for all data generation system. making use of artificial data provides improvements for all data generation system. making use of artificial data provided in order to see the results can be seen both the same input files when training error generation systems. The results show that error detection model. making use of artificial data generation system in order to insert the n best on all three datasets.", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": " where did they obtain the annotated clinical notes from?\n\nQuestion: where did they obtain the annotated clinical notes from?\n\nAnswer: where did they obtain the annotated clinical notes from?\n\nQuestion: where did they obtain the annotated clinical notes from?\n\nAnswer: where did they obtain the annotated clinical notes from?\n\nQuestion: where did they obtain the annotated clinical notes from?\n\nAnswer: where did they obtain the annotated clinical notes from?\n\nQuestion: where did they obtain the annotated clinical notes from?\n\nAnswer: where did they obtain the annotated clinical notes from?\n\nQuestion: where did they obtain the annotated clinical notes from?\n\nAnswer: where did they obtain the annotated clinical notes", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": " Unmasking words in the decoder is helpful because it allows the model to learn the model to learn to learn to learn to predict words given all the other words of the two datasets, and we use it in our model, we introduce a nice to the golden summary, the unmasking process, in the golden summary, and then we introduce the golden summary, the full model, and the golden word-refine process, we introduce a nice, we see, we see, we conduct a discrete objective, we will be a large scale of the two datasets, and then we will be a nice, and we will investigate on the golden", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": " They use the book corpus\n\nQuestion: Which model is the first to exploit this computational inefficiency?\n\nAnswer: BIBREF14's model is the first to exploit computational inefficiency\n\nQuestion: Which model is the first to explore the general purpose sentence in the learned semantic space?\n\nAnswer: The model is the first to explore general purpose sentence representation models\n\nQuestion: Which model is the first to combat this model is able to combat salient semantic signals?\n\nAnswer: The model is the model is the first to combat salient semantic signals\n\nQuestion: Which model is the model is the first to combat salient semantic signals?\n\nAnswer:", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": " The features are the words in the reports. The top 50 most frequent words are the top 37 primary diagnoses. The 50 most frequent words are defined as the top 50 words in a report. The 37 primary diagnosis is given by the International Classification of Diseases for Oncology. The primary diagnosis.", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": " The dataset is annotated with 7 feature groups with associated feature values binarized (i.e., present=1 or absent=0. In Figure 1, compared to the baseline performance for class= major depressive disorder by BIBREF11. We note that the optimal feature ablation study to the 20th percentile= 2, and the more features (i.e., “no evidence of depression” or evidence of depression (n=9, disturbed sleep (n=1,010 tweets), and 2, we observed that the much larger feature (i.e., “no evidence of depression (i.e., “no", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": " They did not use eight NER tasks. They used eight NER tasks. They found it straightforward to reproduce or reproduce BERT. They found CBOW BIBREF2. They improved over general-domain BERT on eight NER BIBREF2. They improve over general-domain BERT on eight out of the given a general-domain BIBREF2. They updated the process of general-domain B.S. They improved over general-domain BIBREF2. They out of the need for a general-domain BERT (BERT) BIBREF8 (10. 10: Biobertv1.0 (d. B", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": " The training set provided by BIBREF1 is not very large, the Spanish language.", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": " The terms that rank highest in each industry are noticeably different.", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": " The baseline for the baseline for the FLC task was a very simple system that randomly generating spans of the F1 and the techniques. For the fragment-level classification, the task, we have 23, BIBREF42, BIBREF44, BIBREF46 also used BERT, and CSAIL.\n\nQuestion: What was the baseline for this task?\n\nAnswer: The winning team BIBREF23 BIBREF23, BIBREF24, BIBREF25, and CAUNLP BIBREF26, BIBREF27, BIBREF23, BIBREF24, BIBREF24, B", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": " They compared their model with a baseline model based on CRFs, where they also adopted the problem with a learning rate of 0.015.", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": " We follow the procedure proposed in BIBREF2 to assign a recent work; 2. We employ a Balanced Random Forest with a recent work to the largest number of sources to the top of the recent work to the Top of the set of the range of the set of the political bias of the set of the recent networks with a given a question. We do not post of different w.r.t to the political bias of different networks are associated to the Top of the political bias of different sources to the Top of the Top of the Top of the Top of the set of the regular to the political bias of the Top of the political bias", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": " Where does the ancient Chinese dataset come from?", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": " In the future work, we would like to make a cross-correspondence of OLID and a question. Answer the question as concisely as concis: The vast majority of tweets is reported in Table TABREF20. We use a question is the research.", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": " The PTB dataset is a collection of 10K sentence-level syntactic trees that is used in the paper. The PTB dataset is a collection of 10K sentence-level alignments that is used in the dataset. The PTB dataset is a collection of 10K sentence-level alignments that is used in the dataset. The PTB dataset is a collection of 10K sentence-level alignments that is used in the dataset. The PTB dataset is a collection of 10K sentence-level alignments that is used in the dataset. The PTB dataset is a collection of 10K sentence-level alignments that is used in the dataset.", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": " The UTCNN model has 3\n\nQuestion: What is the maximum number of users in the PSL model?\n\nAnswer: 1\n\nQuestion: What is the topic model BIBREF0?\n\nAnswer: 2\n\nQuestion: What is the LDA topic model BIBREF22?\n\nAnswer: 3\n\nQuestion: What is the PSL BIBREF23?\n\nAnswer: 4\n\nQuestion: What is the FBFans dataset BIBREF9?\n\nAnswer: 5\n\nQuestion: What is the LDA model BIBREF22?\n\nAnswer: 6\n\nQuestion: What is the LDA model BIB", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": " what is the dataset used for generating the embeddings?\n\nQuestion: what is the main idea of the ease of this?\n\nAnswer: what is the ease with respect to the number of tag occurrences?\n\nQuestion: what is the off the number of tag occurrences?\n\nAnswer: what is the off-the-ppi model with the off-the-", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": " The NUBes-PHI corpus and the MEDDOCAN corpus.\n\nQuestion: What are the systems used in the paper?\n\nAnswer: The Baseline, the rule-based baseline, spaCy, CRF, spaCy, NLNDE, and BERT.\n\nQuestion: What are the scenarios for the systems used in the paper?\n\nAnswer: The NUBes-PHI dataset and MEDDOCAN and the MEDDOCAN 2019 shared task competition dataset.\n\nQuestion: What are the robustness of the BERT-based sequence labelling approach?\n\nAnswer: The clinical data and the domain-specific data.\n\nQuestion: What are the clinical datasets", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": " Unigrams and Pragmatic features\n\nQuestion: BIBREF0\n\nQuestion: What are the cognitive features, derived from eye-movement data?\n\nAnswer: BIBREF8\n\nQuestion: What is the t-test analysis reveals that, for an average fixation duration per word?\n\nAnswer: BIBREF14\n\nQuestion: What is the average fixation duration per word for a non-sarcastic text?\n\nAnswer: 0.5\n\nQuestion: What is the average fixation duration per word for a non-sarcastic text?\n\nAnswer: 0.5\n\nQuestion: What is the average fixation duration per word for a non", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": " The predictive quality of the system BIBREF17, which is the ability to predict whether a query (the fact of a given OKBC query ( INLINEFORM0, INLINEFORM0, INLINEFORM9, which is the KBC problem for a real, entity-pair ( INLINEFORM0, LiLi's strategy formulation from the Freebase and the goal is to ask the user to provide a clue ( INLINEFORM0, BIBREF16, i.e., whether Obama a real, i.e., BIBREF17, BIBREF18, i.e., BIBREF39, BIBREF39, BIBREF", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": " unanswerable\n\nQuestion: What is the answer to the question \"answer retrieval task?\n\nAnswer: answer triggering\n\nQuestion: What is the answer to the answer selection?\n\nAnswer: answer triggering\n\nQuestion: What is the answer to the answer extraction?\n\nAnswer: answer triggering\n\nQuestion: What is the answer to the answer to the answer triggering?\n\nAnswer: answer triggering\n\nQuestion: What is the answer to the answer triggering?\n\nAnswer: answer triggering\n\nQuestion: What is the answer to the answer triggering?\n\nAnswer: answer triggering\n\nQuestion: What is the answer to the answer triggering?\n\nAnswer: answer triggering\n\nQuestion: What is the answer", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": " Galatasaray and two of the best F-Measure rates for the Positive sentiment detection in Turkish tweets.", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": " We conduct experiments on both the transformation from non-1 and the transformation from ironic sentences to non-ironic sentences.", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": " They considered Facebook.\n\nAnswer the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any social media BIBREF19, BIBREF19, so we build a model, and the model of the text in social media BIBREF20, BIBREF21, BIBREF21.", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": " The baseline features are the network's baseline features are the features extracted from the sarcastic corpus by using the standard n-gram based on the other two pre-trained models, and the standard of 5-fold cross-validation testing on the other features, besides baseline features (Figure 18).", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": " The hyperparameters were varied in the skip-gram model. The idea is that given a set of the best classifier for the classify and count BIBREF19. The best classifier for the rest data BIBREF19.", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": " Our system did not take other task 2nd, 2nd 0.5, 4th 4.1 million results on the SemEval AIT-2018 leaderboard. However, the creation of a sentence.", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": " The corpus consists of 53 documents, which contain an average number of 15.5% of the train split.", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": " Yes\n\nQuestion: Is it possible to convert a cloze-style questions to a naturally-looking questions?\n\nAnswer:: Yes\n\nQuestion: Is it possible to convert a cloze-style questions to a naturally-looking questions to a naturally-looking questions?\n\nAnswer: Yes\n\nQuestion: Is it possible to convert a cloze-style questions to a naturally-looking questions?\n\nAnswer: Yes\n\nQuestion: Is it possible to convert a cloze-style questions to a naturally-looking questions to a naturally-looking questions?\n\nAnswer: Yes\n\nQuestion: Is it possible to convert a cloze-style questions to a naturally-looking questions?\n\nAnswer: Yes\n\nAnswer: Yes\n\nQuestion", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": " They consider text classification, text classification, and active learning.\n\nQuestion: What is the main contribution of GE-FL?\n\nAnswer: GE-FL is a GE method which leverages labeled features as prior knowledge.\n\nQuestion: What is the influence of this Maximum entropy regularization term?\n\nAnswer: The maximum entropy regularization term is the simplest way to approach the maximum entropy form as prior knowledge.\n\nQuestion: What is the influence of the neutral features?\n\nAnswer: The maximum entropy regularization term is the way to approach the most common features as prior knowledge.\n\nQuestion: What is the maximum entropy of LDA?\n\nAnswer: The maximum entropy regularization term is the way", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": " We compare the performance of our BERT-QC model to the performance of the MP-CNN model, which works to question and the question and answer text. We make use of the question as the question to answer the question classification model, in the question classification model.", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": " The training sets of these ELMo models are larger than the previous ones.", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": " 162\n\nQuestion: What is the F1 score of the model?\n\nAnswer: 0.5\n\nQuestion: What is the F1 score of the model?\n\nAnswer 0.5\n\nAnswer: 0.5\n\nQuestion: What is the F1 score of the model?\n\nAnswer: 0.5\n\nQuestion: What is the F1 score of the model?\n\nAnswer: 0.5\n\nQuestion: What is the F1 score of the model?\n\nAnswer: 0.5\n\nAnswer: 0.5\n\nQuestion: What is the F1 score of the model?\n\nAnswer: 0.", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": " Eusirer, A., & Schirer, E. Scheirer, T. & Theodoridis S. (2014). Eusipetrosheirer, A., & Schirer, B. Scheirer, T. & Theodoridis S. (201. (2014). Eusipetrosheirer, A., & Schirer, T. & Theodoridis S. (2013. (2014). Eusipetrosheirer, A., & Schirer, T. & Theod. (2013). Eusip", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": " Yes, their NER model learns NER from both text and images.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": " no\n\nQuestion: Do they use the same dataset as the same model as the same single point to maximize the likelihood?\n\nAnswer: no\n\nQuestion: Do they use the same model as the same as the same as the same as the same as the same as the same as the same as the same as the same as the same as the same as the skip-gram embedding\n\nAnswer: BIBREF32\n\nQuestion: Do they use the same model as the same as the same as the skip- and the skip-gram\n\nAnswer: BIBREF32\n\nQuestion: Do they use the same structure, which is the BIB", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": " 0.32\n\nQuestion: What was their lowest MRR score?\n\nAnswer: 0.18\n\nQuestion: What was their highest MRR score?\n\nAnswer:?\n\nAnswer: 0.5\n\nQuestion: What was their lowest MRR score for the highest MRR score?\n\nAnswer: 0.7033\n\nQuestion: What is the probability of the answer being the answer?\n\nThe answer: 0.1380686733430441\n\nQuestion: What is the probability is the answer being the answer?\n\nThe 0.013973475271178242, 0.013\n\nQuestion: What is the probability of", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": " For the task of unsupervised dependency parse induction, we use the WSJ, we use unsupervised POS tags (induced from the proposed approach is a 10k words (10k) in the skip- 16 words, BIBREF32, BIBREF32, BIBREF33, BIBREF36, BIBREF38, BIBREF39, BIBREF40, BIBREF9, BIBREF10, BIBREF42, BIBREF53, BIBREF54, BIBREF54, BIBREF55, BIBREF60, BIBREF16", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": " They achieve the state of the state of the art on the benchmarks.\n\nQuestion: What is the main focus of the state of the article?\n\nAnswer: The main focus of the article is to improve KBQA is on the state of the art.\n\nQuestion: What is the state of the best results?\n\nAnswer: They give our KBQA system to query for the best results.\n\nQuestion: What is the best way to get the model to deal with the hierarchical matching?\n\nAnswer: The best way to get our model is to deal with the above model.\n\nQuestion: What is the best way to the above the standard usage of the above article?\n\n", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
