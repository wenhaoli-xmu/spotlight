{"pred": " The ground truth for fake news is established by manually inspecting the text field of the tweets and labelling them as containing fake news or not containing them. The manual inspection is done by a single person in order to obtain a consistent annotation throughout the dataset.", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": " GhostVLAD is an extension of the NetVLAD approach, which we discussed in the previous section. The GhostVLAD model was proposed for face recognition by Y. Zhong [10]. GhostVLAD works exactly similar to NetVLAD except it adds Ghost clusters along with the NetVLAD clusters. So, now we will have a K+G number of clusters instead of K clusters. Where G is the number of ghost clusters, we want to add (typically 2-4). The Ghost clusters are added to map any noisy or irrelevant or irrelevant content into ghost clusters and are not included during the feature aggregation stage, as", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": " 0.688 to 0.691\n\nQuestion: What is the WAP of the ASR-processed transcripts?\n\nAnswer: 0.691\n\nQuestion: What is the WAP of the WAP of the ASR-processed transcripts?\n\nAnswer: 0.691\n\nQuestion: What is the WAP of the ASR-processed transcripts?\n\nAnswer: 0.691\n\nQuestion: What is the WAP of the WAP of the ASR-processed transcripts?\n\nAnswer: 0.691\n\nQuestion: What is the WAP of the ASR-processed transcripts?\n\nAnswer: 0.691\n\n", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": " Additional features and context are proposed.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": " They looked at the pages that were available to them, and chose different pages (and therefore different domains and stances), aiming at a balanced and with an eye to the nature of the datasets available for evaluation (see Section SECREF5). The choice of which pages to select posts from is far from trivial, and we believe this is actually an interesting aspect of our approach, as by using different Facebook pages one can intrinsically tackle the domain-adaptation problem (See Section SECREF6 for further discussion on this). The final collection of Facebook pages for the experiments described in this paper is as follows: FoxNews, CNN, ESPN", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": " Yes, the hashtag and SemEval datasets contain only English data.\n\nQuestion: What is the difference between the current state-of-the-art approach and our proposed approach?\n\nAnswer: The current state-of-the-art approach is a maximum-entropy classifier that predicts whether a hashtag is a space should be inserted at each position in the hashtag and is the current state-of-the-art. Our proposed approach is a pairwise neural ranking model that ranks the candidates that predicts a real number for each candidate segmentation pair.\n\nQuestion: What is the difference between the 24.6% error reduction on STAN and 16.6% on STAN?\n\nAnswer: The", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": " The task is defined as follows: Given a set of related documents, create a concept map that represents its most important aspects, satisfies a specified size limit and is connected. We define a concept map as a labeled graph showing concepts as nodes and relationships between them as edges. Labels are arbitrary sequences of tokens taken from the taken from the documents, making the summarization task extractive. A concept map BIBREF28.", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": " We used three benchmark datasets, namely CNN/DailyMail news highlights dataset, the New York Times Annotated Corpus (NYT), and XSum.\n\nQuestion: We used the three datasets to evaluate our model. We used the same evaluation protocol in the same way. We used the same model to evaluate our model. We used the same model to evaluate our model. We used the same model to evaluate our model. We used the same model to evaluate our model. We used the same model. We used the same model to evaluate our model. We used the same model to evaluate our model. We used the same model to evaluate our model", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": " The proposed approach is a word embedding based approach which is a non-parametric based alternative to handle polysemous words. However, this approach fails to consider entailment (asymmetry) and hence textual entailment recognition is necessary to capture lexical inference relations such as causality (for example, mosquito $\\rightarrow $ malaria$ malaria), hypernymy (for example, dog $\\models$ animal) etc. The proposed approach is a variant of max-margin objective which is a variant of the energy function to capture textual entailment (asymmetry) and hence the proposed approach is a word embedding based approach which is a non-parametric based", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": " The ensembles were formed by simply by simply by simply by simply by averaging the predictions from the 5 models.", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": " The Friends and EmotionPush are from the scripts of the Friends TV sitcom and the Facebook messenger chats, respectively.", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": " unanswerable\n\nQuestion: what is the name of the name of the main limitation of the aforementioned nmt models for text simplification?\n\nAnswer: unanswerable\n\nQuestion: what is the main limitation of the aforementioned n. pbmt-r, hybrid, sbmt-sari, dress, dress, dress, dress, dress is a phrase-based method with a reranking post-processing step. hybrid performs sentence splitting and deletion operations based on discourse representation structures, and then we do not provide any explanation.\n\nAnswer: unanswerable\n\nQuestion: what is the main limitation of the aforementioned nmt systems?\n\nAnswer: unanswerable\n\nQuestion", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": " The IMDb dataset of movie reviews by BIBREF11 while that for NER is Groningen Meaning Bank (GMB) by BIBREF12, containing 47,959 sentence samples. The GMB dataset has 17, 959 sentence samples. The GMB dataset has 17, 959 sentence samples. The GMB dataset has 17, 17, 959 sentence samples. The GMB dataset has 17, 17, 959 sentence samples. The GMB dataset has 17, 17, 959 sentence samples. The GMB dataset has 17, 17, 959 sentence samples", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": " 0. 0.98\n\nQuestion: What is the number of annotators?\n\nAnswer: 0\n\nQuestion: What is the number of annotators?\n\nAnswer: 0\n\nQuestion: What is the number of annotators?\n\nAnswer: 0\n\nQuestion: What is the number of annotators?\n\nAnswer: 0\n\nQuestion: What is the number of annotators?\n\nAnswer: 0\n\nQuestion: What is the number of annotators?\n\nAnswer: 0\n\nQuestion: What is the number of annotators?\n\nAnswer: 0\n\nQuestion: What is the number of annotators?\n\nAnswer: 0\n\n", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": " unanswerable\n\nExplanation: The article does not mention that the authors want to use the data for training and extracting relevant aspects of text understanding and annotation directly from the source, i.e. eye-tracking and brain activity signals during reading. However, they do not mention that they have already used the data that they have used for this purpose.", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": " The proposed system has defined our classification method based on features extracted from word vectors. For this, we have defined our classification method based on features extracted from word vectors. The Trigger for this is the system has a maximum time to wait. If a bot corrects the one or not. For CognIA we have defined our classification on the case of a member of the system. The Hub was implemented in C. The Hub is implemented in SABIA through the Chatbots with the case and it is a conversation is the number of classes and they are represented by the case. The CognIA system has also Chatbots with the case", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": " The best performance is \"Energy sector\" with $R^2$ of 0.45.\n\nQuestion: Which stock market sector achieved the worst performance?\n\nAnswer: The worst performance is \"Healthcare sector\" with $R^2$ of 0.15.\n\nQuestion: Which stock market sector achieved the best performance?\n\nAnswer: Which stock market sector achieved the worst performance?\n\nAnswer: Which stock market sector achieved the best performance?\n\nAnswer: Which stock market sector achieved the worst performance?\n\nAnswer: Which stock market sector achieved the worst performance?\n\nAnswer: Which stock market sector achieved the worst performance?\n\nAnswer: Which stock market sector achieved the", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": " RNN-based NMT, Transformer-NMT, SMT\n\nQuestion: what is the main difference between ancient and modern Chinese?\n\nAnswer: ancient and modern Chinese are both written in Chinese characters, but ancient Chinese is highly concise and its syntactical structure is different from modern Chinese\n\nQuestion: what is the reason for the reason why they used 2-layer RNN?\n\nAnswer: because the reason is because the reason for the reason for the reason for the reason for the reason for the reason for the reason for the reason for the reason for the reason for the reason for the reason for the reason for the reason for the reason for the reason", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": " The three regularization terms are: (1) a regularization term associated with neutral features; (2) a maximum entropy of class distribution regularization term; and (3) a KL divergence between reference and predicted class distribution regularization term.", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": " 1. SVM with unigram, which is a standard yet rather strong classifier for text features. 2. 2. SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words. 3. SVM with average transformed word embeddings, where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words. 4. two mature deep learning models on text classification, CNN and RCNN, where the hyperparameters are based on their work. 5. the above-mentioned models with comment information. 6. UTCNN without user information, representing", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": " They improved by several points.\n\nQuestion: What is the probability INLINEFORM0?\n\nAnswer:?\n\nAnswer:: The entry “Balikas et al.” stands for the winning system of the 2016 edition of the challenge BIBREF2, which to the best of our knowledge holds the state-of-the-art. Due to the winning system makes use of the available resources efficiently and improves the state-of-the-art performance. In conjunction with the fact that we found the optimal probability INLINEFORM0, this highlights the benefits of multitask learning over single task learning. Furthermore, as described above, the baseline systems were retrained on the union", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": " The $\\alpha $-entmax function is a sparse, which is a sparse, piecewise linear function. The $\\alpha $-entmax function is a generalization of the $\\alpha $-entmax function, which is a sparse, which is a softmax function. The $\\alpha $-entmax function is a sparse, which is a sparse, which is a sparse, which is a sparse, which is a sparse, which is a sparse, which is a sparse, which is a sparse, which is a sparse, which is a sparse, which is a sparse, which is a sparse, which is a sparse, which", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": " The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models. More precisely, the number of layers is $N=6$ with $h = 6$ parallel attention layers, or heads. The dimensionality of input and the scope of the context-agnostic MT system. Not all types of consistency errors that we want to fix emerge from a round-trip translation, BIBREF19, BIBREF21. Note that the model is the European Research and the European Union’s Horizon 2020, somewhat relaxing the main MT system is that of the European Union’s Horizon ", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": " We build on top of RAMEN a graph-based dependency parser BIBREF27. For the purpose of evaluating the contextual representations learned by RAMEN, we do not use part-of-speech tags. We use the trained word- and label scores. Table TABREF34 presents the Labeled Attachment Scores (LAS) for zero-shot dependency parsing.\n\nExplanation: We build on top of RAMEN a graph-based dependency parser BIBREF27. For the purpose of evaluating the contextual representations learned by RAMEN, we do not use part-of-speech tags. We use part-of-speech tags. We use part-of-speech tags. We", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": " The attention module is a many-to-many multi-task model contains two encoders as well as two decoders as well as two encoders as well as two encoders as well as two encoders as well as two encoders as well as two encoders as well as two encoders as well as two encoders as well as two encoders as well as two encoders as well as two encoders as well as two encoders as well as two encoders as well as two encoders as well as two encoders as well as two encoders as well as two encoders as well as many-to-many multi-task model where the", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": " Unigrams and Pragmatic features (emoticons, laughter expressions such as “lol” etc).\n\nQuestion: What is the average fixation duration per word?\n\nAnswer:: We observe distinct behavior during sarcasm reading, by analyzing the “fixation duration on the text” (also referred to as “dwell time” in the literature) and “scanpaths” of the readers' eye-movement data in the form of simple gaze-based features and complex features.\n\nQuestion: What is the average fixation duration per word?\n\nAnswer: We observe distinct behavior during sarcasm reading, by analyzing the “fixation duration on the text” (", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": " The baseline provided by the shared task organisers is a seq2seq model with attention. We use this baseline implementation as a starting point and achieve the best overall accuracy of 49.87 on task 2 by introducing three augmentations to the provided baseline system: (1) We use an LSTM to encode the entire available context; 2) We employ a multi-task learning approach with the auxiliary objective of MSD prediction; and 3) We train the auxiliary component in a multilingual fashion, over sets of two to three languages.\n\nQuestion: What is the main task of the system?\n\nAnswer?\n\nAnswer:: The main task of the", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": " unanswerable\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer:: unanswerable\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer:: unanswerable\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: unanswerable\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: unanswerable\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: unanswerable\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: unanswerable\n\nQuestion:", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": " The baselines are the models are the models that were used to compare the Jasper model's performance on a conversational English corpus. The Hub5 Year 2000 (Hub5'00) evaluation (LDC2002S09, LDC2005S13) is widely used in academia. It is divided into two subsets: Switchboard (SWB) and Callhome (CHM). The training data for both the acoustic and language models consisted of the 2000hr Fisher+Switchboard training data (LDC2004S13, LDC2005S13, LDC97S62). Jasper DR ", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "  22,880\n\nQuestion: How many blogs do they look at?\n\nAnswer:  41,094\n\nQuestion: How many posts do they look at?\n\nAnswer:  561,003\n\nQuestion: What is the best result on the development set?\n\nAnswer:  0.643\n\nQuestion: What is the best result on the development set?\n\nAnswer:  0.564\n\nQuestion: What is the best result on the development set?\n\nAnswer:  0.534\n\nQuestion: What is the best result on the development?\n\nAnswer:  0.477\n\nQuestion: What is the best result", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": " They create labels for each symptom/attribute, it can take on different linguistic expressions, defined as entities. Note that each given utterance could be categorized to more than one type, which are summarized in Table 4 along with the corresponding occurrence frequency statistics.", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": " The amount of data is needed to train the task-specific encoder is 4, 1, 3, and 3, respectively.", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": " We apply our models were trained on four machine translation tasks: IWSLT 2017 German $\\rightarrow$ English, KFTT Japanese $\\rightarrow$ English, WMT 2016 Romanian $\\rightarrow$ English, and WMT 2014 English $\\rightarrow$ German.\n\nQuestion: What is the difference between the @!START@$\\alpha $@!END@-entmax and @!START@$\\alpha $@!END@-entmax?\n\nAnswer: The @!START@$\\alpha $@!END@-entmax is a piecewise linear @!START@$\\alpha $@!END@-", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": " The improvement in performance for Estonian in the NER task is 0.03.\n\nQuestion: What is the difference between the differences between the tested languages depend more on the properties of the NER datasets than on the quality of embeddings?\n\nAnswer: The difference between the differences between the tested languages depend more on the properties of the NER datasets than on the quality of embeddings.\n\nQuestion: What is the difference between the tested languages depend more on the properties of the NER datasets than on the quality of embeddings?\n\nAnswer: The difference between the tested languages depend more on the properties of the NER datasets than on the quality of embeddings.\n\n", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": " They have a background in the humanities and the social sciences.\n\nQuestion: What is the purpose of the data they use?\n\nAnswer: The purpose of the data is to examine the purpose of the data is to understand the purpose of the data.\n\nQuestion: What is the purpose of the data they use?\n\nAnswer: The purpose of the purpose of the data they are looking for the purpose of the data they are the purpose of the purpose of the purpose of the data they are using the data to the purpose of the data they are the purpose of the data they are the purpose of the purpose of the data they are the purpose of the data they", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": " No, the paper is introducing a supervised approach to spam detection. The paper uses LDA to extract features to train a supervised classifier.", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": " The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.\n\nQuestion: Which languages are harder to classify a naive Bayes classifier?\n\nAnswer:: Classifying text only by language of a piece of text by its specific language label. Classifying text only by language group or family is a much easier task as reported in BIBREF8.\n\nQuestion: Which languages are reported to achieve 95.12% in the DSL 2015 shared task?\n\nAnswer:: The LID algorithm described in BIBREF24. The LID algorithm seems to be dependent on the", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": " 1) 2-layers regular-trained LSTM; 2) 2-layers distilled LSTM; 3) 9-layers regular-trained LSTM.\n\nQuestion: what is the difference between the 2-layers distilled LSTM and the 9-layers LSTM?\n\nAnswer: the 2-layers LSTM is a shallow network, and the 9-layers is a deep network. The 2-layers can match the 9-layers in the performance of the real-time requirement.\n\nQuestion: what is the difference between the 2-layers trained with the 9-layers and the 9-layers?\n\nAnswer: the", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": " The Wikipedia dataset is 29,794 articles, resulting in a dataset of 29,794 articles.", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": " The human evaluation was done by a group of 50 native people who were native speakers of both languages. They were well-versed in both Tamil and English and were asked to rate the translations on a 5-point scale of 5-point scale. The human evaluation was done on a collection of 100 sentences from the test set. The human evaluation was done on a collection of 100 sentences from the test set. The human evaluation was done on a collection of 100 sentences from the test set. The human evaluation was done on a collection of 100 sentences from the test set. The human evaluation was done on a collection of ", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": " Yes, they test their framework performance on commonly used language pairs, such as English-to-German.\n\nQuestion: What is the main difference between the proposed approach of BIBREF8 and the approach proposed by BIBREF11?\n\nAnswer: The main difference between proposed approach and the approach proposed by BIBREF11 is that the proposed approach is able to accommodate the attention layers seemlessly, while the approach proposed by BIBREF11 is able to accommodate the attention layers seemlessly.\n\nQuestion: What is the main difference between the mix-source system and the mix-multi-source strategy? Answer: The main difference between the mix-source system and the", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": " We provide additional experimental results on the robustness of learned communication schemes as well as well as in-depth analysis on the correlation between the retention rates of tokens and their properties, which we defer to Appendix and for space.", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": " Precision, Recall and F-measure are looked at for classification tasks.\n\nQuestion: What are the 3 classes of classes of sentences that are of interest in the PA corpus?\n\nAnswer: STRENGTH, WEAKNESS, SUGGESTION\n\nQuestion: What is the overall accuracy of the ILP-based summarization algorithm?\n\nAnswer: 0.75\n\nQuestion: What is the overall accuracy of the ILP-based summarization algorithm?\n\nAnswer: 0.75\n\nQuestion: What is the overall accuracy of the overall accuracy of the ILP-based summarization algorithm?\n\nAnswer: 0.75\n\nQuestion: What is the", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": " The source domain is the domain where we may have sufficient labeled data, and the target domain is the domain where we may have very few labeled data or no labeled data.\n\nQuestion: What is the aim to explicitly minimize the distance between the source and target feature representations?\n\nAnswer: : The aim to align the domain-specific words with the help of domain-invariant features (pivot features) and the aim to explicitly minimize the distance between the source and target feature representations ( INLINEFORM0 and INLINEFORM1 ) and the aim to align the aim to align the domain-specific words with the help of domain-inative words and the aim to explicitly minimize the", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": " They compare with RAN, QRNN, QRNN, QRNN, QRNN, LSTM, and NAS.\n\nQuestion: They compare with RAN, QRNN, QRNN, LSTM, and NAS.\n\nQuestion: what previous RNN models do they compare with?\n\nAnswer: They compare with RAN, QRNN, QRNN, LSTM, and NAS.\n\nQuestion: what previous RNN models do they compare with?\n\nAnswer: They compare with RAN, QRNN, QRNN, LSTM, and NAS.\n\nQuestion: what previous RNN models do they compare with?\n\nAnswer: They compare with RAN, QRNN, QR", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": " The data-driven sequence-to-sequence model is a neural encoder–decoder model with attention model with attention BIBREF9. The model consists of two main parts: the encoder compresses each source grapheme sequence INLINEFORM0 into a fixed-length vector. The decoder, conditioned on this fixed-length vector, generates the output phoneme sequence INLINEFORM1. The encoder and decoder are both implemented as recurrent neural networks, which have the advantage of being able to process sequences of arbitrary length and use long histories efficiently. They are trained jointly to minimize cross-entropy on the training data. We had our best results when the language ID token was trained", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": " The results for BERT, BIBREF12, and XLNet, BIBREF12, are taken from Khandelwal and Sawant (BIBREF12). The results for RoBERTa are taken from the original paper (BIBREF22). The results for BERT are taken from the original paper (BIBREF12). The results for XLNet and RoBERTa are averaged across 5 runs for statistical significance. The results for BERT are taken from the original paper (BIBREF12). The results for XLNet and RoBERTa are averaged across 5 runs for statistical significance. The results", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": " English, Spanish, Finnish, and German.\n\nQuestion: What are the three stress tests they run?\n\nAnswer: The competence test tests they run are competence test, distraction test, and noise test.\n\nQuestion: What are the three stress tests they run?\n\nAnswer: The stress tests they run are competence test, distraction test, and noise test.\n\nQuestion: What are the three stress test they run?\n\nAnswer: The stress tests they run are competence test, competence test, distraction test, and noise test.\n\nQuestion: What are the three stress test they run?\n\nAnswer: The stress tests they run are competence test, competence test, distraction test,", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": " We have focused only on English language posts, but the character model requires no language specific preprocessing to a minimum number of out-of-vocabulary words.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": " Yes, they use Glove embeddings.\n\nQuestion: What is the model compared to the previous state of the art method?\n\nAnswer: The model is 21% better than the previous state of the art method.\n\nQuestion: What is the basic seq2seq model?\n\nAnswer: The basic seq2seq2seq model is the vanilla encode-attend-decode model.\n\nQuestion: What is the model with only bifocal attention?\n\nAnswer: The model with only bifocal attention is the model without gated orthogonalization.\n\nQuestion: What is the model with limited data?\n\nAnswer: The model with limited data is the model from the target domain.\n\nQuestion", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": " unanswerable\n\nQuestion: What is the difference between the current approach to task-oriented dialogue and the approach presented in this paper?\n\nAnswer: unanswerable\n\nQuestion: What is the difference between the current approach and the approach presented in this paper?\n\nAnswer: unanswerable\n\nQuestion: What is the difference between the current approach and the approach presented in this paper?\n\nAnswer: unanswerable\n\nQuestion: What is the difference between the current approach and the approach presented in this paper?\n\nAnswer: unanswerable\n\nQuestion: What is the difference between the current approach and the approach presented in this paper?\n\nAnswer: unanswerable\n\n", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": " They obtain psychological dimensions of people's psychological dimensions by using the distribution of the individual words in a category, which is a lexical resources, such as Roget or Linguistic Inquiry and Word Count.", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": " The ML methods aim to identify the argument components in the logical dimension of an argument, i.e., the logos dimension.\n\nQuestion: What is the main focus of the ML methods?\n\nAnswer: The main focus of ML is to identify argument components in the document-level distinction of an argument, i.e., the pathos dimension.\n\nQuestion: What is the main focus of ML?\n\nAnswer: The main focus of ML is to the pathos dimension of an argument, i.e., the logos dimension.\n\nQuestion: What is the main focus of ML?\n\nAnswer: The main ML is the main focus on the pathos dimension of an argument, i", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": " 2\n\nQuestion: What is the main focus of the article?\n\nAnswer:: The article is about the article is on the automatic evaluation of table-to-text systems when the references divergent references divergent references in WikiBio.\n\nQuestion: What is the main focus of the article about?\n\nAnswer:: The article is about the automatic evaluation of table-to-text systems when the references divergent references in WikiBio.\n\nQuestion: What is the main focus of the article?\n\nAnswer: The article is about the automatic evaluation of table-to-text systems when the divergent references in WikiBio.\n\nQuestion: What is the main focus of the article?\n\n", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": " The Twitter dataset contains 1,873 conversation threads, roughly 14k tweets.\n\nQuestion: How large is the ratio of potentially therapeutic conversations in Twitter?\n\nAnswer: The ratio of potentially therapeutic conversations that lead to the increase of sentiment score of the original poster is 0.", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": " Multi-SimLex, and we also cover the (yue) and (est (zeta) languages. The 12 languages are: English, Spanish, Mandarin Chinese, French, French, Russian, Russian, Polish, Finnish, Hebrew, German, Hebrew, and English, and the 12 languages are: English, and the 12 languages are: English, French, Spanish, Russian, Mandarin Chinese, and English, and the 12 languages are: English, French, French, and the 12 languages are: English, French, and English, and the 12 languages are: English, English, English, and", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": " The first dataset is an expanded version of the annotated Wikipedia conversations dataset from BIBREF9. This dataset uses carefully-controlled crowdsourced labels, strictly filtered to ensure that the conversations are civil up to the moment of a personal attack. The second dataset is constructed from the subreddit ChangeMyView (CMV that does not use post-hoc annotations. Instead, the prediction task is to forecast whether the conversation will be subject to moderator action in the future.\n\nQuestion:", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": " unanswerable\n\nQuestion: What is the name of the tool used for the criminal law BIBREF15 micro-thesaurus?\n\nAnswer: : unanswerable\n\nQuestion: What is the name of the tool for the criminal law BIBREF15 micro-thesaurus?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the tool for the criminal law BIBREF15 micro-thesaurus?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the tool for the criminal law BIBREF15 micro-thesaurus?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the tool for", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": " We run the same quality checks for TT as for CoVoST but we do not find poor quality translations according to our criteria. Finally, we report the overlap between CoVo transcripts and TT transcripts and TT sentences in Table 2. We see that CoVoST has over 11 languages, to our knowledge, contributes the first public Dutch ST model for spoken language translation. CoVoST is free to use, and we do not find poor $\\textrm {CoefVar}_{MS}$ than all individual languages.", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": " The audio-RNN encodes MFCC features from the audio features from the audio signal using equation EQREF2. The text-RNN encodes the word sequence of the transcript using equation EQREF2. The final hidden states of the text-RNN are also passed through another fully connected neural network layer to form a textual encoding vector T. Finally, the emotion class is predicted by applying the softmax function to the concatenation of the vectors A and T and T and T.", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": " by 2.11 BLEU, 1.7 FKGL and 1.07 SARI\n\nQuestion: what is the main limitation of the amount?\n\nAnswer: by 2.11 BLEU, 1.7 FKGL and 1.07 SARI\n\nQuestion: what is the main limitation of the amount of vocabulary?\n\nAnswer: by 2.11 BLEU, 1.7 FKGL and 1.07 SARI\n\nQuestion: what is the main limitation of the amount of vocabulary?\n\nAnswer: by 2.11 BLEU, 1.7 FKGL and 1.07", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": " 1\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 1\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 1\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 1\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 1\n\nQuestion: how many humans evaluated the results evaluated the results?\n\nAnswer: 1\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 1\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 1\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 1\n\nAnswer: 1\n\nQuestion: how many humans", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": " A tweet is their definition of tweets going viral is if it was retweeted on 290 290 1000 times.\n\nQuestion: What is their definition of a tweet containing fake news?\n\nAnswer: What is their definition of a tweet containing fake news is if its text field within the tweets that do not contain fake news.\n\nQuestion: What is their definition of a tweet containing fake news?\n\nAnswer: What is their definition of a tweet containing fake news is if its text field within the tweets that do not contain fake news.\n\nQuestion: What is their definition of a tweet containing fake news?\n\nAnswer: What is their definition of a tweet containing", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": " BERT\n\nExplanation: The basic neural architecture perform best by itself is BERT. The BERT. The BERT is a multi-layered bidirectional transformer encoder. It is a neural network that is trained to predict the next word in a sentence. It has been shown to be able to learn the meaning of words and their context, and it can be used to generate propaganda. The BERT is a neural network is a neural network that is trained to predict the next word in a sentence. It has been shown to be able to learn the meaning of words and context, and it can be used to generate propaganda. The BERT", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": " The data collection was done using an Android application. The application. The application was developed by the Sharif DeepMine company. The application was designed to collect speech from at least a few thousand speakers, enabling research and development and development of deep learning methods. The project started at the beginning of 2017, and after designing the database and the development of the Android and server applications, the data collection began in the middle of 2017. The project was completed in the end of 2018 and the final and cleaned version of the database was released at the beginning of 2019.", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": " The RQE is a deep learning method that uses a neural network to recognize the relationship between two questions. The RQE is a supervised learning method that uses a dataset of sentence pairs (i.e. SNLI, MultiNLI, Quora, Quora, Quora, RTE, cQA-2016 and cQA-2017) to learn the similarity between questions. The RQE is a deep learning model consists of three 600d ReLU layers, with a bottom layer takes the concatenated sentence representations as input and a top layer feeding a softmax classifier. The RQE is a feature-based method that provides an Accuracy of", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": " The benchmark dataset is a dataset that is used to evaluate the performance of a particular model or compare the performance of different models. The quality of the benchmark dataset is high is a dataset that is used to evaluate the performance of a particular models or compare the performance of different models. The quality of the benchmark dataset is high is a dataset that is used to evaluate the performance of particular models or compare the performance of different models. The quality of the benchmark dataset is a dataset that is used to evaluate the performance of particular models or compare the performance of different models. The quality of the benchmark dataset is a dataset that is used to evaluate the performance of", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": " The MSD (auxiliary-task) decoder is an LSTM one component at a time, e.g. the tag PRO;N, NOM, SG, 1 is predicted as a sequence of four components, PRO, NOM, SG, 1.\n\nQuestion: What is the number of languages does the system is an encoder-decoder model?\n\nAnswer::  The system is an encoder-decoder on character sequences. It takes a lemma as input and generates a word form. The process is conditioned on the context of the context of the lemma. The baseline system concatenates embeddings for context word forms, lemmas and MSDs into a context", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": " No, they report results on three datasets in three datasets, including FSD (social media), Twitter (social media), and Google (news articles).", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": " The best performing model among author's submissions is the ensemble of (r19) for SLC task, which is a relax-voting ensemble of predictions of the three different models: Logistic Regression, CNN and BERT. The F1 score on dev (external) is 0.673. The best performing model among author's submissions is the ensemble+ of (II and IV) and IV) for FLC task, which is a multi-granularity and multi-tasking architecture. The F1 score on dev (external) is 0.673.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": " The baseline Transformer model (b3) achieved BLEU scores of 6.7, 6.8, and 6.9 for Ja INLINEFORM0 En, Ru INLINEFORM1 En, and Ru INLINEFORM2 En, respectively.", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": " '0.7033\n\nQuestion: What was the answer returned by the model for the model when the system 'UNCC_QA_1' was trained on BioASQ trained at 'SQuAD' 2.0 data?\n\nAnswer: 'SQuAD' 2.0 BIBREF7 for Context is a question word is being returned as the 'LAT'.\n\nQuestion: What was the answer: 'SQuAD' 2.0 BIBREF7. BIBREF0. For the word 'How', e.g. 'Answer: 'BioASQ' BIBREF8. When", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": " The paper explores the use of second–order co–occurrence vectors, which are based on the distributional methods of co–occurrence vectors. The second–order co–occurrence vector is a second–order co–occurrence vector that is used to quantify the similarity between the concepts as the LCS (CHD) relations. The relatedness of each term pair is annotated by the four–order co–occurrence vector. The results show that the results show that the results show that the results are not the results show that the results show that the results show that the results show that the results show that the results show that", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": " They match words by using a bilingual dictionary (Google Translate word translation in our case).", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": " No\n\nQuestion: Does the paper explore extraction of relations between entities?\n\nAnswer: Yes\n\nQuestion: Does the paper explore extraction of relations between entities?\n\nAnswer: Yes\n\nQuestion: Does the paper explore extraction of relations between entities?\n\nAnswer: Yes\n\nQuestion: Does the paper explore extraction of relations between entities?\n\nAnswer: Yes\n\nQuestion: Does the paper explore extraction of relations between entities?\n\nAnswer: Yes\n\nQuestion: Does the paper explore extraction of relations between entities?\n\nAnswer: Yes\n\nQuestion: Does the paper explore extraction of relations between entities?\n\nAnswer: Yes\n\nQuestion: Does the paper explore extraction of relations between entities?\n\nAnswer:", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": " We recruited seven experts with legal training to construct answers to questions posed by crowdworkers.", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": " The painting embedding is done using a CNN-RNN generative model. The language style transfer is done using a seq2seq model.\n\nQuestion: What is the difference between the average content, creativity and similarity to Shakespearean style?\n\nAnswer: The average content is the average of the scores given by the students. The creativity is the average of the scores given by the students. The similarity to Shakespearean style is the average of the scores given by the students.\n\nQuestion: What is the average content, creativity and similarity to Shakespearean style?\n\nAnswer: The average content is the average of the scores given by the students. creativity is the average", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": " The transformer layer works better.\n\nQuestion: What is the maximum length of the input sequence for BERT?\n\nAnswer: The maximum length of the input sequence for BERT is 512.\n\nQuestion: What is the difference between the pre-trained BERT features and the fine-token prediction?\n\nAnswer: The pre-trained BERT features are the pooled output from the pooled output from the final transformer block, while the fine-token prediction is the most frequently predicted class.\n\nQuestion: What is the difference between the pre-trained BERT features and the fine-tuned BERT features?\n\nAnswer: The pre-trained BERT features are the pooled output from the", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": " unanswerable\n\nQuestion: What is the key problem that the data enrichment method aimed at solving?\n\nAnswer:: extracting inter-word semantic connections from each given passage-question pair\n\nQuestion: What is the key problem of the problem of the data enrichment method?\n\nAnswer:: the key problem is to determine whether a WordNet is a lexical database of English, where words are organized into the pre-extracted general knowledge to construct the enhanced context embeddings\n\nQuestion: What is the key of the data enrichment layer?\n\nAnswer: the key problem of a semantic relation is the key problem is aimed at fusing the passage context embeddings to the passage- is", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": " They addressed three different types of cyberbullying: personal attack, racism, racism, and racism, and sexism.\n\nQuestion: What are the three types of social networks they used?\n\nAnswer: They used three different types of social networks: teen oriented Q&A forum (Formspring), large microblogging platform (Twitter), and collaborative knowledge repository (Wikipedia talk pages).\n\nQuestion: What is the effect of the use of swear words and anonymity?\n\nAnswer: Depending on the topic of cyberbullying, vocabulary and perceived meaning of words vary significantly across SMPs. For example, in our experiments we found that for word `fat', the most", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": " 1. The extended middle context uses all parts of the sentence (the relation arguments, the relation arguments, left of the relation arguments, between the arguments, right of the arguments) and pays special attention to the middle part.", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": " Answer: 4\n\nQuestion: What is the name of the model that is used to create the grapheme clusters?\n\nAnswer: uniseg\n\nQuestion: What is the name of the model that is used to create grapheme clusters?\n\nAnswer: uniseg\n\nQuestion: What is the name of the model that is used to create grapheme clusters?\n\nAnswer: uniseg\n\nQuestion: What is the name of the model that is used to create grapheme clusters?\n\nAnswer: uniseg\n\nQuestion: What is the name of the model that is used to create grapheme clusters?\n\nAnswer: uniseg\n\nQuestion:", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": " The results show that the model trained with expert annotations achieve 2% higher F1 than the model trained with crowd annotations. This indicates that the expert annotations are higher quality.", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": " 65% of the speakers are men, speaking more than 75% of the time.\n\nQuestion: What is the impact of the observed gender on ASR performance?\n\nAnswer: Gender bias varies across speaker's role and speech spontaneity level. Performance for Punctual speakers respectively spontaneous speech seems to reinforce this gender bias with a WER increase of 27.2% respectively 31.8% between male and female speakers.\n\nQuestion: What is the conclusion?\n\nAnswer: Being aware of the demographic skews our data set might contain is a necessary step to control the tools we develop.\n\nQuestion: What is the conclusion?\n\nAnswer", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": " Multi30K\n\n## Dataset\n\n### (Source: 2018)\n\nQuestion: What is the name of the model used to get a set of 10-best samples from the first pass decoder?\n\nAnswer: : Adam\n\nQuestion: What is the name of the model used to tag the entire source sentences using the spacy toolkit?\n\nAnswer: : spacy\n\nQuestion: What is the name of the model to get an average of 10-best results of our source degradation experiments are shown in Table?\n\nAnswer: : Meteor\n\nQuestion: What is the name of the model to get an of 10-best samples from the first pass decoder", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": " They use NLP toolkits like BIBREF23, BIBREF17, and BIBREF18 for entity-level sentiment analysis; in the second set, BIBREF17, BIBREF17, BIBREF19, BIBREF20, BIBREF26, and BIBREF18 for named-entity recognition.\n\nQuestion: Which toolkits do they use?\n\nAnswer: They use NLP toolkits like BIBREF23, BIBREF17, and BIBREF18 for entity-level sentiment analysis; in the second set, BIBREF17, BIBREF19, BIBREF20, B", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": " We conduct experiments on the SQuAD dataset, which is a reading comprehension dataset. It contains 536 Wikipedia articles and 100k crowd-sourced question-answer pairs. We split the dataset into a development set into a development set and a test set, and the original SQuAD training set into a training set and a test set. We also filter out questions which do not have any overlapped tokens with the corresponding sentences and perform some preprocessing steps, such as tokenization and sentence splitting. The data statistics are given in Table 1.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": " There are many approaches to the problem of representing geographic locations using low-dimensional vector space embeddings. One of the most popular methods is the GloVe model, which was designed to learn word vectors by learning word vectors. Another popular method is the Skip-Soil, which is a method that uses the SVM to learn POI data to train the model. The method of CrossMap was trained with the data of the CBOW model to predict the location of the location of the model. The method of the CrossMap method was trained with the SVM to learn the POI data of the data of the CrossMap method was trained with the SVM to", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": " no\n\nQuestion: What is the main difference between the proposed model and the proposed model?\n\nAnswer: The main difference is the additional binary classifier added in the model justifying whether the question is unanswerable.\n\nQuestion: What is the main difference is the additional binary classifier added in the model justifying whether the model justifying whether the question is unanswerable?\n\nAnswer: The main difference is the main difference is the additional binary classifier added in the model justifying whether the question is unanswerable.\n\nQuestion: What is the main difference is the main difference is the additional binary classifier added in the model justifying whether the model justifying", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": " 20 newsgroups for topic identification task, consisting of written text; CSAT dataset for CSAT prediction, consisting of spoken transcripts (automatic via ASR); Fisher Phase 1 corpus for topic identification task, consisting of spoken transcripts (manual).\n\nQuestion: What is the main contribution of the paper?\n\nAnswer?\n\nAnswer: We propose two methods for long documents using BERT model: RoBERT and ToBERT. We evaluated our experiments on two classification tasks - customer satisfaction prediction and topic identification using 3 datasets: CSAT, 20newsgroups and Fisher. We observed that ToBERT outperforms RoBERT on pre-trained B", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": " The IMDb movie review dataset BIBREF17. The dataset consists of a balanced sample of 25,000 positive and 25,000 negative reviews, divided into equal-size train and test sets, with an average document length of 231 words BIBREF18.\n\nQuestion: What is the best performance on a development set (TED.tst2013) was achieved using a four-layer encoder–decoder QRNN with 320 units per layer?\n\nAnswer: The best performance on a development set (TED.tst2013) was achieved using a four-layer encoder–decoder QRNN with 320 units per layer.\n\nQuestion: What is", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": " Were any of these tasks evaluated in any previous work? Yes, yes, yes, yes, and the previous work is BIBREF1, BIBREF1, BIBREF2, and BIBREF3.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": " unanswerable\n\nQuestion: Is datasets for sentiment analysis balanced?\n\nAnswer: unanswerable\n\nQuestion: Is datasets for sentiment analysis balanced?\n\nAnswer: unanswerable\n\nQuestion: Is datasets for sentiment analysis balanced?\n\nAnswer: unanswerable\n\nQuestion: Is datasets for sentiment analysis balanced?\n\nAnswer: unanswerable\n\nQuestion: Is datasets for sentiment analysis balanced?\n\nAnswer: unanswerable\n\nQuestion: Is datasets for sentiment analysis for sentiment analysis balanced?\n\nAnswer: unanswerable\n\nQuestion: Is datasets for sentiment analysis for sentiment analysis balanced?\n\nAnswer: unanswerable\n\nQuestion: Is datasets for sentiment analysis balanced?\n\nAnswer: unanswer", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": " For ease of exposition, we explain the learning algorithm in terms of Markov structure without loss of generality. As shown in Eq. ( EQREF17 ), the optimization challenge in our approach comes from the intractability of the marginalized emission factor INLINEFORM0. If we can marginalize out the latent state INLINEFORM1 and compute INLINEFORM2, then the posterior and marginal likelihood of our Markov-structured model can be computed with the forward-backward algorithm. We can apply Eq. ( EQREF14 ) and obtain : INLINEFORM3 : DISPLAYFORM0\nBy using the change of variable rule to the integration, which allows", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": " The proposed qualitative annotation schema looks like a taxonomy of features that introduce variance between the supporting facts and the supporting facts and the answer.", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": " The sizes of both datasets are shown in Table 1.", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": " We conduct experiments on the experiments on three pre-training baselines: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, in which both the encoder and decoder and decoder are pre-trained.", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": " unanswerable\n\nQuestion: What is the name of the dataset used in this paper?\n\nAnswer: unanswerable\n\nQuestion: What are the keywords of the PubMed ID(s)?\n\nAnswer: unanswerable\n\nQuestion: What are the keywords for the minimum and maximum p-values and their interpretations for ten such runs of each of these datasets?\n\nAnswer: unanswerable\n\nQuestion: What are the minimum and maximum p-values for similarity?\n\nAnswer: unanswerable\n\nQuestion: What are the minimum and maximum p-values and their interpretations for ten such runs of each of these datasets?\n\nAnswer: unanswerable\n\nQuestion: What are the minimum", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": " The performance of all systems for the OTH class is 0. This poor performances can be explained by two main factors. First, unlike the research presented in this paper was partially supported by an ERAS fellowship awarded by the University of Wolverhampton. This paper presents a new dataset with annotation of type and target of offensive language in social media and is reported in Table TABREF18. The vast majority of the research presented in this paper was partially supported by an offensive language in social media and is reported in Table TABREF18. The vast majority of the research presented in this paper presents a new dataset with annotation of type and target of", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": " No, the usefulness of the answer is measured by the number of upvotes it receives by the answer.", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": " We focus on the word vector representations (word embeddings) which are created specifically for the task of emotion analysis. We use the word embeddings created by the Twitter BIBREF19 and the Edinburgh BIBREF20 BIBREF20 corpora. We also use the emoji embeddings BIBREF21 which are learned from the emoji descriptions.", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": " We observe that personalized models make more diverse recipes than baseline. They thus perform better in BLEU-1 with more key entities (ingredient mentions) present, but worse in BLEU-4, as these recipes are written in a personalized way and deviate from gold on the phrasal level. Similarly, the `Prior Name` model generates more unigram-diverse recipes than other personalized models and obtains a correspondingly lower BLEU-1 score.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: In this paper, we propose a novel task: to generate personalized recipes from incomplete input specifications and user histories. We show", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": " The irony reward for the combination of rewards for reinforcement learning is defined as: DISPLAYFORM0\n\nQuestion: What is the harmonic mean of irony reward and sentiment reward?\n\nAnswer:: The harmonic mean of irony reward and sentiment reward is defined as: DISPLAYFORM0 Question: What is the harmonic mean of irony reward and sentiment reward? The harmonic mean of irony reward and sentiment reward is defined as: DISPLAYFORM0 Question: What is the harmonic mean of irony reward and sentiment reward harmonic mean of irony reward and sentiment reward is defined as: DISPLAYFORM0 Question: What is the harmonic mean of irony reward and sentiment reward harmonic mean of irony reward", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": " Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.\n\nQuestion: What is the average content score across the paintings?\n\nAnswer: The average content score is 3.7 which demonstrates that the prose generated is relevant to the painting.\n\nQuestion: What is the average creativity score?\n\nAnswer:", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": " They compare their models to existing ones, differently from BIBREF7, BIBREF7, BIBREF7, BIBREF7, BIBREF7, BIBREF7, BIBREF7, BIBREF7, BIBREF7, BIBREF7, BIBREF7, BIBREF7, BIBREF7, BIBREF7, BIBREF7, BIBREF7, BIBREF7, BIBREF7, BIBREF7, BIBREF7, BIBREF, BIBREF, BIBREF, BIBREF, BIB", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": " The sample collected consisted on 1 1.5M tweets published by 848 196 different users. Within our sample, we identified 1327 tweets that went viral (i.e. they were retweeted more than 1000 times) produced by 643 users. Such users were exposed to fake news related to the election for a shorter period of 1.5M tweets. In other words, tweets containing fake news mostly contained fake news mostly contained fake news. The distribution of those viral tweets labelled as containing fake news.", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": " The dataset of hashtags is sourced from the 1.", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": " The last status of the database, as well as other related and other related and useful information about its parts can be found on its website, together with some useful statistics related to the database.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": " Word subspace representation is a low-dimensional linear subspace in a word vector space with high dimensionality.", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": " The baseline B1.\n\nQuestion: What is the significance of the news article titles?\n\nAnswer the first paragraph?\n\nAnswer: The first five paragraphs of the news article.\n\nQuestion: What is the second feature set we consider?\n\nAnswer: The number of entities and their corresponding entity classes.\n\nQuestion: What is the second feature set we consider?\n\nAnswer: The number of paragraphs in the news article.\n\nQuestion: What is the third feature set we consider?\n\nAnswer: The number of news articles in Wikipedia.\n\nQuestion: What is the third feature set we consider?\n\nAnswer: The number of news articles in Wikipedia.\n\nQuestion: What is the third feature set", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": " unanswerable\n\nQuestion: Is SemCor3.0 reflective of English language data in general?\n\nAnswer: unanswerable\n\nQuestion: Is SemCor3.0 reflective of English language data in general?\n\nAnswer: unanswerable\n\nQuestion: Is SemCor3.0 reflective of English language data in general?\n\nAnswer: unanswerable\n\nQuestion: Is SemCor3.0 reflective of English language data in general?\n\nAnswer: unanswerable\n\nQuestion: Is SemCor3.0 reflective of English language data in general?\n\nAnswer: unanswerable\n\nQuestion: Is SemCor3.0 reflective of English language data in", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": " 100 hours\n\nExplanation:", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": " They used the high-quality\n\nQuestion: What is the name of the winning system of the 2016 edition of the challenge?\n\nAnswer: Balikas et al.\n\nQuestion: What is the winning system of the 2016 edition of the challenge?\n\nAnswer: Balikas et al.\n\nQuestion: What is the winning system of the 2016 edition of the challenge?\n\nAnswer: Balikas et al.\n\nQuestion: What is the winning system of the 2016 edition of the challenge?\n\nAnswer: Balikas et al.\n\nQuestion: What is the winning system of the 2016 edition of the challenge?\n\nAnswer:", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": " unanswerable\n\nQuestion: Are the automatically constructed datasets subject to quality control?\n\nAnswer:?\n\nAnswer: unanswerable\n\nQuestion: Are the automatically constructed datasets subject to quality control subject to quality control?\n\nAnswer: unanswerable\n\nQuestion: Are the automatically constructed datasets subject to quality control?\n\nAnswer: unanswerable\n\nQuestion: Are the automatically constructed datasets subject to quality control?\n\nAnswer: unanswerable\n\nQuestion: Are the automatically constructed datasets subject to quality control?\n\nAnswer: unanswerable\n\nQuestion: Are the automatically constructed datasets subject to quality control?\n\nAnswer: unanswerable\n\nQuestion: Are the automatically constructed datasets subject to", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": " unanswerable\n\nQuestion: Are the images from a specific domains that are easy to understand?\n\nAnswer: unanswerable\n\nQuestion: Are the images from domains that are easy to understand?\n\nAnswer: unanswerable\n\nQuestion: Are the images from domains that are easy to understand?\n\nAnswer: unanswerable\n\nQuestion: Are the images from domains that are easy to understand?\n\nAnswer: unanswerable\n\nQuestion: Are the images from domains that are easy to understand?\n\nAnswer: unanswerable\n\nQuestion: Are the images from domains that are easy to understand?\n\nAnswer: unanswerable\n\nQuestion: Are the images from domains that", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": " Our B-M model relies on subsets of Facebook pages for training, which were chosen according to results of a basic model run on development data as well as on the distribution of emotions in the development dataset (Figure FIGREF9), we selected a best model (B-M), namely the combined set of Time, The Guardian and Disney, which yields the highest results on development data. Time and The Guardian perform well on most emotions but Disney helps to boost the performance for the Joy class.\n\nQuestion: What are the main observations of the results of the results of the results of the results?\n\nAnswer:: Our B-M model's performance is the best", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": " No\n\nExplanation:", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": " They define robustness of a model as the ability of the model to predict the output a) to the prior knowledge and b) to the true class distribution. They define robustness of a model as the ability of the model to predict out a) to the prior knowledge and b) to the true class distribution. They define robustness of a model as the ability of the model to predict out a to the prior knowledge and b) to the true class distribution. They define robustness of a model as the ability of the model to predict out the prior knowledge and b) to the true class distribution. They define robustness of a model", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": " The results for the other sentence embeddings methods are depicted in Table TABREF6. The results for the other sentence embeddings methods are depicted in Table TABREF6. The results for the other sentence embeddings methods are depicted in Table TABREF6. The results for the other sentence embeddings methods are depicted in Table TABREF6. The results for the other sentence embeddings methods are depicted in Table TABREF6. The results for the other sentence embeddings methods are depicted in TABREF6. The results for the other sentence embeddings methods are depicted in TABREF6. The results for the other sentence embeddings methods are depicted in TABREF6. The results for", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": " For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively. We observe huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively. As far as we are concerned, we are setting new SOTA performances on all of the four NER datasets.\n\nQuestion: What are the experiment results for the two losses?\n\nAnswer: As can be seen from Eq.DISPLAY_FORM8, each", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": " They test their conflict method on two tasks: Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask.", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": " They compared against the other models that use the same tree structure as their own model, such as the TG-RNN, TE-RNN, TE-RNTN, and the Tag-SPINN, and the other models that do not use the same task, such as the latent tree models, the ELMo, and the Residual Residual stacked encoders, the BiLSTM with the generalized pooling, and the Reinforced Self-Attention Network.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: The main contribution of the paper is that they proposed a novel RvNN architecture, which is called SATA Tree", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": " The key focus of our work is to improve KB relation detection.", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": " The methods that are considered to find examples of biases and unwarranted in inferences are: (1) Browser-based annotation tool, (2) Ethnicity/race, (3) Other methods, (4) Conclusion, (5) Conclusion, (6) Discussion, (7) Acknowledgments, (8) Answer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": " They explore French, Italian, Spanish, Spanish, and English.\n\nQuestion: What is the challenge?\n\nAnswer: The Winograd Schema Challenge (WSC) is a challenge for AI programs. The program is presented with a collection of sentences, each of which is one element of a Winograd schema, and is required to find the correct referent for the ambiguous pronoun. An AI program passes the challenge if its success rate is comparable to a human reader.\n\nQuestion: What is the challenge for machine translation programs? Answer: Winograd schemas as the basis for challenges for machine translation programs. In many cases, the identification of the referent of", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": " They experimented with the following models, however in the proposed architecture does not restrict the selection of models by large margin, and the proposed architecture does not restrict the selection of models by large margin, and the proposed architecture does not restrict the selection of models by large margin, and the proposed architecture does not restrict the selection of models by large margin, and the proposed architecture does not restrict the selection of models by large margin, and the proposed architecture does not restrict the selection of models by large margin, and the proposed architecture does not restrict the selection of models by large margin, and the proposed architecture does not restrict the selection of models by large margin,", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": " Yes, the authors report results only on English data on English data on English data.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": " The authors experimented with a few summarization algorithm that is provided by the Sumy package. A common parameter which is required by all these algorithms is number of sentences keep in the final summary. ILP-based summarization requires a similar parameter K, which is automatically decided based on number of total candidate phrases. Assuming a sentence is equivalent to roughly 3 phrases, for Sumy algorithms, we set number of sentences parameter to the ceiling of K/3.", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": " The previous state of the art for this task was the Any Post Attention (APA) model. This model enabled instructors to tune the model to predict intervention early or late.", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": " The Master node. That is, the master node vector is initialized with the least impactful.", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": " The two corpora used in the shared task were DTA18 and DTA19. They consist of subparts of DTA corpus BIBREF11 which is a freely available lemmatized, POS-tagged and spelling-normalized diachronic corpus of German containing texts from the 16th to the 20th century. DTA18 contains 18 contains 40 million sentences published between 1750-1799 and DTA19 26 million between 1850-1899. The corpus version used in the task has the following format: \"year [tab] lemma1 lemma1 lemma1 lemma2 lemma", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": " The 7 Indian languages are Kannada, Hindi, Telugu, Telugu, Malayalam, Bengali, and English.\n\nQuestion: What is the total amount of audio data for training and testing for each of the language?\n\nAnswer: The amount of audio data for training and testing for each of the language is shown in the table bellow.\n\nQuestion: What is the size of the best model, as shown in Table 4?\n\nAnswer: The size of the best model is 5sec spectrogram.\n\nQuestion: What is the plot of the t-sne plot of the embeddings space?\n\nAnswer: The plot of the t-s", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": " The model performance on target language reading comprehension is not good.\n\nQuestion: What is the model performance on target language reading comprehension?\n\nAnswer: The model performance on target language reading comprehension is good.\n\nQuestion: What is the model performance on target language reading comprehension?\n\nAnswer: The model performance on target language reading comprehension is good.\n\nQuestion: What is the model performance on target language reading comprehension?\n\nAnswer: The model performance on target language reading comprehension is good.\n\nQuestion: What is the model performance on target language reading comprehension?\n\nAnswer: The model performance on target language reading comprehension is good.\n\nQuestion: What is the model performance on target language reading comprehension", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": " As observed from Table TABREF44, ALOHA, ALOHA, combined with the HLAs and dialogue dataset, achieves a significant improvement on the target character language style retrieval task compared to the baseline open-domain chatbot models. As observed from Table TABREF44, ALOHA demonstrates an accuracy boost for all five evaluation characters that is consistent across all five folds (see Tables TABREF44 and TABREF45), indicating that the model is able to recover the language style of the target character without its dialogue.", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": " 1. ARAML is a novel adversarial training framework to deal with the instability issue of training GANs for text generation. 2. ARAML is a RL approach to incorporate MLE objective into RL training framework, which regards BLEU as rewards. 3. ARAML is nearly the best reverse perplexity (PPL) in the stability of adversarial training. 4. ARAML is a RL approach to incorporate MLE objective into RL training framework, which regards BLEU as rewards. 5. ARAML is a RL approach to incorporate MLE objective into RL training framework, which regards BLEU as rewards", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": " The authors present some evidence that the model can capture some biases in data annotation and collection by examining the results of the model. They show that the model can detect some new information from the process of data sampling. They also show that the model can detect some information from the hate speech detection task. They also show that the model can detect some information from the hate speech detection task. They also show that the model can detect some information from the hate speech detection task. They also show that the model can detect some biases in the data. They also show that the model can detect some biases in the data. They also show that the model can", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": " No other baselines were tested to compare with the neural baseline.\n\nQuestion: What is the first word of the question asked by the user?\n\nAnswer: What, why and how information is collected from the privacy policy document.\n\nQuestion: What, why and how information is collected from the privacy policy document.\n\nQuestion: What, why and how information is collected from the privacy policy document.\n\nQuestion: What, why and how information is collected from the privacy policy document.\n\nQuestion: What, why and how information is collected from the privacy policy document.\n\nQuestion: What, why and how information is collected from the privacy policy document.\n\nQuestion: What,", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": " The dataset is not lemmatized originally, we lemmatized only the post-positions like Ek, kO, l, mA, m, y, y, y, aEG which are just the few examples among 299 post-299 post-positions in Nepali language. We obtained these post-positions from sanjaalcorps and added few more to match our dataset. We will be releasing this list in our github repository. We found out that lemmatizing the post-positions boosted the F1 score by almost 10%.", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": " We find that replacing the training objective with DSC introduces performance boost for both BERT and XLNet. Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP.\n\nQuestion: What are method improvements of F1 for paraphrase identification?\n\nAnswer:: We find that replacing the training objective with DSC introduces performance boost for both BERT and XLNet. Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP.\n\nQuestion: What are method improvements of F1 for paraphrase identification?\n\nAnswer::", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": " The datasets used are the eye-tracking, self-paced reading time and ERP data.\n\nQuestion: What is the N400 is qualitatively similar?\n\nAnswer: The N400 is qualit is qualitatively similar.\n\nQuestion: What is the N400 is so-named because it is well established that it is associated with semantic complexity?\n\nAnswer: The N400 is so-named because it is well established that it is associated with semantic complexity.\n\nQuestion: What is the N400 is so-named because it is well established that it is associated with semantic complexity?\n\nAnswer: The N400 is so-named because it is well established that", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": " The subjects were presented with a scientific article and a question. The question was presented to the subjects to elicit event-related responses.\n\nQuestion: What was the question?\n\nAnswer: The question was: Answer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes, no\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nQuestion: What was the question?\n\nAnswer: The question was: Answer the question as concis", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": " We first compare all four models, Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-RL-SEN, and Pointer-Gen+ARL-SEN, to establish that our model produces relevant headlines and we leave the sensationalism of generated outputs for our sensationalism scorer as the reward function. Then we use the best model generated by a Chi-square test on the domains of the headline and the article. We find that the best performance to establish that our model, and it will hurt the ROUGE-L BIBREF9 as the reward. We also use the full LCST-RL-RO", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": " The learning models used on the dataset are Naïve Bayes, Logistic Regression, Logistic Regression, Support Vector Machine, Random Forests, Gradient Boosted Trees, Convolutional Neural Networks, Recurrent Neural Networks, and their variants of their variants.\n\nQuestion: What is the best learning model?\n\nAnswer: The best learning model is the best learning model.\n\nQuestion: What is the best learning model?\n\nAnswer: The best learning model is the best learning model.\n\nQuestion: What is the best learning model?\n\nAnswer: The best learning model is the best learning model.\n\nQuestion: What is the best learning model?\n\nAnswer: The best learning", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": " The two types of architectures are: a bi-directional language model to augment the sequence to sequence encoder and a uni-directional model to augment the decoder. Both use self-attention and the uni-directional model contains INLINEFORM0 transformer blocks, followed by a word classifier to predict the next word on the right. The bi-directional model solves a cloze-style token prediction task at training time. The model consists of two towers, the forward tower operates left-to-right and the tower operating right-to-left as backward tower; each tower contains INLINEFORM1 transformer blocks. The forward and backward representations are combined via a self-attention module and", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": " We propose a dynamic weight adjusting strategy, which associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds. This strategy helps to deemphasize confident examples with $p$ approaching 0 or 1, and thus makes the model attends more focus to hard-negative examples.\n\nQuestion: What is the harmonic mean of 1.25?\n\nAnswer:: The harmonic mean of precision and recall and recall is 1.25.\n\nQuestion: What is the derivative of $\\frac {p_{i1}$\n\nAnswer: The derivative of $\\frac {p_{i1}$ with respect", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": " The results from these proposed strategies are that the agents are far outperform the A2C and KG-A2C. The agents using the agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. The knowledge graph representation without enhanced exploration appears to thus be a better indication of what a promising state is as opposed to just the textual observation.", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": " The individual models consist of a model consists of a semantic role and a crosslingual latent variable.", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": " We use the techniques in BIBREF7 for found data to re-align the text to the text and find out which turns are best (or worst) aligned so that we can select segments that give the most accurate alignments. Some of the misalignments may in part be due to varied orthography, and we intend, but have not yet, to investigate normalization of orthography (i.e. spelling correction) to mitigate this.\n\nQuestion: What is the difference between the two directions of the two settings?\n\nAnswer: It it worth noting the difference in quality between the two directions, with translation into Spanish reaching 20.4 (almost", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": " A semicharacter architecture is a type of architecture that uses both semicharacters to recognize words.", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": " Answer: unanswerable\n\nQuestion: which system is used to generate the baseline system: Answer: unanswerable\n\nQuestion: which system is used to assess the contribution of external lexicons: Answer: unanswerable\n\nQuestion: which languages are explored: Answer: unanswerable\n\nQuestion: which system is used to understand: Answer: unanswerable\n\nQuestion: which languages are explored: Answer: unanswerable\n\nQuestion: which languages are explored: Answer: Answer: Answer: unanswerable\n\nQuestion: which languages are explored: Answer: Answer: unanswerable\n\nQuestion: which languages are explored: Answer: Answer", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": " The NCEL approach is effective in terms of efficiency. The main reason is as shown in Figure FIGREF10 ( INLINEFORM1 ). In the first document, we heuristically found a suitable window to 6, and 3% on the prior probability of 0.42 to the correct candidate INLINEFORM2. The overall feature vector INLINEFORM3 is as follows: INLINEFORM4. For each candidate INLINEFORM5, we represent INLINEFORM6 in the number of candidates of neighbor mentions INLINEFORM7 : INLINEFORM8. For each document INLINEFORM9, we define neighbor mentions INLINEFORM10, and the mention INLINEFORM", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": " Yes\n\nQuestion: Is the data de-identified?\n\nAnswer: Yes\n\nQuestion: Is the data de-identified?\n\nAnswer: Yes\n\nQuestion: Is the data de-identified?\n\nAnswer: Yes\n\nQuestion: Is the data de-identified?\n\nAnswer: Yes\n\nQuestion: Is the data de-identified?\n\nAnswer: Yes\n\nQuestion: Is the data de-identified?\n\nAnswer: Yes\n\nQuestion: Is the data de-identified?\n\nAnswer: Yes\n\nQuestion: Is the data de-identified?\n\nAnswer: Yes\n\nQuestion: Is the data de-identified?\n\nAnswer: Yes\n\nQuestion: Is the data de-identified?\n\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": " The baseline is the error detection system by Rei2016, trained using the same FCE dataset.\n\nQuestion: What is the best overall performance?\n\nAnswer: The best performance on all datasets.\n\nQuestion: What is the best overall performance on all datasets?\n\nAnswer: The best performance on all datasets.\n\nQuestion: What is the best performance on all datasets?\n\nAnswer: The best performance on all datasets.\n\nQuestion: What is the best performance on all datasets?\n\nAnswer: The performance on all datasets.\n\nQuestion: What is the performance on all datasets?\n\nAnswer: The performance on all datasets.\n\nQuestion: What is the performance on all datasets?\n\nAnswer:", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": " The clinical notes were obtained from the 2010 i2b2/VA BIBREF18 and 2013 ShARe/CLEF BIBREF1 NLP challenges.", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": " The main reason to introduce the refine process is to enhance the decoder using BERT's contextualized representations, so we do not modify the encoder during this process. On the decoder side, we propose a new word-refine decoder. The refine process works as follows: first the draft decoder writes a summary draft based on a document, and then the refine decoder edits the draft. It concentrates on one word at a time, based on the source document as well as other words of the other words of the summary.", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": " unanswerable\n\nQuestion: What is the main goal of the paper?\n\nAnswer: To investigate the paper is the first of its kind survey of the recent tweet-specific unsupervi\n\nQuestion: What is the main goal of the paper?\n\nAnswer: To combat this computational inefficiency, FastSent BIBREF17 propose a simple additive (log-linear) sentence model, which predicts adjacent sentences (represented as BOW) in the book corpus BIBREF16. Although the testing is cheap as it involves a cheap forward propagation of the test sentence, STV is the authors in their subsequent work BIBREF18's model is", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": " The dataset is annotated with 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression (e.g., “Citizens fear an economic depression\") or evidence of depression (e.g., “depressed over disappointment\"). If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood (e.g., “feeling down in the dumps\"), disturbed sleep (e.g., “another restless night\"), or fatigue or loss of energy (e.g., “the fatigue is unbearable\") BIBREF10.\n\nQuestion: What is the baseline", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": " The training set provided by BIBREF0 is not very large, so it was interesting to find a way to augment the training set. A possible method is to simply translate the datasets into other languages, leaving the labels intact. Since the present study focuses on Spanish tweets in Spanish tweets, all tweets from the English datasets were translated into Spanish. This new set of “Spanish” data was then added to our original training set.", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": " They used the multinomial NB algorithm.\n\nQuestion: What is the top-ranked content words for the different industries using the AFR method?\n\nAnswer:?\n\nAnswer: The top-ranked words for the different industries using the AFR method are: The top-ranked words for the different industries using the AFR method are: The top-ranked words for the different industries using the AFR method are: The top-ranked words for the different industries using the AFR method The top-ranked words for the different industries using the AFR method The top-ranked words for the different industries using the AFR method The top-ranked words for the different industries using the AFR", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": " The baseline for the baseline for the FLC task generates spans and selects one of the 18 techniques randomly. The inefficacy of such a simple random baseline is illustrated in Tables TABREF36 and TABREF41.\n\nQuestion: What was the baseline for the baseline for the FLC task?\n\nAnswer: The baseline for the baseline for the FLC task generates spans and selects one of the 18 techniques randomly. The inefficacy of such a simple random baseline is illustrated in Tables TABREF36 and TABREF41.\n\nQuestion: What was the baseline for the FREF23?\n\nAnswer: The baseline for the FREF23 is a", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": " The authors implemented a baseline model based on conditional random fields (CRF, where features like POS tags, n-grams, label transitions, word suffixes and relative position to the end of the text are considered. They also implemented a baseline model based on the INLINEFORM0 tagging scheme, which is a simple approach is able to yield new state-of-the-art INLINEFORM1 scores on the detection and location tasks.", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": " We prefer the term disinformation to the more specific fake news to refer to a variety of misleading and harmful information. Therefore, we follow a source-based approach, a consolidated strategy also adopted in BIBREF6BIBREF6BIBREF6BIBREF16BIBREF2BIBREF2BIBREF1, in order to obtain relevant data for our analysis. We collected tweets associated to a dozen US mainstream news websites, i.e. most trusted sources described in BIBREF18, with the Streaming API, and we referred to the dataset provided by BIBREF23 to obtain a dozen US disinformation outlets.", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": " The ancient Chinese dataset comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": " The OLID dataset contains 14,100 tweets and is split into  English.", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": " The datasets used for the experiments are the Penn Treebank of the Penn Chinese Treebank (PTB) and the Chinese Gigawiki (CTB) BIBREF1. The PTB is a corpus of 1.7M words of Chinese text, and the CTB is a corpus is a 1.2M word corpus of Chinese text. The CTB is a corpus of Chinese text that is used for the experiments. The datasets. The CTB is a corpus of Chinese text that is used for the CTB is a corpus of Chinese text that is used for the experiments. The CTB is a corpus of", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": " The UTCCNN has 3 layers: the user layer, the topic layer, and the document layer.", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": " The structured information is derived from the CORINE Land Cover (CLC, SoilGrids, and SoilGrids) datasets. The numerical features are obtained from the Natura 2000 dataset. The categorical features are obtained from the Natura 2000 dataset. The numerical features are derived from the CORINE Land Cover (CLC, SoilGrids, and SoilGrids) datasets. The numerical features are derived from the CORINE Land Cover (CLC, SoilGrids, and SoilGrids) datasets. The numerical features are derived from the CORINE Land Cover (CLC, SoilGrids, and SoilGrid", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": " The clinical datasets used in the paper are NUBes-PHI and MEDDOCAN. NUBes-PHI is a corpus of real medical reports of real medical reports manually annotated with sensitive information. The MEDDOCAN corpus is a synthetic corpus of clinical cases enriched with sensitive information by health documentalists.\n\nQuestion: What are the systems used in the paper?\n\nAnswer: The systems used in the paper are: rule-based baseline, CRF, spaCy, and BERT. The rule-based baseline serves to give a quick insight about how challenging the data is. The BERT-based model is a Fully Connected (FC layer on top", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": " Unigrams and Pragmatic features\n\nQuestion: BIBREF0, BIBREF1, BIBREF2, BIBREF3\n\nQuestion: What are the two primary feature types?\n\nAnswer: The cognitive features, derived from human eye movement patterns observed during reading, include two primary feature types:", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": " The metrics are used to establish that this makes the chatbots more knowledgeable and better at learning and better at learning and conversation. The metrics are used to establish that the predictive performance of the prediction model. The results are used to establish the effectiveness of the performance of the performance of the prediction model for inference, which is highly correlated with the performance of the prediction model for inference. The results are shown in the results in the form of the said in the figure 1, which is a knowledge of the word in the Freebase and WordNet. The results are not in the Freebase and WordNet. The results are shown in the table", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": " No, they do not employ their indexing-based method to create a sample of a QA Wikipedia dataset.", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": " Galatasar You are given a scientific article and a question. Answer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable. If the question is a yes/no question, answer yes, no, no, or unanswerable. Do not provide any explanation. Article: Introduction Stance detection (also called stance identification or stance classification) is one of the considerably recent research topics in natural language processing (NLP). It is usually defined as a classification problem where for a text and a target pair,", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": " We conduct experiments on the transformation from non-irony to irony and from ironic to non-irony.", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": " Gaussian-masked directional multi-head attention is a variant of multi-head attention that we use to capture the representation of input sequence. The method of multi-head attention is similar as scaled dot-product attention. The difference is that we use Gaussian-masked directional attention to let the attention focus on the adjacent characters of adjacent characters of one sequence. The method of multi-head attention is similar as scaled dot-product attention. The difference is that we use multi-head attention to let the attention focus on adjacent characters of one sequence. The method of multi-head attention is similar as scaled dot-product attention. The difference is that we use multi-head attention to let the attention", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": " They considered Facebook status update\n\nQuestion: What types of models did they use?\n\nAnswer:: We build two types of models (RBF SVM, Linear SVM, and Random Forest, Random forest with N-gram, charater N-gram, and tweet POS tags, and Random forest with N-gram, charater N-gram, tweet POS tags, sentiment tags, average word lengths and word counts from each message as they have a well-optimized model which can be used for the language analysis in social media. We filtered out low frequency Word Pairs features as they tend to be noisy and 1) these models were trained", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": " The baseline features are the features extracted from the baseline CNN architecture. These features are extracted from the features that are extracted from the fully-connected layer of the baseline CNN. The baseline features. The baseline features are the features extracted by the pre-trained models. In the literature, we use the features extracted by the pre-trained models as the static as the standard pre-trained features are used in our experimental results show that those features, when they are not used with the other two pre-trained features are the authors. In the above, we have carried out the pre-trained features are the features are the features are the most useful features, besides baseline features are", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": " The number of clusters INLINEFORM0, the number of clusters INLINEFORM1, the number of clusters INLINEFORM2, the number of clusters INLINEFORM3, the number of clusters INLINEFORM4, the number of clusters INLINEFORM5, the number of clusters INLINEFORM6, the number of clusters INLINEFORM7, the number of clusters INLINEFORM8, the number of clusters INLINEFORM9, the number of clusters INLINEFORM10, the number of clusters INLINEFORM11, the number of clusters INLINEFORM12, the number of clusters INLINEFORM13, the number of clusters INLINEFORM14, the number of clusters INLINEFORM15,", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": " The scores of their system are shown in Table TABREF18.", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": " The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total. However, as mentioned above, only case presentation sections, headings and abstracts are annotated. The numbers of annotated entities are summarized in Table TABREF24. Findings are the most frequently annotated type of entity. This makes sense given that findings paint a clinical picture of the patient's condition. The numbers of tokens per entity range from one token for all types to 5 tokens for cases (average", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": " Yes\n\nQuestion: Is it possible to convert a cloze-style questions to a naturally-looking questions?\n\nAnswer:: Yes\n\nQuestion: Is it possible to convert a cloze-style questions to a naturally-looking questions to a naturally-looking questions?\n\nAnswer: Yes\n\nQuestion: Is it possible to convert a cloze-style questions to a naturally-looking questions?\n\nAnswer: Yes\n\nQuestion: Is it possible to convert a cloze-style questions to a naturally-looking questions?\n\nAnswer: Yes\n\nQuestion: Is it possible to convert a cloze-style questions to a naturally-looking questions?\n\nAnswer: Yes\n\nQuestion: Is it possible to convert a cloze", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": " They consider text categorization, sentiment classification, and text classification.\n\nQuestion: What is the main contributions do they propose?\n\nAnswer: They propose three regularization terms to address the problem: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution regularization term.\n\nQuestion: What is the baseline method they set the maximum entropy term can be derived by setting the constraint function to G(x, y) to be?\n\nAnswer: The baseline is the empirical distribution of the neutral features.\n\nQuestion: What is the baseline is the maximum entropy", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": " The authors compare the performance of their model to 5 common models in previous work primarily intended for learned models in the science exam question classification.", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": " The training sets of these versions of ELMo are larger than the previous ones.", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "   unanswerable\n\nQuestion: What is the name of the dataset?\n\nAnswer:  ?\n\nAnswer:  OurNepali\n\nQuestion: What is the name of the dataset?\n\nAnswer:  OurNepali\n\nQuestion: What is the name of the dataset?\n\nAnswer:  OurNepali\n\nQuestion: What is the name of the dataset?\n\nAnswer:  OurNepali\n\nQuestion: What is the name of the dataset?\n\nAnswer:  OurNepali\n\nQuestion: What is the name of the name of the?\n\nAnswer:  OurNepali\n\n", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": " They compare their proposed s2sL method to MLP, EMO-2, Eusboost, and MWMOTE techniques.", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": " Yes, their NER model learns NER from both text and images.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": " Yes\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: The main contribution of this paper is to introduce a new approach to learn a new latent syntactic representation (e.g. a dependency parse) from a latent syntactic structure (e.g. a latent POS tag) by using a latent word (or tag) embeddings in their parameterization. The main syntactic categories are not only to learn a latent embedding space that is more suitable for the syntax model, but also to flexibly model the underlying syntax model. In this case, we can be viewed in terms of the underlying syntax model. The basic HMM,", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": " 0.32\n\nQuestion: What was their highest MRR score?\n\nAnswer: 0.32\n\nQuestion: What was their highest MRR score?\n\nAnswer: 0.32\n\nQuestion: What was their highest MRR score?\n\nAnswer: 0.32\n\nQuestion: What was their highest MRR score?\n\n0.32\n\nQuestion: What was their highest MRR score?\n\n0.32\n\nQuestion: What was their highest MRR score?\n\n0.32\n\nQuestion: What was their highest MRR score?\n\n0.32\n\nQuestion: What was their highest MRR score?\n\n0.32\n\nQuestion:", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": " For the task of unsupervised dependency parse induction, we employ the Dependency Model with Valence (DMV) BIBREF2 as the syntax model as the syntax model in our approach, which is a generative model that is trained using a broad context window size. The authors also use the Penn Treebank BIBREF13 dataset for the task of unsupervised dependency parse induction. The Penn Treebank is a dataset that is used to train the Penn Treebank BIBREF13. The Penn Treebank is a dataset that is used to train the Penn Treebank is a dataset that is used to train the Penn Treebank", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": " unanswerable\n\nQuestion: What is the main contribution of the claim of the authors?\n\nAnswer: unanswerable\n\nQuestion: What is the authors claim that about 87.5% NLP related jobs belong to a commercial centralized GPU cluster?\n\nAnswer: unanswerable\n\nQuestion: What is the authors claim that about 87.5% NLP related jobs belong to a commercial centralized GPU cluster?\n\nAnswer: unanswerable\n\nQuestion: What is the authors claim that about 87.5% NLP related jobs belong to a commercial centralized GPU cluster?\n\nAnswer: unanswerable\n\nQuestion: What is the authors evidence the", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": " On the benchmarks of the SimpleQuestions BIBREF2 and WebQSP BIBREF25 data sets, they achieve the state of the art.", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
