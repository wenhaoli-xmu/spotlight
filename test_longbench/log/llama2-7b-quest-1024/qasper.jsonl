{"pred": "The ground truth for fake news is established by a single expert.\n\nQuestion: What is the main finding of the article?\n\nAnswer: The main finding of the article is that there are differences in the meta-data of viral tweets containing fake news and viral tweets not containing fake news.\n\nQuestion: What is the main contribution of the article?\n\nAnswer: The main contribution of the article is that it provides a preliminary characterization of fake news in Twitter.\n\nQuestion: What is the main limitation of the article?\n\nAnswer: The main limitation of the article is that", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "GhostVLAD is an extension of the NetVLAD approach, which we discussed in the previous section. The GhostVLAD model was proposed for face recognition by Y. Zhong [10]. GhostVLAD works exactly similar to NetVLAD except it adds Ghost clusters along with the NetVLAD clusters. So, now we will have a K+G number of clusters instead of K clusters. The Ghost clusters are added to map any noisy or irrelevant content into ghost clusters and are not included during the feature aggregation stage, as shown in Figure 1 (Right Side), where the g", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "By how much does their model outperform the state of the art results?\n\nQuestion: What is the performance gain of the proposed model?\n\nAnswer: What is the performance gain of the proposed model?\n\nQuestion: What is the performance gain of the proposed model?\n\nAnswer: What is the performance gain of the proposed model?\n\nQuestion: What is the performance gain of the proposed model?\n\nAnswer: What is the performance of the proposed model?\n\nQuestion: What is the performance of the proposed model?\n\nAnswer: What is the performance of the proposed model?\n\nQuestion:", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "\n\nThe use of context tweets generally have little effect on baseline models, however they noticeably improve the scores of several metrics. For instance, CNN with context tweets score the highest recall and F1 for “hateful\" labels, and RNN models with context tweets have the highest recall for “abusive\" tweets.\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "They looked at the following Facebook pages:\n\n\\begin{itemize}\n\\item FoxNews\n\\item CNN\n\\item ESPN\n\\item New York Times\n\\item Time magazine\n\\item Huffington Post\n\\item Weird News\n\\item The Guardian\n\\item Cartoon Light\n\\item Cooking Light\n\\item Home Cooking Adventure\n\\item Justin Bieber\n\\item Nickelodeon\n\\item Spongebob\n\\item Disney\n\\end{itemize}\n\nQuestion: What did they do with the Facebook pages?\n\nAnswer: They used", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "No, the SemEval dataset contains tweets in English, French, and Spanish, and the hashtag dataset contains tweets in English, French, and Spanish.\n\nQuestion: Do the hashtag and SemEval datasets contain only tweets?\n\nAnswer: No, the SemEval dataset contains tweets, blog posts, and news articles, and the hashtag dataset contains tweets, blog posts, and news articles.\n\nQuestion: Do the hashtag and SemEval datasets contain only tweets from 2010?\n\nAnswer: No, the SemEval", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "unanswerable\n\nQuestion: What is the size of the corpus?\n\nAnswer: 97,880 token\n\nQuestion: What is the average length of the documents?\n\nAnswer: 2413 token\n\nQuestion: What is the average length of the documents?\n\nAnswer: 2413 token\n\nQuestion: What is the average length of the documents?\n\nAnswer: 2413 token\n\nQuestion: What is the average length of the documents?\n\nAnswer: 241 token\n\nQuestion: What is the average length", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "The CNN/DailyMail dataset is a collection of news articles and associated highlights, i.e., a few bullet points giving a brief overview of the article. We used the standard splits of BIBREF24 for training, validation, and testing (90,266/1,220/1,093 CNN documents and 196,961/1,093 NYT documents). We did not anonymize entities. We first split sentences with the Stanford CoreNLP toolkit BIBREF26 and pre-processed the dataset following", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "The proposed approach is based on the asymmetric KL divergence energy function which capturing lexical inference relations such as causality (for example, mosquito $\\rightarrow $ malaria), hypernymy (for example, dog $\\models $ animal) etc. The proposed approach is compared with the previous approaches w2g BIBREF9 ( single Gaussian model) and w2gm BIBREF10 (mixture of Gaussian model with expected likelihood kernel). The proposed approach GM$\\_$KL (Gaussian Mixture using KL Divergence) outperforms the previous approaches w2g", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "The ensemble method works by taking the average of the predictions from multiple models.\n\nQuestion: How does their ensemble method work?\n\nAnswer: The ensemble method works by taking the average of the predictions from multiple models.\n\nQuestion: How does their ensemble method work?\n\nAnswer: The ensemble method works by taking the average of the predictions from multiple models.\n\nQuestion: How does their ensemble method work?\n\nAnswer: The ensemble method works by taking the average of the predictions from multiple models.\n\nQuestion: How does their ensemble method work?\n\nAnswer: The ensemble method works by taking the average", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "The EmotionX challenge consists of $1,000$ dialogues for both Friends and EmotionPush. In all of our experiments, each dataset is separated into top 800 dialogues for training and last 200 dialogues for validation. Since the EmotionX challenge considers only the four emotions (anger, joy, neutral, and sadness) in the evaluation stage, we ignore all the data point corresponding to other emotions directly. The details of emotions distribution are shown in Table TABREF18.\n\nQuestion: What are the hyperparameters and training setup of your", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\n", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "IMDb dataset\n\nQuestion: What is the optimal combination of word2vec hyper-parameters for intrinsic and extrinsic NLP purposes?\n\nAnswer:: w8s0h0\n\nQuestion: What is the optimal combination of word2vec hyper-parameters for intrinsic and extrinsic NLP purposes?\n\nAnswer:: w8s0h0\n\nQuestion: What is the optimal combination of word2vec hyper-parameters for intrinsic and extrinsic NLP purposes?\n\nAnswer:: w8s0h0\n\nQuestion: What is the optimal combination of word2vec hyper-", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "The proposed system achieves +1.08 F1 improvements on DL-PS, +1.24 on EC-MT, and +2.38 on EC-UQ, respectively.\n\nQuestion: What is the impact of character embeddings?\n\nAnswer: The impact of character embeddings is significant, demonstrating that the pretrained embeddings successfully provide useful information.\n\nQuestion: What are the case studies?\n\nAnswer: The case studies are two examples from the DL-PS dataset, which compares the outputs of the baseline and our final models,", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "Yes\n\nQuestion: What is the name of the new dataset?\n\nAnswer: ZuCo 2.0\n\nQuestion: What is the purpose of the new dataset?\n\nAnswer: To compare eye-tracking and brain activity data during normal reading and during annotation.\n\nQuestion: What is the length of the sentences in the new dataset?\n\nAnswer: 739 sentences\n\nQuestion: How many sentences are there in the new dataset?\n\nAnswer: 739 sentences\n\nQuestion: How many sentences are there in the new dataset?\n\nAnswer: 739", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "The datasets used are the ones described in the article.\n\nQuestion: What is the architecture?\n\nAnswer: The architecture is described in the article.\n\nQuestion: What is the implementation?\n\nAnswer: The implementation is described in the article.\n\nQuestion: What is the performance?\n\nAnswer: The performance is described in the article.\n\nQuestion: What is the scalability?\n\nAnswer: The scalability is described in the article.\n\nQuestion: What is the cost?\n\nAnswer: The cost is described in the article.\n\nQuestion: What is the security?\n", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "The HealthCare sector achieved the best performance.\n\nQuestion: Which stock market sector achieved the worst performance?\n\nAnswer: The Energy sector achieved the worst performance.\n\nQuestion: Which stock market sector achieved the best performance?\n\nAnswer: The HealthCare sector achieved the best performance.\n\nQuestion: Which market sector achieved the worst performance?\n\nAnswer: The Energy sector achieved the worst performance.\n\nQuestion: Which stock market sector achieved the best performance?\n\nAnswer: The HealthCare sector achieved the best performance.\n\nQuestion: Which market sector achieved the worst performance?\n\nAnswer", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "RNN-based NMT, Transformer-NMT\n\nQuestion: what is the average length of the sentences translated by Transformer?\n\nAnswer: 16.78\n\nQuestion: what is the average length of the translation results of the three models?\n\nAnswer: 17.72\n\nQuestion: what is the average length of the sentences translated by SMT?\n\nAnswer: 15.50\n\nQuestion: what is the average length of the sentences translated by RNN-based NMT?\n\nAnswer: 17.12\n\nQuestion", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "The three regularization terms are incorporating neutral features, maximum entropy, and KL divergence.\n\nQuestion: What is the difference between incorporating neutral features and maximum entropy?\n\nAnswer: The difference between incorporating neutral features and maximum entropy is that incorporating neutral features assumes that the categories are uniformly distributed, while maximum entropy assumes that the categories are equally distributed.\n\nQuestion: What is the difference between incorporating neutral features and KL divergence?\n\nAnswer: The difference between incorporating neutral features and KL divergence is that incorporating neutral features assumes that the categories are uniformly distributed, while KL", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "The baselines are SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embedding, CNN, and RCNN.\n\nQuestion: What is the best f-score of UTCNN?\n\nAnswer: The best f-score of UTCNN is 0.84.\n\nQuestion: What is the best f-score of the baselines?\n\nAnswer: The best f-score of the baselines is 0.83.\n\nQuestion: What is the best f-score of UTCNN without user information?", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "By a lot.\n\nAnswer: By a lot.\n\nAnswer: By a lot.\n\nAnswer: By a lot.\n\nAnswer: By a lot.\n\nAnswer: By a lot.\n\nAnswer: By a lot.\n\nAnswer: By a lot.\n\nAnswer: By a lot.\n\nAnswer: By a lot.\n\nAnswer: By a lot.\n\nAnswer: By a lot.\n\nAnswer: By a lot.\n\nAnswer: By a lot.\n\nAnswer: By a lot.\n\nAnswer: By a lot.\n\nAnswer:", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "The authors of this paper propose a new attention mechanism called entmax, which is a generalization of the softmax attention mechanism. The authors show that entmax can be used to improve the interpretability of the model by providing more information about the attention weights assigned to different words.\n\nQuestion: What are the advantages of using entmax over softmax?\n\nAnswer: The authors of this paper show that entmax can be used to improve the interpretability of the model by providing more information about the attention weights assigned to different words. Additionally, entmax can be used to improve the performance of the model by providing more information about", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "the baseline was a sentence-level translation model\n\nQuestion: what was the model trained on?\n\nAnswer: the model was trained on monolingual data\n\nQuestion: what was the model trained on?\n\nAnswer: the model was trained on monolingual data\n\nQuestion: what was the model trained on?\n\nAnswer: the model was trained on monolingual data\n\nQuestion: what was the model trained on?\n\nAnswer: the model was trained on monolingual data\n\nQuestion: what was the model trained on?\n\nAnswer: the model was", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "The metrics used for evaluation are accuracy and LAS.\n\nQuestion: How does the performance of the target LM correlate with the performance of the source LM?\n\nAnswer: The performance of the target LM correlates with the performance of the source LM.\n\nQuestion: How does linguistic knowledge transfer happen through each training stages?\n\nAnswer: The linguistic knowledge transfer happens through each training stages.\n\nQuestion: How does the performance of the target LM correlate with the performance of the source LM?\n\nAnswer: The performance of the target LM correlates with the performance of the", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "The attention module is pretrained on the WMT data (out-of-of-domain) and then fine-tuned on in-domain data.\n\nQuestion: What is the difference between the pre-training and the multi-task learning?\n\nAnswer: The pre-training is to train the model with $\\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.8$ in the pre-training stage, since the MT dataset is much larger than ASR dataset. For fine-tune, we use $\\alpha _{st}=0.6, \\alpha _{", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The encoder is a character-based encoder-decoder.\n\nQuestion: What is the size of the context window?\n\nAnswer: The size of the context window is 100 characters.\n\nQuestion: What is the size of the MSD decoder?\n\nAnswer: The size of the MSD decoder is 100 characters.\n\nQuestion: What is the size of the auxiliary decoder?\n\nAnswer: The size of the auxiliary decoder is 100 characters.\n\nQuestion: What is the size of the LSTM layer?\n", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "\n\n\\begin{itemize}\n\\item yes\n\\item no\n\\item unanswerable\n\\end{itemize}\n\nAnswer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer:\n\n\\begin", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "20,000\n\nQuestion: How many users do they look at?\n\nAnswer: 20,000\n\nQuestion: How many users do they look at?\n\nAnswer: 20,000\n\nQuestion: How many users do they look at?\n\nAnswer: 20,000\n\nQuestion: How many users do they look at?\n\nAnswer: 20,000\n\nQuestion: How many users do they look at?\n\nAnswer: 20,000\n\nQuestion: How many", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "\n\nThe metrics used for evaluation are:\n\n1. BLEU-1/4\n2. ROUGE-L\n3. Distinct-1/2\n4. Recipe-level coherence\n5. Step entailment\n\nAnswer:\n\nThe metrics used for evaluation are:\n\n1. BLEU-1/4\n2. ROUGE-L\n3. Distinct-1/2\n4. Recipe-level coherence\n5. Step entailment\n\nAnswer:\n\nThe metrics used for evaluation are:\n\n1", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "\n\nAnswerable:\n\n1. No answer\n\n2. Yes\n\n3. No\n\n4. Unanswerable\n\n5. Unanswerable\n\n6. Unanswerable\n\n7. Unanswerable\n\n8. Unanswerable\n\n9. Unanswerable\n\n\nUnanswerable:\n\n1. Unanswerable\n\n2. Unanswerable\n\n3. Unanswerable\n\n4. Unanswerable\n\n5. Unanswerable\n\n6. Unanswerable\n\n7. Unanswerable\n\n8. Unanswerable\n\n", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "1000 sentences\n\nQuestion: How much data is needed to train the task-specific encoder?\n\nAnswer: 100000 sentences\n\nQuestion: How much data is needed to train the task-specific encoder?\n\nAnswer: 1000 sentences\n\nQuestion: How much data is needed to train the task-specific encoder?\n\nAnswer: 1000 sentences\n\nQuestion: How much data is needed to train the task-specific encoder?\n\nAnswer: 1000 sentences\n\nQuestion: How much data is", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "\n\nAnswer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nQuestion: What is the difference between the softmax and the $\\alpha $-entmax functions?\n\nAnswer:\nAnswer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "The improvement in performance for Estonian in the NER task is significant.\n\nQuestion: What is the improvement in performance for Latvian in the NER task?\n\nAnswer: The improvement in performance for Latvian in the NER task is significant.\n\nQuestion: What is the improvement in performance for Slovenian in the NER task?\n\nAnswer: The improvement in performance for Slovenian in the NER task is significant.\n\nQuestion: What is the improvement in performance for Finnish in the NER task?\n\nAnswer: The improvement in performance for Finnish in the NER task", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "Yes.\n\nQuestion: What is the difference between the two datasets?\n\nAnswer: The Honeypot dataset is a public dataset, and the Weibo dataset is a self-collected dataset.\n\nQuestion: What is the difference between the two features?\n\nAnswer: The two features are based on the topic probability of each user. The \"GOSS\" feature is based on the global information of the user, and the \"LOSS\" feature is based on the local information of the user.\n\nQuestion: What is the difference between the three baseline classification methods?\n\nAnswer: The", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "\n\nAnswer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nAnswer:\n\nAnswer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "29,794\n\nQuestion: How many documents are in each quality class?\n\nAnswer: 5,000 FA, 2,800 GA, 2,800 B, 2,800 C, 2,800 Start, 2,800 Stub\n\nQuestion: How many documents are in each quality class?\n\nAnswer: 5,000 FA, 2,800 GA, 2,8000 B, 2,800 C, 2,8", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "The human judgements were assembled by the authors of the article.\n\nQuestion: What is the purpose of the RNNMorph model?\n\nAnswer: The purpose of the RNNMorph model is to improve the performance of the translation by reducing the vocabulary size of the target language.\n\nQuestion: What is the difference between the RNNMorph model and the RNNSearch + Word2Vec model?\n\nAnswer: The RNNMorph model has a smaller vocabulary size than the RNNSearch + Word2Vec model.\n\nQuestion: What is", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "Yes\n\nQuestion: Do they test their framework performance on commonly used language pairs, such as English-to-German?\n\nAnswer: Yes\n\n\nQuestion: Do they test their framework performance on commonly used language pairs, such as English-to-German?\n\nAnswer: Yes\n\nQuestion: Do they test their framework performance on commonly used language pairs, such as English-to-German?\n\nAnswer: Yes\n\nQuestion: Do they test their framework performance on commonly used language pairs, such as English-to-German?\n\nAnswer: Yes\n\nQuestion: Do they", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "The models are evaluated by measuring the accuracy of the reconstruction of the target sentence from the keywords.\n\nQuestion: How does the model learn to generate keywords?\n\nAnswer: The model learns to generate keywords by optimizing a constrained objective that balances the efficiency and accuracy of the communication scheme.\n\nQuestion: How does the model learn to decode the keywords?\n\nAnswer: The model learns to decode the keywords by optimizing a constrained objective that balances the efficiency and accuracy of the communication scheme.\n\nQuestion: How does the model learn to generate keywords?\n\nAnswer: The model", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "\n\nThe evaluation metrics for classification tasks are accuracy, precision, recall, F-measure, and confusion matrix.\n\nAnswer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nQuestion: What is the difference between a classifier and a summarization algorithm?\n\nAnswer:\n\nThe difference between a class", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "The source domain is the domain of the source data, and the target domain is the domain of the target data.\n\nQuestion: What is the difference between feature adaptation and semi-supervised learning?\n\nAnswer: Feature adaptation is the process of learning domain-invariant features from the source domain, while semi-supervised learning is the process of learning domain-specific features from the target domain.\n\nQuestion: What is the difference between NaiveNN and DAS?\n\nAnswer: NaiveNN is a neural network that is trained on the source domain, while DAS is a neural network that is trained", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "they compare with LSTM, RNN, and CNN.\n\nQuestion: what is the difference between the LSTM and the PRU?\n\nAnswer: the LSTM has a single layer of LSTM while the PRU has a single layer of PRU.\n\nQuestion: what is the difference between the LSTM and the PRU in terms of the number of parameters?\n\nAnswer: the LSTM has 19M parameters while the PRU has 19M parameters.\n\nQuestion: what is the difference between the LSTM and the PRU in terms", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "\n\n[itemsep= -0.4em,topsep = 0.3em, align=left, labelsep=-0.6em, leftmargin=1.2em]\n\n[itemsep= -0.4em,topsep = 0.3em, align=left, labelsep=-0.6em, leftmargin=1.2em]\n\n[itemsep= -0.4em,topsep = 0.3em, align=left, labelsep=-0.6em, leftmargin=1.2em]\n\n[itemsep", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "The Wiktionary pronunciation corpus, which is already partitioned into training, validation, and test sets.\n\nQuestion: what is the size of the training set?\n\nAnswer: 10,000 words per language.\n\nQuestion: what is the size of the validation set?\n\nAnswer: 1000 words per language.\n\nQuestion: what is the size of the test set?\n\nAnswer: 1000 words per language.\n\nQuestion: what is the size of the high resource set?\n\nAnswer: 85 languages.\n", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "English, Spanish, Finnish, French, German, Italian, Japanese, Korean, Portuguese, Russian, and Turkish.\n\nQuestion: What is the size of the dataset?\n\nAnswer: 10,000 examples for each language.\n\nQuestion: What is the size of the test set?\n\nAnswer: 10,000 examples for each language.\n\nQuestion: What is the size of the training set?\n\nAnswer: 10,000 examples for each language.\n\nQuestion: What is the size of the training set?\n\nAnswer: ", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "They test their method on hashtag prediction for social media.\n\nQuestion: What is the size of the training set?\n\nAnswer: The training set is 2 million tweets for training, 10K for validation and 50K for testing.\n\nQuestion: What is the size of the vocabulary?\n\nAnswer: The vocabulary size is 20K.\n\nQuestion: What is the size of the character look-up table?\n\nAnswer: The character look-up table is 2829 unique characters.\n\nQuestion: What is", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Yes\n\nQuestion: Do they use attention?\n\nAnswer: Yes\n\n\nQuestion: Do they use gated orthogonalization?\n\nAnswer: Yes\n\nQuestion: Do they use bifocal attention?\n\nAnswer: Yes\n\nQuestion: Do they use a bidirectional GRU?\n\nAnswer: Yes\n\nQuestion: Do they use a copy mechanism?\n\nAnswer: Yes\n\nQuestion: Do they use a feedforward network?\n\nAnswer: Yes\n\nQuestion: Do they use a CNN?\n\nAnswer: Yes\n\nQuestion: Do they use a RNN?", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "Yes, PolyResponse was evaluated against a baseline that uses a state-of-of-the-art dialogue system that is trained on a large amount of conversational and image data, and a state-of-the-art ranking model that learns to respond by training on hundreds of millions context-reply $(c,r)$ pairs.\n\nAnswer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "They use a method called Meaning Extraction Method (MEM) BIBREF10 . MEM is a topic modeling approach applied to a corpus of texts created by hundreds of survey respondents from the U.S. who were asked to freely write about their personal values.\n\nQuestion: How do they obtain geographical distributions of psycholinguistic and semantic categories?\n\nAnswer: They use the distribution of the individual words in a category, they can compile distributions for the entire category, and therefore generate maps for these word categories.\n\nQuestion: How do they obtain geographical distributions of words related to", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "The ML methods aim to identify the following argument components:\n\n\\begin{itemize}\n\\item Claim: A claim is a statement that is asserted to be true or false.\n\\item Premise: A premise is a statement that supports the claim.\n\\item Rebuttal: A rebuttal is a statement that opposes the claim.\n\\item Refutation: A refutation is a statement that opposes the rebuttal.\n\\end{itemize}\n\n\\end{document}", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, ", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "The Twitter dataset is 1.5 million comments.\n\nQuestion: How large is the OSG dataset?\n\nAnswer: The OSG dataset is 295 thousand conversations.\n\nQuestion: How many conversations in the Twitter dataset have a positive final sentiment?\n\nAnswer: 100% of conversations in the Twitter dataset have a positive final sentiment.\n\nQuestion: How many conversations in the OSG dataset have a positive final sentiment?\n\nAnswer: 100% of conversations in the OSG dataset have a positive final sentiment.\n\nQuestion: How many", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "The 12 languages covered are: English, French, German, Spanish, Italian, Portuguese, Russian, Polish, Czech, Hungarian, Welsh, and Kiswahili.\n\nQuestion: What is the number of word pairs covered?\n\nAnswer: The number of word pairs covered is 1,888.\n\nQuestion: What is the number of word pairs covered per language?\n\nAnswer: The number of word pairs covered per language is 188.\n\nQuestion: What is the number of word pairs covered per language pair?\n\nAnswer: The number of word pairs", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "Wikipedia and CMV\n\nQuestion: What is the task of forecasting derailment?\n\nAnswer: To predict whether a conversation will derail.\n\nQuestion: What is the model's architecture?\n\nAnswer: A hierarchical recurrent encoder-decoder model.\n\nQuestion: What is the model's training process?\n\nAnswer: The model is trained on unlabeled data, and then fine-tuned on labeled data.\n\nQuestion: What is the model's performance?\n\nAnswer: The model achieves state-of-the-", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "No.\n\nReferences\n\n[1] BIBREF1: Agatha, A. (2017). Agatha: A framework for the automatic analysis of open sources information for surveillance/crime control. In Proceedings of the 2017 IEEE International Conference on Multimedia and Expo (ICME), Singapore, pp. 1-6.\n\n[2] BIBREF2: Agatha, A. (2017). Agatha: A framework for the automatic analysis of open sources information for surveillance/crime", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "The quality of the data is empirically evaluated by comparing the BLEU scores of the model trained on the data with the BLEU scores of the model trained on the same data but without punctuation. The BLEU scores of the model trained on the data with punctuation is 29.8/25.4 for French and 29.8/25.4 for German. The BLEU of the model trained on the same data without punctuation is 29.8/25.4 for French and 29.8/25 for German.\n", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "They use a dual RNN, which is a combination of an audio RNN and a text RNN.\n\nQuestion: What is the difference between the ARE and TRE models?\n\nAnswer: The ARE model uses only uses audio features, while the TRE model uses both audio and text features.\n\nQuestion: What is the difference between the MDRE and MDREAREA models?\n\nAnswer: The MDRE model uses a feed-forward neural network to combine the audio and text features, while the MDREAREA model uses an attention mechanism to focus on the specific parts of the", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "by 2.11 BLEU, 1.7 FKGL and 1.07 SARI\n\nQuestion: how does the method compare with the baselines?\n\nAnswer:: Our method produces better results comparing with the baselines, which demonstrates the effectiveness of adding simplified training data.\n\nQuestion: how does the method compare with the baselines?\n\nAnswer: Our method produces better results comparing with the baselines, which demonstrates the effectiveness of adding simplified training data.\n\nQuestion: how does the method compare with the baselines?\n\nAnswer: Our method", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "unanswerable\n\nQuestion: what is the main limitation of the approach?\n\nAnswer: unanswerable\n\nQuestion: what is the main novelty of the approach?\n\nAnswer: unanswerable\n\nQuestion: what is the main difference between the approach and the related work?\n\nAnswer: unanswerable\n\nQuestion: what is the main difference between the approach and the related work?\n\nAnswer: unanswerable\n\nQuestion: what is the main difference between the approach and the related work?\n\nAnswer: unanswerable\n\nQuestion: what is the main difference between the approach", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "\n\n\\begin{itemize}\n\\item CNN\n\\item BERT\n\\item LSTM-CRF\n\\item Multi-grained LSTM-CRF\n\\item Multi-tasking LSTM-CRF\n\\item Multi-granularity LSTM-CRF\n\\end{itemize}\n\nAnswer:\n\n\\begin{itemize}\n\\item CNN\n\\item BERT\n\\item LSTM-CRF\n\\item Multi-grained LSTM-CRF\n\\item Multi-tasking LSTM-CRF\n", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "The data was collected using crowdsourcing.\n\nQuestion: what is the size of the database?\n\nAnswer: The database is the largest public text-dependent and text-prompted speaker verification database in two languages: Persian and English. In addition, it is the largest text-independent speaker verification evaluation database, making it suitable to robustly evaluate state-of-the-art methods on different conditions.\n\nQuestion: what is the number of speakers in the database?\n\nAnswer: There are 1000 speakers in the Persian text-dependent part of the", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "\n\nThe RQE task is a binary classification task. The input is a question INLINEFORM0 and the output is a binary label INLINEFORM1 . The RQE task is a binary classification task. The input is a question INLINEFORM0 and the output is a binary label INLINE1 .\n\nThe RQE task is a binary classification task. The input is a question INLINEFORM0 and the output is a binary label INLINE1 .\n\nThe RQE task is a binary classification task. The input is a question INLINEFORM0 and the output is a binary label IN", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "The benchmark dataset is the Honeypot dataset and the quality is high.\n\nQuestion: What is the dataset and is its quality high?\n\nAnswer: The dataset is the Weibo dataset and the quality is high.\n\nQuestion: What is the performance of the proposed features?\n\nAnswer: The performance of the proposed features is high.\n\nQuestion: What is the performance of the proposed features?\n\nAnswer: The performance of the proposed features is high.\n\nQuestion: What is the performance of the proposed features?\n\nAnswer: The performance of the proposed features is high.\n\n", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "The decoder has an LSTM.\n\nQuestion: What is the main task of the auxiliary decoder?\n\nAnswer: The auxiliary decoder predicts the MSD of the target form.\n\nQuestion: What is the effect of the multilingual multi-tasking approach?\n\nAnswer: The multilingual multi-tasking approach outperforms the baseline by 18.30%.\n\nQuestion: What is the effect of the multi-tasking approach on the main task?\n\nAnswer: The effect of the multi-tasking approach on the main task is", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "No\n\nQuestion: Do they report results only on English data?\n\nAnswer: No\n\nQuestion: Do they report results only on English data?\n\nAnswer: No\n\nQuestion: Do they report results only on English data?\n\nAnswer: No\n\nQuestion: Do they report results only on English data?\n\nAnswer: No\n\nQuestion: Do they report results only on English data?\n\nAnswer: No\n\nQuestion: Do they report results only on English data?\n\nAnswer: No\n\nQuestion: Do they report results only on English data?\n\nAnswer: No\n", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "\n\nThe best performing model among the authors' submissions is the ensemble of (r19, r4, r12) on dev (internal) and (II, IV) on dev (external). The scores on dev (internal and external) are 0.673 and 0.663, respectively.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "unanswerable\n\nQuestion: what was the baseline?\n\nAnswer: unanswerable\n\nQuestion: what was the baseline?\n\nAnswer: unanswerable\n\nQuestion: what was the baseline?\n\nAnswer: unanswerable\n\nQuestion: what was the baseline?\n\nAnswer: unanswerable\n\nQuestion: what was the baseline?\n\nAnswer: unanswerable\n\nQuestion: what was the baseline?\n\nAnswer: unanswerable\n\nQuestion: what was the baseline?\n\nAnswer: unanswerable\n\nQuestion: what was", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "0.2862\n\nQuestion: What was their highest F-Measure score?\n\nAnswer: 0.2862\n\nQuestion: What was their highest MRR score?\n\nAnswer: 0.2862\n\nQuestion: What was their highest precision score?\n\nAnswer: 0.2862\n\nQuestion: What was their highest recall score?\n\nAnswer: 0.2862\n\nQuestion: What was their highest F-Measure score?\n\nAnswer: 0.862\n\nQuestion: What", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "The paper explores the use of second–order co–occurrence vectors to quantifying the similarity and relatedness between two terms.\n\nQuestion: What is the goal of the paper?\n\nAnswer: The goal of the paper is two–fold. First, we restrict the context used by the vector measure to words that exist in the biomedical domain, and second, we apply larger weights to those word pairs that are more similar to each other.\n\nQuestion: What is the hypothesis of the paper?\n\nAnswer: Our hypothesis was that this combination would reduce the amount of noise in the vectors and therefore increase", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "They match words by using a system called CFILT-preorder. It contains two re-ordering systems: (1) generic rules that apply to all Indian languages and (2) hindi-tuned rules which improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering.\n\nAnswer:: They match words by using a system called CFILT-preorder. It contains two re-ordering systems: (1) generic rules that apply to all Indian languages and (2) hindi-tuned rules which improve the generic rules by incorporating", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "Yes\n\nQuestion: Does the paper explore extraction from electronic health records?\n\nAnswer: No\n\nQuestion: Does the paper explore extraction from electronic health records?\n\nAnswer: Unanswerable\n\nQuestion: Does the paper explore extraction from electronic health records?\n\nAnswer: Unanswerable\n\nQuestion: Does the paper explore extraction from electronic health records?\n\nAnswer: Unanswerable\n\nQuestion: Does the paper explore extraction from electronic health records?\n\nAnswer: Unanswerable\n\nQuestion: Does the paper explore extraction from electronic health records?\n\nAnswer:", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "The experts were privacy lawyers, privacy researchers, and privacy policy writers.\n\nQuestion: What is the purpose of the corpus?\n\nAnswer: The purpose of the corpus is to provide a large-scale dataset of questions and answers for researchers to use in developing question-state-of-the-art models for answering questions about privacy policies.\n\nQuestion: What is the size of the corpus?\n\nAnswer: The corpus contains 3500+ questions and answers.\n\nQuestion: What is the format of the corpus?\n\nAnswer", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "\n\nThe models used for painting embedding are:\n\n1. A general end-to-end approach to sequence learning BIBREF3 makes minimal assumptions on the sequence structure. This model is widely used in tasks such as machine translation, text summarization, conversational modeling, and image captioning.\n2. A generative model using a deep recurrent architecture BIBREF0 has also been used for generating phrases describing an image. The network associates image attributes with poetic descriptions using a convolutional neural net. The poem is generated using a recurrent neural net which is trained using multi-ad", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "The transformer layer works better.\n\nAnswer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nQuestion: What is the difference between RoBERT and ToBERT?\n\nAnswer: RoBERT is a recurrent layer based model and ToBERT is a transformer based model.\n\nAnswer", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "Yes.\n\nQuestion: Do the authors hypothesize that humans' robustness to noise is due to their general knowledge?\n\nAnswer: No.\n\nQuestion: Do the authors hypothesize that humans' robustness to noise is due to their general knowledge?\n\nAnswer: Unanswerable.\n\nQuestion: Do the authors hypothesize that humans' robustness to noise is due to their general knowledge?\n\nAnswer: Yes.\n\nQuestion: Do the authors hypothesize that humans' robustness to noise is due to their general knowledge?\n\nAnswer: No.\n\nQuestion:", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "They addressed the following cyberbullying topics:\n\n1. Sexism\n\n2. Racism\n\n3. Anonymity\n\n4. Swearing\n\n5. Personal Attacks\n\n6. Cyberbullying in general\n\nQuestion: What SMPs did they use?\n\nAnswer: They used the following SMPs:\n\n1. Formspring\n\n2. Twitter\n\n3. Wikipedia\n\nQuestion: What was the size of the dataset?\n\nAnswer: The size of the dataset was as follows:\n\n1. Formspring: 1", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "The new context representation is obtained by concatenating the left context, the left entity and the middle context.\n\nQuestion: What is the difference between the CNN and the CNN?\n\nAnswer: The CNN is a standard CNN with only one window size. The CNN is a CNN with multi-windows, i.e. different window sizes.\n\nQuestion: What is the difference between the RNN and the RNN?\n\nAnswer: The RNN is a uni-directional RNN. The RNN is a bi-bi-directional RNN.\n\nQuestion: What is the difference between the", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "There are 4 different types of entities in the dataset.\n\nQuestion: What is the average number of words per sentence?\n\nAnswer: The average number of words per sentence is 12.\n\nQuestion: What is the average number of characters per word?\n\nAnswer: The average number of characters per word is 3.\n\nQuestion: What is the average number of characters per word in the dataset?\n\nAnswer: The average number of characters per word in the dataset is 3.\n\nQuestion: What is the average number of characters per word in the ILPRL dataset?\n\n", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "The resulting annotated data is of higher quality.\n\nQuestion: How much higher quality is the resulting annotated data?\n\nAnswer: The resulting annotated data is of higher quality.\n\nQuestion: How much higher quality is the resulting annotated data?\n\nAnswer: The resulting annotated data is of higher quality.\n\nQuestion: How much higher quality is the resulting annotated data?\n\nAnswer: The resulting annotated data is of higher quality.\n\nQuestion: How much higher quality is the resulting annotated data?\n\nAnswer: The resulting annotated data is of higher quality.\n\n", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "The imbalance in analyzed corpora is big.\n\nAnswer: The imbalance in analyzed corpora is big.\n\nAnswer: The imbalance in analyzed corpora is big.\n\nAnswer The imbalance in analyzed corpora is big.\n\nAnswer The imbalance in analyzed corpora is big.\n\nAnswer The imbalance in analyzed corpora is big.\n\nAnswer The imbalance in analyzed corpora is big.\n\nAnswer The imbalance in analyzed corpora is big.\n\nAnswer The imbalance in analyz", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "The Multi30K dataset BIBREF21 .\n\nQuestion: What is the main difference between the base and deliberation models?\n\nAnswer: The deliberation models use a deliberation network to generate a first-pass translation, which is then refined by a second-pass decoder.\n\nQuestion: What is the main difference between the base+sum, base+att and base+obj models?\n\nAnswer: The base+sum model uses the source text as additional context, base+att uses the source image as additional context, and base+obj uses the detected objects as additional context.\n", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "The strong baselines model is compared to is the Transformer BIBREF240.\n\nQuestion: What is the Transformer?\n\nAnswer: The Transformer is one kind of self-attention networks (SANs) which is proposed in BIBREF38. Encoder of the Transformer consists of one self-attention layer and a position-wise feed-forward layer. Decoder of the Transformer contains one self-attention layer, one encoder-decoder attention layer and one position-wise feed-forward layer. The Transformer uses residual connections around the sublayers and", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "\n\n  - BIBREF17\n  - BIBREF18\n  - BIBREF19\n  - BIBREF20\n  - BIBREF21\n  - BIBREF222\n  - BIBREF23\n  - BIBREF24\n  - BIBREF25\n  - BIBREF26\n  - BIBREF27\n\nQuestion: How many tweets are there?\n\nAnswer:\n\n  - 10000\n\nQuestion: How many tweets are there about Trump?", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "We conduct experiments on the SQuAD dataset BIBREF3. It contains 536 Wikipedia articles and 100k crowd-sourced question-answer pairs. The questions are written by crowd-workers and the answers are spans of tokens in the articles. We employ two different data splits by following Zhou2017NeuralQG and Du2017LearningTA. In Zhou2017NeuralQG, the original SQuAD development set is evenly divided into dev and test sets, while Du2017LearningTA", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "\n\nAnswer: what are the proposed approaches?\n\nAnswer:\n\nAnswer: what are the experimental results?\n\nAnswer:\n\nAnswer: what are the conclusions?\n\nAnswer:\n\nAnswer: what are the limitations?\n\nAnswer:\n\nAnswer: what are the future directions?\n\nAnswer:\n\nAnswer: what are the references?\n\nAnswer:\n\nAnswer: what are the acknowledgments?\n\nAnswer:\n\nAnswer: what are the keywords?\n\nAnswer:\n\nAnswer:\n\nAnswer: what are the questions?\n\nAnswer:\n", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "\n\n\\begin{itemize}\n\\item CSAT dataset for CSAT prediction, consisting of spoken transcripts (automatic via ASR).\n\\item 20 newsgroups for topic identification task, consisting of written text;\n\\item Fisher Phase 1 corpus for topic identification task, consisting of spoken transcripts (manual).\n\\end{itemize}\n\n\\end{document}", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "IMDb\n\nQuestion: What is the size of the PTB dataset?\n\nAnswer: 4,000\n\nQuestion: What is the size of the IWSLT dataset?\n\nAnswer: 209,000\n\nQuestion: What is the size of the TED.tst2013 dataset?\n\nAnswer: 209,000\n\nQuestion: What is the size of the TED.tst2013 dataset?\n\nAnswer: 209,000\n\nQuestion: What is the size of the", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "\n\nYes, the BERT model was evaluated on the same tasks as the LSTM-based models in BIBREF1 , BIBREF2 and BIBREF3 .\n\nNo, the BERT model was not evaluated on these tasks.\n\nUnanswerable.\n\nAnswer:\n\nYes, the BERT model was evaluated on the same tasks as the LSTM-based models in BIBREF1 , BIBREF2 and BIBREF3 .\n\nNo, the BERT model was not evaluated on these tasks.\n\nUnanswerable.\n\nAnswer:\n\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\nAnswer: No\n\n", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "The invertibility condition is that the Jacobian matrix of the projection is a diagonal matrix of the form INLINEFORM0 .\n\nQuestion: What is the normalizing flow?\n\nAnswer: The normalizing flow is a sequence of invertible transformations from the data space INLINEFORM1 to INLINEFORM2 .\n\nQuestion: What is the Gaussian HMM?\n\nAnswer: The Gaussian HMM is a model that assumes the observed word is a Gaussian distribution.\n\nQuestion: What is the DMV?\n\nAnswer: DMV is a model that defines a probability distribution over dependency parse trees and synt", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "\n\n\\begin{itemize}\n\\item The proposed schema is a multi-dimensional annotation schema, that is, it is not only concerned with the linguistic features of the gold standard, but also with the reasoning patterns that are required to be performed by the model.\n\\item The schema is composed of three main dimensions:\n\\begin{itemize}\n\\item Linguistic Features:\n\\begin{itemize}\n\\item The number of words occurring in sentence and question\n\\item The length of the longest n-gram shared by sentence and question\n\\item Whether a (uni- and bigram", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "\n\nWikiSmall: 89,042 sentence pairs\n\nWikiLarge: 296,402 sentence pairs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "The baselines are vanilla ST baseline, pre-training baseline, multi-task baseline, and many-to-many+pre-train.\n\nQuestion: What are the results?\n\nAnswer: The results are shown in Table TABREF29. Our method significantly outperforms the strong `many-to-many+pretrain' baseline by 3.6 and 2.2 BLEU scores respectively, indicating the proposed method is very effective that substantially improves the translation quality.\n\nQuestion: What are the ablation study?\n\nAnswer: To better understand", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "English\n\nQuestion: What is the main purpose of the paper?\n\nAnswer: To provide a dataset for the task of propaganda detection.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: A dataset for the task of propaganda detection.\n\nQuestion: What is the task of propaganda detection?\n\nAnswer: To identify propaganda in a given text.\n\nQuestion: What is the dataset used for the task of propaganda detection?\n\nAnswer: The PTC corpus.\n\nQuestion: What is the PTC corpus?\n\nAnswer: A cor", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "The models used in the experiment are SVMs and neural networks.\n\nQuestion: What is the performance of the models?\n\nAnswer: The performance of the models is not provided in the article.\n\nQuestion: What is the performance of the models in the experiment?\n\nAnswer: The performance of the models is not provided in the article.\n\nQuestion: What is the performance of the models in the experiment?\n\nAnswer: The performance of the models is not provided in the article.\n\nQuestion: What is the performance of the models in the experiment?\n\nAnswer: The performance of the models", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "No, the questions are not measured for the usefulness of the answer.\n\nQuestion: Do the answered questions measure for the quality of the answer?\n\nAnswer:: No, the questions are not measured for the quality of the answer.\n\nQuestion: Do the answered questions measure for the correctness of the answer?\n\nAnswer:: No, the questions are not measured for the correctness of the answer.\n\nQuestion: Do the answered questions measure for the relevance of the answer?\n\nAnswer:: No, the questions are not measured for the relevance of the answer.\n\nQuestion:", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "GloVe BIBREF13 is an unsupervised learning algorithm for obtaining vector representations for words. 200-dimensional GloVe embeddings trained on 2 Billion tweets are integrated. Edinburgh embeddings BIBREF14 are obtained by training skip-gram model on Edinburgh corpus BIBREF15 . Since tweets are abundant with emojis, Emoji embeddings BIBREF16 which are learned from the emoji descriptions have been used. Embeddings for each tweet are obtained by summing up individual word vectors and then", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "The authors found that their models could generate personalized recipes from incomplete input specifications and user histories.\n\nQuestion: What was the dataset used for the study?\n\nAnswer: The dataset used for the study was a large novel dataset of 180K recipes and 700K user-recipe interactions (reviews) scraped from Food.com, covering a period of 18 years (January 200000 to December 2018).\n\nQuestion: What was the name of the dataset used for the study?\n\nAnswer:", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "The combination of rewards for reinforcement learning is the combination of irony accuracy, sentiment and content preservation.\n\nQuestion: What is the main reason for the issue that the other four baselines tend to generate common and even not fluent sentences which are irrelevant to the input sentences and are hard to be identified as ironies?\n\nAnswer: The main reason for the issue is that the other four baselines tend to generate common and even not fluent sentences which are irrelevant to the input sentences and are hard to be identified as ironies.\n\nQuestion: What is the main reason for the issue that the other", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "The authors demonstrate that their model does not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "They compared to the following systems, for which results are reported in the referred literature. Please note that no other existing model was re-implemented, and results are those reported in the respective papers.\n\nDiscussion, conclusions and future work\n\nWe have explored the potential of using Facebook reactions in a distant supervised setting to perform emotion classification. The evaluation on standard benchmarks shows that models trained as such, especially when enhanced with continuous vector representations, can achieve competitive results without relying on any handcrafted resource. An interesting aspect of our approach is the view to domain adaptation via the selection", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "The dataset of hashtags is sourced from Twitter.\n\nQuestion: How is the dataset of hashtags sourced?\n\nAnswer: The dataset of hashtags is sourced from Twitter.\n\nQuestion: How is the dataset of hashtags sourced?\n\nAnswer: The dataset of hashtags is sourced from Twitter.\n\nQuestion: How is the dataset of hashtags sourced?\n\nAnswer: The dataset of hashtags is sourced from Twitter.\n\nQuestion: How is the dataset of hashtags sourced?", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "The corpus contains Persian accents from Tehran, Isfahan, Tabriz, Shiraz, Mashhad, and other cities.\n\nQuestion: what is the average duration of the utterances?\n\nAnswer: The average duration of the utterances is 10 seconds.\n\nQuestion: what is the average duration of the utterances?\n\nAnswer: The average duration of the utterances is 10 seconds.\n\nQuestion: what is the average duration of the utterances?\n\nAnswer: The average duration of the utterances is 10 seconds.\n\nQuestion", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "Word subspace can represent the semantic meaning of words.\n\nQuestion: What is the word subspace modeling?\n\nAnswer: Word subspace modeling is a method to represent the semantic meaning of words.\n\nQuestion: What is the word subspace modeling based on?\n\nAnswer: The word subspace modeling is based on the word2vec representation.\n\nQuestion: What is the word2vec representation?\n\nAnswer: The word2vec representation is a vector representation of words.\n\nQuestion: What is the word2vec representation based on?\n\nAnswer: The word2vec", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "SVM\n\nQuestion: What is the performance of the baseline model?\n\nAnswer: P=0.12\n\nQuestion: What is the performance of the proposed model?\n\nAnswer: P=0.93\n\nQuestion: What is the performance of the proposed model for the second stage?\n\nAnswer: P=0.844\n\nQuestion: What is the performance of the proposed model for the second stage?\n\nAnswer: P=0.844\n\nQuestion: What is the performance of the proposed model for the second stage?\n\nAnswer: P=", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "No.\n\nQuestion: Is SemCor3.0 reflective of English language data in general?\n\nAnswer: No.\n\nQuestion: Is SemCor3.0 reflective of English language data in general?\n\nAnswer: No.\n\nQuestion: Is SemCor3.0 reflective of English language data in general?\n\nAnswer: No.\n\nQuestion: Is SemCor3.0 reflective of English language data in general?\n\nAnswer: No.\n\nQuestion: Is SemCor3.0 reflective of English language data in general?\n\nAnswer: No.\n", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "110 hours\n\nQuestion: How many languages are in Augmented LibriSpeech dataset?\n\nAnswer: 11\n\nQuestion: How many speakers are in Augmented LibriSpeech dataset?\n\nAnswer: 11,000\n\nQuestion: How many sentences are in Augmented LibriSpeech?\n\nAnswer: 110,000\n\nQuestion: How many sentences are in Augmented LibriSpeech per language?\n\nAnswer: 110,000\n\nQuestion: How many sentences are", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "The dataset they used is the SemEval-2016 “Sentiment Analysis in Twitter” dataset.\n\nQuestion: What is the primary measure of performance?\n\nAnswer: The primary measure of performance is the macro-averaged Mean Absolute Error (INLINEREF1).\n\nQuestion: What is the secondary measure of performance?\n\nAnswer: The secondary measure of performance is the micro-averaged INLINEREF2 measure.\n\nQuestion: What is the architecture of the neural network?\n\nAnswer: The architecture of the neural network is a biLSTM network (enclosed by", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "They use large BERT.\n\nQuestion: Do they use large or small BERT?\n\nAnswer: They use large BERT.\n\nQuestion: Do they use large or small BERT?\n\nAnswer: They use large BERT.\n\nQuestion: Do they use large or small BERT?\n\nAnswer: They use large BERT.\n\nQuestion: Do they use large or small BERT?\n\nAnswer: They use large BERT.\n\nQuestion: Do they use large or small BERT?\n\nAnswer: They use large BERT.\n\nQuestion: Do they use", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "Yes.\n\nQuestion: Are the automatically constructed datasets subject to quality control?\n\nAnswer: No.\n\nQuestion: Are the automatically constructed datasets subject to quality control?\n\nAnswer: Unanswerable.\n\nQuestion: Are the automatically constructed datasets subject to quality control?\n\nAnswer: Yes.\n\nQuestion: Are the automatically constructed datasets subject to quality control?\n\nAnswer: No.\n\nQuestion: Are the automatically constructed datasets subject to quality control?\n\nAnswer: Unanswerable.\n\nQuestion: Are the automatically constructed datasets subject to quality control?\n\nAnswer: Yes.\n\n", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "Yes\n\nQuestion: Is the model trained on a specific dataset?\n\nAnswer: Yes\n\nQuestion: Is the model trained on a specific dataset?\n\nAnswer: Yes\n\nQuestion: Is the model trained on a specific dataset?\n\nAnswer: Yes\n\nQuestion: Is the model trained on a specific dataset?\n\nAnswer: Yes\n\nQuestion: Is the model trained on a specific dataset?\n\nAnswer: Yes\n\nQuestion: Is the model trained on a specific dataset?\n\nAnswer: Yes\n\nQuestion: Is the model trained on a specific dataset?\n\nAnswer: Yes\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "They performed well on emotion detection.\n\nQuestion: What was their performance on emotion detection?\n\nAnswer: They performed well on emotion detection.\n\nQuestion: What was their performance on emotion detection?\n\nAnswer: They performed well on emotion detection.\n\nQuestion: What was their performance on emotion detection?\n\nAnswer: They performed well on emotion detection.\n\nQuestion: What was their performance on emotion detection?\n\nAnswer: They performed well on emotion detection.\n\nQuestion: What was their performance on emotion detection?\n\nAnswer: They performed well", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "The tagging scheme employed is INLINEFORM0 .\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: The tagging scheme employed is INLINEFORM1 .\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: The tagging scheme employed is INLINEFORM2 .\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: The tagging scheme employed is INLINEFORM3 .\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: The tagging scheme employed is INLINEFORM4 .\n\nQuestion: What is the tagging", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "No\n\nQuestion: Is CoVost a multilingual speech-to-to-text translation corpus?\n\nAnswer: Yes\n\nQuestion: Is CoVost diversified with over 11,000 speakers and over 60 accents?\n\nAnswer: Yes\n\nQuestion: Is CoVost free to use with a CC0 license?\n\nAnswer: Yes\n\nQuestion: Is the additional Tatoeba evaluation samples are also CC-licensed?\n\nAnswer: Yes\n\nQuestion: Is the article well-written?\n\nAnswer: Yes\n\n", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "Robustness of a model is defined as the ability of the model to handle unbalanced knowledge.\n\nQuestion: What are the three regularization terms?\n\nAnswer: The three regularization terms are incorporating neutral features, maximum entropy principle, and KL divergence.\n\nQuestion: What is the difference between incorporating neutral features and maximum entropy?\n\nAnswer: Incorporating neutral features is the simplest way of regularization, which doesn't require any modification of GE-FL but just finding out some common features. But as Figure 1 (a) shows, only using neutral features are", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "We evaluated SBERT against average GloVe embeddings, InferSent BIBREF4, and Universal Sentence Encoder BIBREF5.\n\nQuestion: What is the performance of SBERT on the STS benchmark BIBREF10?\n\nAnswer: SBERT out-of-the-box maps sentences to a vector space that is rather unsuitable to be used with common similarity measures like cosine-similarity. The performance for seven STS tasks was below the performance of average GloVe embeddings.\n\nQuestion: What is the performance of SBER", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "\n\n\\begin{itemize}\n\\item English OntoNotes5.5: +0.96\n\\item Chinese OntoNotes4.0: +2.36\n\\item English QuoRef: +1.46\n\\item Chinese QuoRef: +1.41\n\\end{itemize}\n\n\\section{References}\n\n\\end{document}", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "They test their conflict method on two tasks:\n\n1. Quora duplicate question pair detection\n2. Ranking questions in Bing's People Also Ask\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "They compared against the following baselines:\n\n1. Bi-LSTM with generalized pooling\n2. Bi-LSTM with generalized pooling\n3. Bi-LSTM with generalized pooling\n4. Bi-LSTM with generalized pooling\n5. Bi-LSTM with generalized pooling\n6. Bi-LSTM with generalized pooling\n7. Bi-LSTM with generalized pooling\n8. Bi-LSTM with generalized pooling\n9. Bi-LSTM with generalized pooling\n10. Bi-LSTM with generalized", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "The core component for KBQA is the relation detection.\n\nQuestion: What is the core component for KBQA?\n\nAnswer: The core component for KBQA is the relation detection.\n\nQuestion: What is the core component for KBQA?\n\nAnswer: The core component for KBQA is the relation detection.\n\nQuestion: What is the core component for KBQA?\n\nAnswer: The core component for KBQA is the relation detection.\n\nQuestion: What is the core component for KBQA?\n\nAnswer:", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "The baseline models are the encoder-decoder model and the personalized models.\n\nQuestion: What are the personal models?\n\nAnswer: The personal models are the personalized models.\n\nQuestion: What are the personalized models?\n\nAnswer: The personalized models are the personalized models.\n\nQuestion: What are the personalized models?\n\nAnswer: The personalized models are the personal models.\n\nQuestion: What are the personalized models?\n\nAnswer: The personalized models are the personal models.\n\nQuestion: What are the personal models?\n\nAnswer:", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "\n\n1. Manual inspection of the data\n2. Automatic methods\n\nQuestion: What is the difference between linguistic bias and unwarranted inferences??\n\nAnswer: Linguistic bias is the use of specific words to describe an entity that does not match the annotator's expectations. Unwarranted inferences are the use of additional assumptions about the world.\n\nQuestion: What is the difference between linguistic bias and unwarranted inferences??\n\nAnswer: Linguistic bias is the use of specific words to describe an entity that does not match the annotator's", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "They experimented with the following models:\n\n1. Plain stacked LSTMs\n2. Models with different INLINEFORM0\n3. Models without INLINEFORM1\n4. Models that integrate lower contexts via peephole connections\n\nQuestion: What is the difference between INLINEFORM0 and INLINEFORM1 ?\n\nAnswer: INLINEFORM0 is the input to the model, and INLINEFORM1 is the output.\n\nQuestion: What is the difference between INLINEFORM0 and INLINEFORM1 ?\n\nAnswer: INLINEFORM1 is", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "No\n\nQuestion: Do they report results only on English data?\n\nAnswer: No\n\nQuestion: Do they report results only on English data?\n\nAnswer: No\n\nQuestion: Do they report results only on English data?\n\nAnswer: No\n\nQuestion: Do they report results only on English data?\n\nAnswer: No\n\nQuestion: Do they report results only on English data?\n\nAnswer: No\n\nQuestion: Do they report results only on English data?\n\nAnswer: No\n\nQuestion: Do they report results only on English data?\n\nAnswer: No\n", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "The authors experimented with the following summarization algorithms:\n\n1. Sumy package in Python\n2. Summarization with integer linear programming (ILP)\n\nQuestion: What is the difference between the two summarization algorithms?\n\nAnswer: The difference between the two summarization algorithms is that the Sumy package in Python uses a sentence-based approach, while the ILP-based summarization uses a phrase-based approach.\n\nQuestion: What is the performance of the ILP-based summarization algorithm?\n\nAnswer: The performance of the ILP-based summarization algorithm is comparable to", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "Unanswerable\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: Unanswerable\n\nQuestion: What is the main limitation of the paper?\n\nAnswer: Unanswerable\n\nQuestion: What is the main takeaway from the paper?\n\nAnswer: Unanswerable\n\nQuestion: What is the main conclusion of the paper?\n\nAnswer: Unanswerable\n\nQuestion: What is the main conclusion of the paper?\n\nAnswer: Unanswerable\n\nQuestion: What is the main conclusion of the paper?\n\nAnswer: Unanswerable\n\nQuestion:", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "The answer is \"unanswerable\".\n\nAnswer: The answer is \"unanswerable\".\n\nAnswer: The answer is \"unanswerable\".\n\nAnswer: The answer is \"unanswerable\".\n\nAnswer: The answer is \"unanswerable\".\n\nAnswer: The answer is \"unanswerable\".\n\nAnswer: The answer is \"unanswerable\".\n\nAnswer: The answer is \"unanswerable\".\n\nAnswer: The answer is \"unanswerable\".\n\nAnswer: The answer is \"unanswerable\".\n\nAnswer: The answer is \"unanswerable\".", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "The corpus used for the task is DTA18 and DTA19.\n\nAnswer::: The corpus used for the task is DTA18 and DTA19.\n\nAnswer:: The corpus used for the task is DTA18 and DTA19.\n\nAnswer:: The corpus used for the task is DTA18 and DTA19.\n\nAnswer:: The corpus used for the task is DTA18 and DTA19.\n\nAnswer:: The corpus used for the task is DTA18", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "Kannada, Hindi, Telugu, Malayalam, Bengali, and English.\n\nQuestion: What is the amount of audio data for training and testing for each of the language?\n\nAnswer: The amount of audio data for training and testing for each of the language is shown in the table bellow.\n\nQuestion: What is the architecture of the model?\n\nAnswer: The model uses convolution layers with Relu activations to map the spectrogram of size 257x500 input into 3D feature map of size 1x32x51", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "The model performance on target language reading comprehension is not good.\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nAnswer the question: What is the model performance on target language reading comprehension?\n\nAnswer: The model performance on target language reading comprehension is not good.\n\nAnswer the question as concisely", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "The proposed model outperforms the baselines by a significant margin.\n\nQuestion: How big is the difference in performance between proposed model and Uniform Model?\n\nAnswer: The proposed model outperforms the Uniform Model by a significant margin.\n\nQuestion: How big is the difference in performance between proposed model and Uniform Model?\n\nAnswer: The proposed model outperforms the Uniform Model by a significant margin.\n\nQuestion: How big is the difference in performance between proposed model and Uniform Model?\n\nAnswer: The proposed model outperforms the Uniform Model by a significant margin.", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "ARAML improves the performance of text generation tasks by 1.5% on average.\n\nAnswer: ARAML improves the performance of text generation tasks by 1.5% on average.\n\nAnswer: ARAML improves the performance of text generation tasks by 1.5% on average.\n\nAnswer: ARAML improves the performance of text generation tasks by 1.5% on average.\n\nAnswer: ARAML improves the performance of text generation tasks by 1.5% on average.\n\nAnswer: ARAML improves the performance of", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "The authors present evidence that the model can capture some biases in data annotation and collection by examining the results of the model. The authors show that the model can detect some biases in the process of collecting or annotating datasets by investigating a mixture of contextual information embedded in the BERT’s layers and a set of features associated to the different type of biases in data.\n\nAnswer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "Yes, we test a number of baselines, including a majority-class baseline, a word count baseline, a BERT-based baseline, and a two-stage classifier.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset consists of 573 questions and 3500+ expert annotations.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset consists of 573 questions and 3500+ expert annotations.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "The size of the dataset is 100000.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The size of the dataset is 100000.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The size of the dataset is 100000.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The size of the dataset is 100000.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The size of the dataset is 1", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "The proposed method improves the F1 score by +0.58 for MRPC and +0.73 for QQP.\n\nQuestion: What are the hyperparameters in Tversky index?\n\nAnswer:: The hyperparameters in Tversky index are $\\alpha $ and $\\beta $. The highest F1 for Chinese OntoNotes4.0 is 84.67 when $\\alpha $ is set to 0.6 while for QuoRef, the highest F1 is 68.444 when $\\alpha $ is set to 0.4.\n\n", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "\n\nThe data used in this work is from BIBREF0 . The data is a subset of the data from BIBREF0 , which is a collection of EEG data from 100 participants who read a chapter of Harry Potter and the Sorcerer’s Stone BIBREF9 . The data is a subset of the data from BIBREF0 , which is a collection of EEG data from 100 participants who read a chapter of Harry Potter and the Sorcerer’s Stone BIBREF9 . The data is a subset of the data from BIBREF0 , which", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "The subjects were presented with a series of stimuli, including words, phonemes, and syllables.\n\nQuestion: What was the purpose of the study?\n\nAnswer: The purpose of the study was to investigate the investigate the neural correlates of speech production.\n\nQuestion: What was the experimental design?\n\nAnswer: The experimental design was a within-subjects, cross-modal, and event-related design.\n\nQuestion: What was the data collection method?\n\nAnswer: The data collection method was a multichannel EEG recording.\n\nQuestion: What was the data analysis", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "We compare our model to existing models with ROUGE in Table TABREF25 to establish that our model produces relevant headlines and we leave the sensationalism for human evaluation.\n\nQuestion: What is the sensationalism score of the test set?\n\nAnswer: The test set headlines achieves 57.8% sensationalism score, much larger than Pointer-Gen baseline, which also supports our intuition that generated headlines will be less sensational than the original one.\n\nQuestion: What is the difference between Pointer-Gen+ARL-SEN and Pointer", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "\n\n Traditional machine learning classifiers: Naïve Bayes, Logistic Regression, Support Vector Machine, Random Forests, Gradient Boosted Trees\n\n Neural network based models: Convolutional Neural Network, Recurrent Neural Network, HybridCNN, RNN with LTC, CNN with context tweets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "We use the transformer implementation of the fairseq toolkit.\n\nQuestion: What is the vocabulary size of the language model?\n\nAnswer:: The vocabulary size of the language model is 37K.\n\nQuestion: What is the vocabulary size of the sequence to sequence model?\n\nAnswer:: The vocabulary size of the sequence to sequence model is 32K.\n\nQuestion: What is the vocabulary size of the sequence to sequence model?\n\nAnswer:: The vocabulary size of the sequence to sequence", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "The weights are dynamically adjusted based on the probability of the correct answer.\n\nQuestion: How does the model handle imbalanced data?\n\nAnswer: The model handles imbalanced data by using a weighted cross-entropy loss function.\n\nQuestion: How does the model handle imbalanced data?\n\nAnswer: The model handles imbalanced data by using a weighted cross-entropy loss function.\n\nQuestion: How does the model handle imbalanced data?\n\nAnswer: The model handles imbalanced data by using a weighted cross-entropy loss function.\n\n", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "\n\nThe results from these proposed strategies are as follows:\n\n1. KG-A2C-chained:\n\nThis method is able to pass the bottleneck of a score of 40, whereas A2C-chained fails to even outperform the baseline A2C.\n\n2. KG-A2C-Explore:\n\nThis method is able to pass the bottleneck of a score of 40, whereas A2C-Explore fails to even outperform the baseline A2C.\n\n3. A", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "An individual model consists of a set of parameters, which are estimated from the training data.\n\nQuestion: What is the role of the crosslingual latent variables?\n\nAnswer: The crosslingual latent variables capture correlations between roles in different languages, and regularize the parameter estimates of the monolingual models.\n\nQuestion: What is the difference between the monolingual and multilingual models?\n\nAnswer: The monolingual model is trained on the data of one language, and the multilingual model is trained on the data of multiple languages.\n\nQuestion: What", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "unanswerable\n\nQuestion: How is non-standard pronunciation identified?\n\nAnswer: unanswerable\n\nQuestion: How is non-standard pronunciation identified?\n\nAnswer: unanswerable\n\nQuestion: How is non-standard pronunciation identified?\n\nAnswer: unanswerable\n\nQuestion: How is non-standard pronunciation identified?\n\nAnswer: unanswerable\n\nQuestion: How is non-standard pronunciation identified?\n\nAnswer: unanswerable\n\nQuestion: How is non-standard pronunciation identified?\n\nAnswer: unanswer", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "A semicharacter architecture is a type of neural network architecture that uses a combination of characters to represent words.\n\nQuestion: What is the purpose of using a semicharacter architecture?\n\nAnswer: The purpose of using a semicharacter architecture is to improve the accuracy of word recognition in natural language processing (NLP) tasks.\n\nQuestion: What are the benefits of using a semicharacter architecture?\n\nAnswer: The benefits of using a semicharacter architecture include improved accuracy of word recognition, better performance invariance to adversarial attacks, and lower sensitivity to adversarial attacks", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.\n\nQuestion: which models are compared?\n\nAnswer: MElt, MarMoT, FREQBIN, BIBREF20\n\nQuestion: which models are the best?\n\nAnswer: MElt, MarMoT, FREQBIN\n\nQuestion: which models are the best for OOV tagging?\n\nAnswer: FREQBIN, MElt\n\nQuestion: which models are the best for rare words?\n\nAnswer", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "NCEL outperforms the state-of-the-art collective methods across five different datasets.\n\nQuestion: What are the main modules of NCEL?\n\nAnswer: NCEL consists of three main modules: encoder, sub-graph convolution network (sub-GCN) and decoder.\n\nQuestion: What are the main advantages of NCEL?\n\nAnswer: NCEL is end-to-end trainable, and applies GCN on subgraphs instead of the entire entity graph to efficiently learn features from both local and global information.\n\nQuestion: What are", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "Yes\n\nQuestion: Is the data publicly available?\n\nAnswer: Yes\n\nQuestion: Is the data available for download?\n\nAnswer: Yes\n\nQuestion: Is the data available for download?\n\nAnswer: Yes\n\nQuestion: Is the data available for download?\n\nAnswer: Yes\n\nQuestion: Is the data available for download?\n\nAnswer: Yes\n\nQuestion: Is the data available for download?\n\nAnswer: Yes\n\nQuestion: Is the data available for download?\n\nAnswer: Yes\n\nQuestion: Is the data available for download?\n\nAnswer:", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "The baseline used was the system by Rei2016, trained on the same FCE dataset.\n\nQuestion: What was the error generation method used?\n\nAnswer: The error generation method used was the pattern-based method, which randomly samples the error locations.\n\nQuestion: What was the error generation method used?\n\nAnswer: The error generation method used was the machine translation method, which translates from correct to incorrect sentences.\n\nQuestion: What was the error generation method used?\n\nAnswer: The error generation method used was the combination of the pattern-based method and the machine translation", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "The annotated clinical notes were obtained from the 2010 i2b2/VA dataset.\n\nQuestion: what is the purpose of the glossary?\n\nAnswer: The purpose of the glossary is to provide a list of terms that are relevant to the clinical entities in the user queries.\n\nQuestion: what is the purpose of the term matching algorithm?\n\nAnswer: The term matching algorithm is used to match the terms in the glossary to the clinical entities in the user queries.\n\nQuestion: what is the purpose of the NER model?\n\nAnswer", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "Masking words in the decoder is helpful because it forces the decoder to generate a more fluent and natural sequence.\n\nQuestion: Why is the refine decoder necessary?\n\nAnswer: The refine decoder is necessary because it helps the decoder to generate a more fluent and natural sequence.\n\nQuestion: Why is the reinforcement learning objective necessary?\n\nAnswer: The reinforcement learning objective is necessary because it helps the decoder to generate a more fluent and natural sequence.\n\nQuestion: Why is the summary length important?\n\nAnswer: The summary length is important", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "unanswerable\n\nQuestion: What is the objective function they optimize?\n\nAnswer:: unanswerable\n\nQuestion: What is the model architecture they use?\n\nAnswer:: unanswerable\n\nQuestion: What is the model architecture they use?\n\nAnswer:: unanswerable\n\nQuestion: What is the model architecture they use?\n\nAnswer:: unanswerable\n\nQuestion: What is the model architecture they use?\n\nAnswer:: unanswerable\n\nQuestion: What is the model architecture they use?\n\nAnswer:: unanswerable\n\nQuestion: What", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nArticle: Introduction\nCancer is one of the leading causes of death in the world, with over 80,000 deaths registered in Canada in 2017 (Canadian Cancer Statistics 2017). A computer-", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "The dataset is annotated with 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression (e.g., “Citizens fear an economic depression\") or evidence of depression (e.g., “depressed over disappointment\"). If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood (e.g., “feeling down in the dumps\"), disturbed sleep (e.g., “another rest", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "\n\nThe eight NER tasks they evaluated on were:\n\n1.\n\n2.\n\n3.\n\n4.\n\n5.\n\n6.\n\n7.\n\n8.\n\nInexpensive Domain Adaptation of Pretrained Language Models (Appendix) ::: Experiment 2: Covid-19 QA ::: Note on our reproduction experiments\nWe found it easier to reproduce or exceed BIBREF2's results for general-domain BERT, compared to their results for BioBERTv1.0 (see Figure FIGREF", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "The training data was translated by using the machine translation platform Apertium BIBREF5 .\n\nQuestion: How was the training data augmented?\n\nAnswer:: The training data was augmented by using the machine translation platform Apertium BIBREF5 .\n\nQuestion: How was the training data augmented?\n\nAnswer: The training data was augmented by using the machine translation platform Apertium BIBREF5 .\n\nQuestion: How was the training data augmented?\n\nAnswer: The training data was augmented by using the machine translation platform Apertium BIBREF5 .", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "unanswerable\n\nQuestion: What is the name of the system?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the company that developed the system?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the company that developed the system?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the company that developed the system?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the company that developed the system?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the company that developed the", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "The baseline for this task was a very simple logistic regression classifier with default parameters, where we represented the input instances with a single feature: the length of the sentence.\n\nQuestion: What was the best performing team?\n\n\nAnswer: Team newspeak achieved the best results on the test set for the FLC task using 20-way word-level classification based on BERT.\n\nQuestion: What was the best performing team for the sentence-level classification task?\n\nAnswer: Team CAUnLP used two context-aware representations based on BERT. In the first representation, the target", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "The baselines that they compare with are:\n\n1. A rule-based system for pun location that scores candidate words according to eleven simple heuristics.\n\n2. A neural method for homographic pun location that combines predictions from three classifiers.\n\n3. A neural method for homographic pun location that uses word senses to make decisions.\n\n4. A neural method for heterographic pun location that combines predictions from three classifiers.\n\n5. A neural method for heterographic pun location that uses word2vec similarity between every pair of words in the context and position to pinpoint", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "The political bias of different sources is included in the model by using the Balanced Random Forest classifier, which is a machine learning algorithm that is used to classify the political bias of the sources. The Balanced Random Forest classifier is used to balance the political bias of the sources by using a random forest classifier with a balanced dataset. The random forest classifier is used to classify the political bias of the sources by using a random forest classifier with a balanced dataset. The balanced dataset is used to class the political bias of the sources by using a balanced dataset with a random forest classifier.\n\n", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "The dataset comes from ancient Chinese texts.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset contains 1.24M bilingual sentence pairs.\n\nQuestion: What is the size of the augmented dataset?\n\nAnswer: The augmented dataset contains 48K bilingual sentence pairs.\n\nQuestion: What is the size of the unaugmented dataset?\n\nAnswer: The unaugmented dataset contains 48K bilingual sentence pairs.\n\nQuestion: What is the size of the Test set?\n\nAnswer: The Test", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "English\n\nQuestion: Are the tweets offensive?\n\nAnswer: Yes\n\nQuestion: Are the tweets targeted?\n\nAnswer: Yes\n\nQuestion: Are the tweets targeted at an individual?\n\nAnswer: Yes\n\nQuestion: Are the tweets targeted at a group?\n\nAnswer: Yes\n\nQuestion: Are the tweets targeted at other?\n\nAnswer: Yes\n\nQuestion: Are the tweets targeted at an individual?\n\nAnswer: Yes\n\nQuestion: Are the tweets targeted at a group?\n\nAnswer: Yes\n", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "\n\nThe Chinese Penn Treebank (CPTB) is a 100M-word corpus of Chinese text with 100K-word vocabulary and 100K-word nonterminal vocabulary. It was used for training the neural PCFG/compound PCFG.\n\nThe Chinese Parallel Treebank (CPTB) is a 100M-word corpus of Chinese text with 10K-word vocabulary and 100K-word nonterminal vocabulary. It was", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "3\n\nQuestion: What is the purpose of the UTCNN model?\n\nAnswer: To predict the stance of a post based on the content of the post, the user who posted the post, and the user who commented on the post.\n\nQuestion: What is the difference between the UTCNN model and the UTCNN model?\n\nAnswer: The UTC model is a content-based model, while the UTCNN model is a content-based model with user, topic, and comment information.\n\nQuestion: What is the difference between the UTCNN model and the PSL model?\n\nAnswer:", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "The dataset used in this paper is the Flickr dataset.\n\nQuestion: what is the purpose of the paper?\n\nAnswer: The purpose of the paper is to propose a model to learn geographic location embeddings using Flickr tags, numerical environmental features, and categorical information.\n\nQuestion: what is the main contribution of the paper?\n\nAnswer: The main contribution of the paper is to propose a model to learn geographic location embeddings using Flickr tags, numerical environmental features, and categorical information.\n\nQuestion: what is the main result of the paper?\n\n", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "The clinical datasets used in the paper are the NUBes-PHI dataset and the MEDDOCAN dataset.\n\nQuestion: What is the NUBes-PHI dataset?\n\nAnswer: The NUBes-PHI dataset is a dataset of Spanish clinical narrative manually annotated with sensitive information.\n\nQuestion: What is the MEDDOCAN dataset?\n\nAnswer: The MEDDOCAN dataset is a dataset of Spanish clinical narrative manually annotated with sensitive information.\n\nQuestion: What is the BERT model?\n\nAnswer: The BERT model is a", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "They used the following traditional linguistics features:\n\n1. Unigram (with principal components of unigram feature vectors)\n2. Sarcasm (the feature-set reported by joshi2015harnessinging subsuming unigram features and features from other reported systems)\n3. Gaze (the simple and complex cognitive features they introduce, along with readability and word count features)\n4. Gaze+Sarcasm (the complete set of features)\n\nQuestion: What is the difference between the gaze features and the gaze+sarcasm features?", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are: \n\n1. The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are: \n\n2. The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are: \n\n3. The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are: \n\n4. The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\n", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "Galatasaray and Fenerbahçe\n\nQuestion: What is the stance of the tweets towards Galatasaray?\n\nAnswer: Favor\n\nQuestion: What is the stance of the tweets towards Fenerbahçe?\n\nAnswer: Against\n\nQuestion: What is the stance of the tweets towards Galatasaray?\n\nAnswer: Against\n\nQuestion: What is the stance of the tweets towards Fenerbahçe?\n\nAnswer: Favor\n\nQuestion: What is the stance of the tweets", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "The authors conduct experiments on the transformation from non-ironic sentences to ironic sentences and the transformation from ironic sentences to non-ironic sentences.\n\nQuestion: What are the results of the experiments?\n\nAnswer: The results of the experiments are shown in Table TABREF35 and Table TABREF46 .\n\nQuestion: What are the conclusions of the experiments?\n\nAnswer: The conclusions of the experiments are that our model outperforms other generative models and our rewards are effective.\n\nQuestion: What are the limitations of the experiments?\n\nAnswer: The limitations", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "Gaussian-masked directional multi-head attention is a variant of multi-head attention which is proposed in BIBREF24. Gaussian-masked directional multi-head attention is a function to combine the Gaussian weight to the self-attention.\n\nQuestion: How does bi-affine attention scorer works?\n\nAnswer: Bi-affine attention scorer is the component that we use to label the gap. Bi-affine attention is developed from bilinear attention which has been used in dependency parsing BIBREF26 and SRL BIBREF27. The distribution of the gap", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "Twitter\n\nQuestion: What types of social media did they consider?\n\nAnswer: Twitter\n\nQuestion: What types of social media did they consider?\n\nAnswer: Twitter\n\nQuestion: What types of social media did they consider?\n\nAnswer: Twitter\n\nQuestion: What types of social media did they consider?\n\nAnswer: Twitter\n\nQuestion: What types of social media did they consider?\n\nAnswer: Twitter\n\nQuestion: What types of social media did they consider?\n\nAnswer: Twitter\n\nQuestion: What types of social media did they consider?\n\nAnswer: Twitter\n", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "The baseline features are the features extracted by the baseline CNN.\n\nQuestion: What are the network's pre-trained features?\n\nAnswer: The pre-trained features are the features extracted by the pre-trained CNNs.\n\nQuestion: What are the network's pre-trained models?\n\nAnswer: The pre-trained models are the models trained on the pre-trained features.\n\nQuestion: What are the network's baseline features?\n\nAnswer: The baseline features are the features extracted by the baseline CNN.\n\nQuestion: What", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "The hyperparameters varied in the experiments on the four tasks were the number of clusters (INLINEFORM0 ) and the number of classes (INLINEFORM1 ) for the NER task, the number of clusters (INLINEFORM0 ) for the NER task, the number of classes (INLINEFORM1 ) for the sentiment classification task, and the number of classes (INLINEFORM0 ) for the sentiment quantification task.\n\nAnswer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "Unanswerable\n\nQuestion: What were the scores of their system?\n\nAnswer: Unanswerable\n\nQuestion: What were the scores of their system?\n\nAnswer: Unanswerable\n\nQuestion: What were the scores of their system?\n\nAnswer: Unanswerable\n\nQuestion: What were the scores of their system?\n\nAnswer: Unanswerable\n\nQuestion: What were the scores of their system?\n\nAnswer: Unanswerable\n\nQuestion: What were the scores of their system?\n\nAnswer: Unanswerable\n\nQuestion: What were the scores of their system", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "The corpus is 53 documents, which contain an average number of 156.1 sentences per document, each with 19.5555 tokens on average.\n\nQuestion: How many entities are annotated?\n\nAnswer: The corpus consists of 8,275 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total. However, as mentioned above, only case", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "Yes, it is possible to convert a cloze-style questions to a naturally-looking questions.\n\nAnswer: No, it is not possible to convert a cloze-style questions to a naturally-looking questions.\n\nAnswer: Unanswerable\n\nAnswer: Yes, it is possible to convert a cloze-style questions to a naturally-looking questions.\n\nAnswer: No, it is not possible to convert a cloze-style questions to a naturally-looking questions.\n\nAnswer: Unanswerable\n\nAnswer: Yes, it is possible to convert a cloze-style questions to a naturally", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "\n\n\\begin{itemize}\n\\item Sentiment analysis\n\\item Web-page classification\n\\item Science\n\\item Medical and healthcare\n\\end{itemize}\n\nQuestion: What is the difference between GE-FL and GE-FL?\n\nAnswer:\n\n\\begin{itemize}\n\\item GE-FL is a generalization of GE-FL.\n\\item GE-FL is more robust against unbalanced labeled features and unbalanced class distribution.\n\\end{itemize}\n\nQuestion: What is the difference between GE-FL and", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "\n\nTheir model is compared to the following methods:\n\n1. The TREC-2006 QA system (Li and Roth BIBREF6 )\n2. The TREC-2007 QA system (Li and Roth BIBREF6 )\n3. The TREC-2008 QA system (Li and Roth BIBREF6 )\n4. The TREC-2009 QA system (Li and Roth BIBREF6 )\n5. The TREC-2010 QA system (Li and Roth", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "The training sets of these versions of ELMo are larger than the previous ones.\n\nQuestion: How much larger are the training sets of these versions of ELMo compared to the previous ones?\n\nAnswer: The training sets of these versions of ELMo are larger than the previous ones.\n\nQuestion: How much larger are the training sets of these versions of ELMo compared to the previous ones?\n\nAnswer: The training sets of these versions of ELMo are larger than the previous ones.\n\nQuestion: How much larger are the training sets of these versions of ELMo compared to the previous ones?\n\nAnswer", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "16225\n\nQuestion: How many entities does the dataset contain?\n\nAnswer: 16225\n\nQuestion: How many sentences does the dataset contain?\n\nAnswer: 1622225\n\nQuestion: How many entities does the dataset contain?\n\nAnswer: 16225\n\nQuestion: How many sentences does the dataset contain?\n\nAnswer: 16225\n\nQuestion: How many entities does the dataset contain?\n\nAnswer: 16225\n\nQuestion: How many sentences does the dataset", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "They compare to MLP, Eusboost and MWMOTE.\n\nQuestion: What is the proposed s2s framework?\n\nAnswer: The proposed s2s framework is a novel framework to effectively learn the class discriminative characteristics, even from low data resources. In this framework, more than one sample (here, two samples) are simultaneously considered to train the classifier. Further, this framework allows to generate multiple instances of the same test sample, by considering preselected reference samples, to achieve a more profound decision making.\n\nQuestion: What is the significance of the proposed s2s", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "Yes\n\nQuestion: Does their NER model learn NER from both text and images?\n\nAnswer: Yes\n\nQuestion: Does their NER model learn NER from both text and images?\n\nAnswer: Yes\n\n\nQuestion: Does their NER model learn NER from both text and images?\n\nAnswer: Yes\n\nQuestion: Does their NER model learn NER from both text and images?\n\nAnswer: Yes\n\nQuestion: Does their NER model learn NER from both text and images?\n\nAnswer: Yes\n\nQuestion: Does their NER model learn N", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "No.\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: No.\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: No.\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: No.\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: No.\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: No.\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: No.\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: No.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "0.2862\n\nQuestion: What was their highest F-Measure score?\n\nAnswer: 0.2862\n\nQuestion: What was their highest MRR score?\n\nAnswer: 0.2862\n\nQuestion: What was their highest F-Measure score?\n\nAnswer: 0.2862\n\nQuestion: What was their highest MRR score?\n\nAnswer: 0.2862\n\nQuestion: What was their highest F-Measure score?\n\nAnswer: 0.286\n", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "They evaluate on the Wall Street Journal (WSJ) portion of the Penn Treebank.\n\nQuestion: What is the Markov prior in POS tagging?\n\nAnswer: The Markov prior in POS tagging is a Markov model that defines a probability distribution over POS tags.\n\nQuestion: What is the DMV prior in dependency parsing?\n\nAnswer: The DMV prior in dependency parsing is a generative model that defines aims to generate a dependency parse tree and syntactic categories.\n\nQuestion: What is the Gaussian HMM?\n\nAnswer: The Gaussian H", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "The authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques by conducting a survey among engineers and identifying a spectrum of three typical personas.\n\nQuestion: What is the name of the toolkit?\n\nAnswer: The name of the toolkit is NeuronBlocks.\n\nQuestion: What is the name of the article?\n\nAnswer: The name of the article is Introduction.\n\nQuestion: What is the name of the question?\n\nAnswer: The name of the question is How do the authors evidence the claim that many engine", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "\n\nWe evaluate our system on two benchmarks: SimpleQuestions (SQ) and WebQSP (WQ).\n\nFor SQ, we use the gold semantic parse from yin2016simple . We start with their entity linking results and then run our relation detection model on the same data set.\n\nFor WQ, we use the entire Freebase KB for evaluation purposes. We create a new relation detection task from the WebQSP data set. For each question and its labeled semantic parse: (1) we first select the topic entity from the parse; and then (2", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
